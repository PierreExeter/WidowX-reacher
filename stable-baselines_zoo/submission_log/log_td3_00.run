WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/td3/td3.py:131: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/td3/policies.py:124: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_layers.py:57: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/td3/td3.py:194: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/td3/td3.py:226: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/td3/td3.py:240: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

========== widowx_reacher-v5 ==========
Seed: 0
OrderedDict([('batch_size', 128),
             ('buffer_size', 50000),
             ('gamma', 0.99),
             ('gradient_steps', 100),
             ('learning_rate', 0.0003),
             ('learning_starts', 100),
             ('n_timesteps', 10000),
             ('policy', 'MlpPolicy'),
             ('policy_delay', 2),
             ('policy_kwargs', 'None'),
             ('random_exploration', 0.0),
             ('target_noise_clip', 0.5),
             ('target_policy_noise', 0.2),
             ('tau', 0.005),
             ('train_freq', 100)])
Using 1 environments
Creating test environment
TRAINING ENV TYPE :  <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f2e2a59c668>
EVAL ENV TYPE :  <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f2e2a5b5438>
Log path: logs/train_10K_widowx_reacher-v5/td3/widowx_reacher-v5_1
-----------------------------------------
| current_lr              | 0.0003      |
| ep_rewmean              | -5.64       |
| episodes                | 4           |
| eplenmean               | 100         |
| fps                     | 515         |
| mean 100 episode reward | -5.6        |
| n_updates               | 200         |
| qf1_loss                | 0.009575596 |
| qf2_loss                | 0.006227044 |
| time_elapsed            | 0           |
| total timesteps         | 300         |
-----------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -4.65        |
| episodes                | 8            |
| eplenmean               | 100          |
| fps                     | 583          |
| mean 100 episode reward | -4.6         |
| n_updates               | 600          |
| qf1_loss                | 0.0047237473 |
| qf2_loss                | 0.004588723  |
| time_elapsed            | 1            |
| total timesteps         | 700          |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -3.81        |
| episodes                | 12           |
| eplenmean               | 100          |
| fps                     | 591          |
| mean 100 episode reward | -3.8         |
| n_updates               | 1000         |
| qf1_loss                | 0.0048355795 |
| qf2_loss                | 0.004770806  |
| time_elapsed            | 1            |
| total timesteps         | 1100         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -3.76        |
| episodes                | 16           |
| eplenmean               | 100          |
| fps                     | 592          |
| mean 100 episode reward | -3.8         |
| n_updates               | 1400         |
| qf1_loss                | 0.0042353235 |
| qf2_loss                | 0.0041564004 |
| time_elapsed            | 2            |
| total timesteps         | 1500         |
------------------------------------------
-----------------------------------------
| current_lr              | 0.0003      |
| ep_rewmean              | -3.15       |
| episodes                | 20          |
| eplenmean               | 100         |
| fps                     | 599         |
| mean 100 episode reward | -3.2        |
| n_updates               | 1800        |
| qf1_loss                | 0.003846386 |
| qf2_loss                | 0.003821595 |
| time_elapsed            | 3           |
| total timesteps         | 1900        |
-----------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -2.8         |
| episodes                | 24           |
| eplenmean               | 100          |
| fps                     | 601          |
| mean 100 episode reward | -2.8         |
| n_updates               | 2200         |
| qf1_loss                | 0.0031997778 |
| qf2_loss                | 0.0031033773 |
| time_elapsed            | 3            |
| total timesteps         | 2300         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -2.44        |
| episodes                | 28           |
| eplenmean               | 100          |
| fps                     | 603          |
| mean 100 episode reward | -2.4         |
| n_updates               | 2600         |
| qf1_loss                | 0.0023534992 |
| qf2_loss                | 0.0023468395 |
| time_elapsed            | 4            |
| total timesteps         | 2700         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -2.24        |
| episodes                | 32           |
| eplenmean               | 100          |
| fps                     | 604          |
| mean 100 episode reward | -2.2         |
| n_updates               | 3000         |
| qf1_loss                | 0.0021758645 |
| qf2_loss                | 0.0022448965 |
| time_elapsed            | 5            |
| total timesteps         | 3100         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -2.39        |
| episodes                | 36           |
| eplenmean               | 100          |
| fps                     | 604          |
| mean 100 episode reward | -2.4         |
| n_updates               | 3400         |
| qf1_loss                | 0.0023755298 |
| qf2_loss                | 0.0023580233 |
| time_elapsed            | 5            |
| total timesteps         | 3500         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -2.46        |
| episodes                | 40           |
| eplenmean               | 100          |
| fps                     | 605          |
| mean 100 episode reward | -2.5         |
| n_updates               | 3800         |
| qf1_loss                | 0.0025105325 |
| qf2_loss                | 0.0026002077 |
| time_elapsed            | 6            |
| total timesteps         | 3900         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -2.48        |
| episodes                | 44           |
| eplenmean               | 100          |
| fps                     | 606          |
| mean 100 episode reward | -2.5         |
| n_updates               | 4200         |
| qf1_loss                | 0.0020840378 |
| qf2_loss                | 0.0020897647 |
| time_elapsed            | 7            |
| total timesteps         | 4300         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -2.38        |
| episodes                | 48           |
| eplenmean               | 100          |
| fps                     | 607          |
| mean 100 episode reward | -2.4         |
| n_updates               | 4600         |
| qf1_loss                | 0.00288535   |
| qf2_loss                | 0.0028894097 |
| time_elapsed            | 7            |
| total timesteps         | 4700         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -2.24        |
| episodes                | 52           |
| eplenmean               | 100          |
| fps                     | 608          |
| mean 100 episode reward | -2.2         |
| n_updates               | 5000         |
| qf1_loss                | 0.0026398054 |
| qf2_loss                | 0.0026357472 |
| time_elapsed            | 8            |
| total timesteps         | 5100         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -2.12        |
| episodes                | 56           |
| eplenmean               | 100          |
| fps                     | 608          |
| mean 100 episode reward | -2.1         |
| n_updates               | 5400         |
| qf1_loss                | 0.0026049232 |
| qf2_loss                | 0.0025591752 |
| time_elapsed            | 9            |
| total timesteps         | 5500         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -2.04        |
| episodes                | 60           |
| eplenmean               | 100          |
| fps                     | 608          |
| mean 100 episode reward | -2           |
| n_updates               | 5800         |
| qf1_loss                | 0.0023567704 |
| qf2_loss                | 0.0023265434 |
| time_elapsed            | 9            |
| total timesteps         | 5900         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -1.95        |
| episodes                | 64           |
| eplenmean               | 100          |
| fps                     | 607          |
| mean 100 episode reward | -1.9         |
| n_updates               | 6200         |
| qf1_loss                | 0.0025901461 |
| qf2_loss                | 0.002541874  |
| time_elapsed            | 10           |
| total timesteps         | 6300         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -1.87        |
| episodes                | 68           |
| eplenmean               | 100          |
| fps                     | 607          |
| mean 100 episode reward | -1.9         |
| n_updates               | 6600         |
| qf1_loss                | 0.0023850615 |
| qf2_loss                | 0.0023546433 |
| time_elapsed            | 11           |
| total timesteps         | 6700         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -1.8         |
| episodes                | 72           |
| eplenmean               | 100          |
| fps                     | 607          |
| mean 100 episode reward | -1.8         |
| n_updates               | 7000         |
| qf1_loss                | 0.0021223794 |
| qf2_loss                | 0.00210017   |
| time_elapsed            | 11           |
| total timesteps         | 7100         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -1.75        |
| episodes                | 76           |
| eplenmean               | 100          |
| fps                     | 608          |
| mean 100 episode reward | -1.7         |
| n_updates               | 7400         |
| qf1_loss                | 0.0023913311 |
| qf2_loss                | 0.002378596  |
| time_elapsed            | 12           |
| total timesteps         | 7500         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -1.7         |
| episodes                | 80           |
| eplenmean               | 100          |
| fps                     | 607          |
| mean 100 episode reward | -1.7         |
| n_updates               | 7800         |
| qf1_loss                | 0.0021113167 |
| qf2_loss                | 0.0021210045 |
| time_elapsed            | 12           |
| total timesteps         | 7900         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -1.68        |
| episodes                | 84           |
| eplenmean               | 100          |
| fps                     | 607          |
| mean 100 episode reward | -1.7         |
| n_updates               | 8200         |
| qf1_loss                | 0.002111037  |
| qf2_loss                | 0.0020930348 |
| time_elapsed            | 13           |
| total timesteps         | 8300         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -1.67        |
| episodes                | 88           |
| eplenmean               | 100          |
| fps                     | 607          |
| mean 100 episode reward | -1.7         |
| n_updates               | 8600         |
| qf1_loss                | 0.0021935608 |
| qf2_loss                | 0.0022067048 |
| time_elapsed            | 14           |
| total timesteps         | 8700         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -1.63        |
| episodes                | 92           |
| eplenmean               | 100          |
| fps                     | 606          |
| mean 100 episode reward | -1.6         |
| n_updates               | 9000         |
| qf1_loss                | 0.0021460697 |
| qf2_loss                | 0.002124685  |
| time_elapsed            | 15           |
| total timesteps         | 9100         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -1.6         |
| episodes                | 96           |
| eplenmean               | 100          |
| fps                     | 605          |
| mean 100 episode reward | -1.6         |
| n_updates               | 9400         |
| qf1_loss                | 0.001984997  |
| qf2_loss                | 0.0019828905 |
| time_elapsed            | 15           |
| total timesteps         | 9500         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -1.56        |
| episodes                | 100          |
| eplenmean               | 100          |
| fps                     | 605          |
| mean 100 episode reward | -1.6         |
| n_updates               | 9800         |
| qf1_loss                | 0.0021247424 |
| qf2_loss                | 0.0021213458 |
| time_elapsed            | 16           |
| total timesteps         | 9900         |
------------------------------------------
/home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/callbacks.py:285: UserWarning: Training and eval env are not of the same type<stable_baselines.common.base_class._UnvecWrapper object at 0x7f2e2a5b5358> != <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f2e2a5b5438>
  "{} != {}".format(self.training_env, self.eval_env))
Eval num_timesteps=10000, episode_reward=-0.48 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
Saving to logs/train_10K_widowx_reacher-v5/td3/widowx_reacher-v5_1
pybullet build time: May 18 2020 02:46:26
