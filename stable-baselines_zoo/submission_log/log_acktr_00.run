WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/acktr/acktr.py:181: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/acktr/acktr.py:223: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/acktr/kfac.py:973: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.

========== widowx_reacher-v5 ==========
Seed: 0
OrderedDict([('ent_coef', 0.0),
             ('gamma', 0.99),
             ('learning_rate', 0.06),
             ('lr_schedule', 'constant'),
             ('n_envs', 8),
             ('n_steps', 16),
             ('n_timesteps', 2000000.0),
             ('normalize', True),
             ('policy', 'MlpPolicy')])
Using 8 environments
Overwriting n_timesteps with n=500000
Normalizing input and reward
Creating test environment
TRAINING ENV TYPE :  <stable_baselines.common.vec_env.vec_normalize.VecNormalize object at 0x7f0e4937b048>
Normalization activated: {'norm_reward': False}
EVAL ENV TYPE :  <stable_baselines.common.vec_env.vec_normalize.VecNormalize object at 0x7f0e49386198>
Log path: logs/train_0.5M_widowx_reacher-v5/acktr/widowx_reacher-v5_1
---------------------------------
| explained_variance | -0.00162 |
| fps                | 205      |
| nupdates           | 1        |
| policy_entropy     | 8.51     |
| policy_loss        | -0.0227  |
| total_timesteps    | 128      |
| value_loss         | 10.6     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.98 +/- 0.08
Episode length: 100.00 +/- 0.00
New best mean reward!
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -2.87    |
| explained_variance | -0.599   |
| fps                | 3846     |
| nupdates           | 100      |
| policy_entropy     | 8.58     |
| policy_loss        | -0.136   |
| total_timesteps    | 12800    |
| value_loss         | 1.38     |
---------------------------------
Eval num_timesteps=20000, episode_reward=-1.65 +/- 0.01
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 98.1     |
| ep_reward_mean     | -2.18    |
| explained_variance | 0.357    |
| fps                | 4311     |
| nupdates           | 200      |
| policy_entropy     | 8.62     |
| policy_loss        | -0.163   |
| total_timesteps    | 25600    |
| value_loss         | 0.331    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-0.73 +/- 0.01
Episode length: 100.00 +/- 0.00
New best mean reward!
---------------------------------
| ep_len_mean        | 98.6     |
| ep_reward_mean     | -1.69    |
| explained_variance | 0.807    |
| fps                | 4469     |
| nupdates           | 300      |
| policy_entropy     | 8.63     |
| policy_loss        | -0.113   |
| total_timesteps    | 38400    |
| value_loss         | 0.0319   |
---------------------------------
Eval num_timesteps=40000, episode_reward=-0.51 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
Eval num_timesteps=50000, episode_reward=-0.38 +/- 0.05
Episode length: 100.00 +/- 0.00
New best mean reward!
---------------------------------
| ep_len_mean        | 98.5     |
| ep_reward_mean     | -1.59    |
| explained_variance | 0.753    |
| fps                | 4413     |
| nupdates           | 400      |
| policy_entropy     | 8.58     |
| policy_loss        | -0.0572  |
| total_timesteps    | 51200    |
| value_loss         | 0.079    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-0.61 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.71    |
| explained_variance | 0.852    |
| fps                | 4471     |
| nupdates           | 500      |
| policy_entropy     | 8.58     |
| policy_loss        | 0.332    |
| total_timesteps    | 64000    |
| value_loss         | 0.0349   |
---------------------------------
Eval num_timesteps=70000, episode_reward=-0.55 +/- 0.01
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.88    |
| explained_variance | 0.834    |
| fps                | 4512     |
| nupdates           | 600      |
| policy_entropy     | 8.55     |
| policy_loss        | 0.315    |
| total_timesteps    | 76800    |
| value_loss         | 0.104    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-0.45 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 97.8     |
| ep_reward_mean     | -1.54    |
| explained_variance | 0.627    |
| fps                | 4548     |
| nupdates           | 700      |
| policy_entropy     | 8.54     |
| policy_loss        | 0.0842   |
| total_timesteps    | 89600    |
| value_loss         | 0.0714   |
---------------------------------
Eval num_timesteps=90000, episode_reward=-0.35 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
Eval num_timesteps=100000, episode_reward=-0.36 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 99.3     |
| ep_reward_mean     | -1.49    |
| explained_variance | 0.694    |
| fps                | 4513     |
| nupdates           | 800      |
| policy_entropy     | 8.58     |
| policy_loss        | 0.0335   |
| total_timesteps    | 102400   |
| value_loss         | 0.0423   |
---------------------------------
Eval num_timesteps=110000, episode_reward=-0.33 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.47    |
| explained_variance | 0.901    |
| fps                | 4532     |
| nupdates           | 900      |
| policy_entropy     | 8.57     |
| policy_loss        | 0.0137   |
| total_timesteps    | 115200   |
| value_loss         | 0.0204   |
---------------------------------
Eval num_timesteps=120000, episode_reward=-0.28 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
---------------------------------
| ep_len_mean        | 94.7     |
| ep_reward_mean     | -1.38    |
| explained_variance | 0.904    |
| fps                | 4549     |
| nupdates           | 1000     |
| policy_entropy     | 8.53     |
| policy_loss        | -0.229   |
| total_timesteps    | 128000   |
| value_loss         | 0.0184   |
---------------------------------
Eval num_timesteps=130000, episode_reward=-0.43 +/- 0.00
Episode length: 100.00 +/- 0.00
Eval num_timesteps=140000, episode_reward=-0.36 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.55    |
| explained_variance | 0.963    |
| fps                | 4524     |
| nupdates           | 1100     |
| policy_entropy     | 8.5      |
| policy_loss        | -0.127   |
| total_timesteps    | 140800   |
| value_loss         | 0.00935  |
---------------------------------
Eval num_timesteps=150000, episode_reward=-0.36 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 98.7     |
| ep_reward_mean     | -1.48    |
| explained_variance | 0.859    |
| fps                | 4528     |
| nupdates           | 1200     |
| policy_entropy     | 8.52     |
| policy_loss        | 0.0457   |
| total_timesteps    | 153600   |
| value_loss         | 0.0247   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-0.48 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.55    |
| explained_variance | 0.929    |
| fps                | 4539     |
| nupdates           | 1300     |
| policy_entropy     | 8.52     |
| policy_loss        | 0.0339   |
| total_timesteps    | 166400   |
| value_loss         | 0.0132   |
---------------------------------
Eval num_timesteps=170000, episode_reward=-0.39 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 93.4     |
| ep_reward_mean     | -1.36    |
| explained_variance | 0.949    |
| fps                | 4534     |
| nupdates           | 1400     |
| policy_entropy     | 8.52     |
| policy_loss        | -0.251   |
| total_timesteps    | 179200   |
| value_loss         | 0.0128   |
---------------------------------
Eval num_timesteps=180000, episode_reward=-0.32 +/- 0.00
Episode length: 100.00 +/- 0.00
Eval num_timesteps=190000, episode_reward=-0.35 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.45    |
| explained_variance | 0.942    |
| fps                | 4476     |
| nupdates           | 1500     |
| policy_entropy     | 8.48     |
| policy_loss        | 0.0946   |
| total_timesteps    | 192000   |
| value_loss         | 0.0145   |
---------------------------------
Eval num_timesteps=200000, episode_reward=-0.33 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 98.3     |
| ep_reward_mean     | -1.43    |
| explained_variance | 0.648    |
| fps                | 4483     |
| nupdates           | 1600     |
| policy_entropy     | 8.51     |
| policy_loss        | -0.0295  |
| total_timesteps    | 204800   |
| value_loss         | 0.124    |
---------------------------------
Eval num_timesteps=210000, episode_reward=-0.31 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 97       |
| ep_reward_mean     | -1.41    |
| explained_variance | 0.839    |
| fps                | 4485     |
| nupdates           | 1700     |
| policy_entropy     | 8.5      |
| policy_loss        | -0.0401  |
| total_timesteps    | 217600   |
| value_loss         | 0.0273   |
---------------------------------
Eval num_timesteps=220000, episode_reward=-0.36 +/- 0.00
Episode length: 100.00 +/- 0.00
Eval num_timesteps=230000, episode_reward=-0.34 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 98.6     |
| ep_reward_mean     | -1.44    |
| explained_variance | 0.949    |
| fps                | 4467     |
| nupdates           | 1800     |
| policy_entropy     | 8.5      |
| policy_loss        | -0.00191 |
| total_timesteps    | 230400   |
| value_loss         | 0.0127   |
---------------------------------
Eval num_timesteps=240000, episode_reward=-0.32 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.45    |
| explained_variance | 0.961    |
| fps                | 4465     |
| nupdates           | 1900     |
| policy_entropy     | 8.48     |
| policy_loss        | -0.199   |
| total_timesteps    | 243200   |
| value_loss         | 0.0106   |
---------------------------------
Eval num_timesteps=250000, episode_reward=-0.32 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.45    |
| explained_variance | 0.933    |
| fps                | 4451     |
| nupdates           | 2000     |
| policy_entropy     | 8.48     |
| policy_loss        | -0.0234  |
| total_timesteps    | 256000   |
| value_loss         | 0.018    |
---------------------------------
Eval num_timesteps=260000, episode_reward=-0.34 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.44    |
| explained_variance | 0.933    |
| fps                | 4447     |
| nupdates           | 2100     |
| policy_entropy     | 8.49     |
| policy_loss        | -0.173   |
| total_timesteps    | 268800   |
| value_loss         | 0.0181   |
---------------------------------
Eval num_timesteps=270000, episode_reward=-0.33 +/- 0.00
Episode length: 100.00 +/- 0.00
Eval num_timesteps=280000, episode_reward=-0.33 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 99.4     |
| ep_reward_mean     | -1.42    |
| explained_variance | 0.908    |
| fps                | 4425     |
| nupdates           | 2200     |
| policy_entropy     | 8.51     |
| policy_loss        | -0.286   |
| total_timesteps    | 281600   |
| value_loss         | 0.0162   |
---------------------------------
Eval num_timesteps=290000, episode_reward=-0.33 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.43    |
| explained_variance | 0.904    |
| fps                | 4423     |
| nupdates           | 2300     |
| policy_entropy     | 8.49     |
| policy_loss        | 0.107    |
| total_timesteps    | 294400   |
| value_loss         | 0.0172   |
---------------------------------
Eval num_timesteps=300000, episode_reward=-0.32 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.43    |
| explained_variance | 0.874    |
| fps                | 4434     |
| nupdates           | 2400     |
| policy_entropy     | 8.48     |
| policy_loss        | -0.121   |
| total_timesteps    | 307200   |
| value_loss         | 0.0226   |
---------------------------------
Eval num_timesteps=310000, episode_reward=-0.33 +/- 0.00
Episode length: 100.00 +/- 0.00
Eval num_timesteps=320000, episode_reward=-0.27 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
---------------------------------
| ep_len_mean        | 98.8     |
| ep_reward_mean     | -1.39    |
| explained_variance | 0.962    |
| fps                | 4427     |
| nupdates           | 2500     |
| policy_entropy     | 8.45     |
| policy_loss        | -0.192   |
| total_timesteps    | 320000   |
| value_loss         | 0.0105   |
---------------------------------
Eval num_timesteps=330000, episode_reward=-0.33 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 95       |
| ep_reward_mean     | -1.36    |
| explained_variance | 0.93     |
| fps                | 4435     |
| nupdates           | 2600     |
| policy_entropy     | 8.46     |
| policy_loss        | -0.364   |
| total_timesteps    | 332800   |
| value_loss         | 0.021    |
---------------------------------
Eval num_timesteps=340000, episode_reward=-0.33 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 98.1     |
| ep_reward_mean     | -1.39    |
| explained_variance | 0.968    |
| fps                | 4445     |
| nupdates           | 2700     |
| policy_entropy     | 8.45     |
| policy_loss        | -0.22    |
| total_timesteps    | 345600   |
| value_loss         | 0.00815  |
---------------------------------
Eval num_timesteps=350000, episode_reward=-0.33 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.43    |
| explained_variance | 0.958    |
| fps                | 4453     |
| nupdates           | 2800     |
| policy_entropy     | 8.46     |
| policy_loss        | 0.198    |
| total_timesteps    | 358400   |
| value_loss         | 0.0113   |
---------------------------------
Eval num_timesteps=360000, episode_reward=-0.32 +/- 0.00
Episode length: 100.00 +/- 0.00
Eval num_timesteps=370000, episode_reward=-0.32 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 97.6     |
| ep_reward_mean     | -1.37    |
| explained_variance | 0.811    |
| fps                | 4449     |
| nupdates           | 2900     |
| policy_entropy     | 8.48     |
| policy_loss        | 0.0856   |
| total_timesteps    | 371200   |
| value_loss         | 0.0397   |
---------------------------------
Eval num_timesteps=380000, episode_reward=-0.31 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 95.8     |
| ep_reward_mean     | -1.34    |
| explained_variance | 0.886    |
| fps                | 4456     |
| nupdates           | 3000     |
| policy_entropy     | 8.5      |
| policy_loss        | -0.16    |
| total_timesteps    | 384000   |
| value_loss         | 0.0332   |
---------------------------------
Eval num_timesteps=390000, episode_reward=-0.32 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 90.8     |
| ep_reward_mean     | -1.27    |
| explained_variance | 0.887    |
| fps                | 4460     |
| nupdates           | 3100     |
| policy_entropy     | 8.49     |
| policy_loss        | -0.0874  |
| total_timesteps    | 396800   |
| value_loss         | 0.0309   |
---------------------------------
Eval num_timesteps=400000, episode_reward=-0.32 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 95.5     |
| ep_reward_mean     | -1.38    |
| explained_variance | 0.694    |
| fps                | 4463     |
| nupdates           | 3200     |
| policy_entropy     | 8.47     |
| policy_loss        | -0.289   |
| total_timesteps    | 409600   |
| value_loss         | 0.0749   |
---------------------------------
Eval num_timesteps=410000, episode_reward=-0.32 +/- 0.00
Episode length: 100.00 +/- 0.00
Eval num_timesteps=420000, episode_reward=-0.32 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 91.9     |
| ep_reward_mean     | -1.29    |
| explained_variance | 0.9      |
| fps                | 4454     |
| nupdates           | 3300     |
| policy_entropy     | 8.46     |
| policy_loss        | 0.0363   |
| total_timesteps    | 422400   |
| value_loss         | 0.0228   |
---------------------------------
Eval num_timesteps=430000, episode_reward=-0.32 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 92       |
| ep_reward_mean     | -1.3     |
| explained_variance | 0.915    |
| fps                | 4462     |
| nupdates           | 3400     |
| policy_entropy     | 8.43     |
| policy_loss        | 0.0674   |
| total_timesteps    | 435200   |
| value_loss         | 0.0198   |
---------------------------------
Eval num_timesteps=440000, episode_reward=-0.13 +/- 0.10
Episode length: 62.80 +/- 30.37
New best mean reward!
---------------------------------
| ep_len_mean        | 95.7     |
| ep_reward_mean     | -1.36    |
| explained_variance | 0.866    |
| fps                | 4473     |
| nupdates           | 3500     |
| policy_entropy     | 8.42     |
| policy_loss        | 0.08     |
| total_timesteps    | 448000   |
| value_loss         | 0.0209   |
---------------------------------
Eval num_timesteps=450000, episode_reward=-0.06 +/- 0.00
Episode length: 36.00 +/- 0.00
New best mean reward!
Eval num_timesteps=460000, episode_reward=-0.06 +/- 0.00
Episode length: 40.20 +/- 1.47
---------------------------------
| ep_len_mean        | 94.6     |
| ep_reward_mean     | -1.38    |
| explained_variance | 0.866    |
| fps                | 4482     |
| nupdates           | 3600     |
| policy_entropy     | 8.41     |
| policy_loss        | 0.0468   |
| total_timesteps    | 460800   |
| value_loss         | 0.0354   |
---------------------------------
Eval num_timesteps=470000, episode_reward=-0.06 +/- 0.00
Episode length: 47.00 +/- 0.00
---------------------------------
| ep_len_mean        | 93.5     |
| ep_reward_mean     | -1.35    |
| explained_variance | 0.649    |
| fps                | 4492     |
| nupdates           | 3700     |
| policy_entropy     | 8.41     |
| policy_loss        | -0.0536  |
| total_timesteps    | 473600   |
| value_loss         | 0.0865   |
---------------------------------
Eval num_timesteps=480000, episode_reward=-0.11 +/- 0.00
Episode length: 62.00 +/- 0.00
---------------------------------
| ep_len_mean        | 98.4     |
| ep_reward_mean     | -1.4     |
| explained_variance | 0.845    |
| fps                | 4499     |
| nupdates           | 3800     |
| policy_entropy     | 8.38     |
| policy_loss        | 0.0326   |
| total_timesteps    | 486400   |
| value_loss         | 0.0343   |
---------------------------------
Eval num_timesteps=490000, episode_reward=-0.14 +/- 0.00
Episode length: 62.20 +/- 0.75
---------------------------------
| ep_len_mean        | 99.4     |
| ep_reward_mean     | -1.44    |
| explained_variance | 0.81     |
| fps                | 4505     |
| nupdates           | 3900     |
| policy_entropy     | 8.41     |
| policy_loss        | 0.0535   |
| total_timesteps    | 499200   |
| value_loss         | 0.032    |
---------------------------------
Saving to logs/train_0.5M_widowx_reacher-v5/acktr/widowx_reacher-v5_1
pybullet build time: May 18 2020 02:46:26
