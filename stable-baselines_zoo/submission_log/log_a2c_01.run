WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/a2c/a2c.py:160: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/clip_ops.py:286: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/a2c/a2c.py:184: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/a2c/a2c.py:196: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

========== widowx_reacher-v5 ==========
Seed: 1
OrderedDict([('alpha', 0.99),
             ('ent_coef', 0.01),
             ('epsilon', 1e-05),
             ('gamma', 0.99),
             ('learning_rate', 0.0007),
             ('lr_schedule', 'constant'),
             ('max_grad_norm', 0.5),
             ('momentum', 0.0),
             ('n_envs', 8),
             ('n_steps', 5),
             ('n_timesteps', 10000),
             ('normalize', True),
             ('policy', 'MlpPolicy'),
             ('vf_coef', 0.25)])
Using 8 environments
Normalizing input and reward
Creating test environment
TRAINING ENV TYPE :  <stable_baselines.common.vec_env.vec_normalize.VecNormalize object at 0x7fae783a9a20>
Normalization activated: {'norm_reward': False}
EVAL ENV TYPE :  <stable_baselines.common.vec_env.vec_normalize.VecNormalize object at 0x7fae7834c7b8>
Log path: logs/train_10K_widowx_reacher-v5/a2c/widowx_reacher-v5_2
---------------------------------
| explained_variance | -0.186   |
| fps                | 202      |
| nupdates           | 1        |
| policy_entropy     | 8.51     |
| total_timesteps    | 40       |
| value_loss         | 10.6     |
---------------------------------
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -2.94    |
| explained_variance | -12.2    |
| fps                | 4096     |
| nupdates           | 100      |
| policy_entropy     | 8.52     |
| total_timesteps    | 4000     |
| value_loss         | 0.139    |
---------------------------------
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -2.89    |
| explained_variance | -10.9    |
| fps                | 4564     |
| nupdates           | 200      |
| policy_entropy     | 8.53     |
| total_timesteps    | 8000     |
| value_loss         | 0.143    |
---------------------------------
Eval num_timesteps=10000, episode_reward=-2.12 +/- 0.02
Episode length: 100.00 +/- 0.00
New best mean reward!
Saving to logs/train_10K_widowx_reacher-v5/a2c/widowx_reacher-v5_2
pybullet build time: May 18 2020 02:46:26
