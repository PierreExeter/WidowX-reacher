[sonicgpu1.compute:27913] pml_ucx.c:285  Error: UCP worker does not support MPI_THREAD_MULTIPLE
WARNING:tensorflow:From /home/people/paumjaud/WidowX-reacher/stable-baselines/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING:tensorflow:From /home/people/paumjaud/WidowX-reacher/stable-baselines/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /home/people/paumjaud/WidowX-reacher/stable-baselines/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /home/people/paumjaud/WidowX-reacher/stable-baselines/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /home/people/paumjaud/WidowX-reacher/stable-baselines/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/people/paumjaud/WidowX-reacher/stable-baselines/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From /home/people/paumjaud/WidowX-reacher/stable-baselines/stable_baselines/acktr/acktr.py:181: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /home/people/paumjaud/.conda/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /home/people/paumjaud/WidowX-reacher/stable-baselines/stable_baselines/acktr/acktr.py:223: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

WARNING:tensorflow:From /home/people/paumjaud/WidowX-reacher/stable-baselines/stable_baselines/acktr/kfac.py:973: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.

========== widowx_reacher-v5 ==========
Seed: 1
OrderedDict([('ent_coef', 4.675966676588504e-06),
             ('gamma', 0.9999),
             ('learning_rate', 2.6481434034790655e-05),
             ('lr_schedule', 'linear'),
             ('n_envs', 8),
             ('n_steps', 32),
             ('n_timesteps', 2000000.0),
             ('normalize', True),
             ('policy', 'MlpPolicy'),
             ('vf_coef', 0.08033789685230008)])
Using 8 environments
Overwriting n_timesteps with n=200000
Normalizing input and reward
Creating test environment
TRAINING ENV TYPE :  <stable_baselines.common.vec_env.vec_normalize.VecNormalize object at 0x7f7d1b972048>
Normalization activated: {'norm_reward': False}
EVAL ENV TYPE :  <stable_baselines.common.vec_env.vec_normalize.VecNormalize object at 0x7f7d1b97e0b8>
Log path: logs/train_0.2M_widowx_reacher-v5_SONIC/acktr/widowx_reacher-v5_2
---------------------------------
| explained_variance | -0.0292  |
| fps                | 220      |
| nupdates           | 1        |
| policy_entropy     | 8.51     |
| policy_loss        | -0.0228  |
| total_timesteps    | 256      |
| value_loss         | 18.2     |
---------------------------------
Eval num_timesteps=10000, episode_reward=-2.74 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
Eval num_timesteps=20000, episode_reward=-2.66 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -2.91    |
| explained_variance | -0.754   |
| fps                | 2925     |
| nupdates           | 100      |
| policy_entropy     | 8.51     |
| policy_loss        | -0.0784  |
| total_timesteps    | 25600    |
| value_loss         | 0.525    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-2.66 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
Eval num_timesteps=40000, episode_reward=-2.64 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
Eval num_timesteps=50000, episode_reward=-2.60 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -3.08    |
| explained_variance | -1.43    |
| fps                | 3106     |
| nupdates           | 200      |
| policy_entropy     | 8.51     |
| policy_loss        | -0.0608  |
| total_timesteps    | 51200    |
| value_loss         | 0.542    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-2.58 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
Eval num_timesteps=70000, episode_reward=-2.58 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -3.17    |
| explained_variance | -0.607   |
| fps                | 3235     |
| nupdates           | 300      |
| policy_entropy     | 8.51     |
| policy_loss        | 0.152    |
| total_timesteps    | 76800    |
| value_loss         | 0.865    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-2.57 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
Eval num_timesteps=90000, episode_reward=-2.55 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
Eval num_timesteps=100000, episode_reward=-2.56 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -3.19    |
| explained_variance | 0.185    |
| fps                | 3254     |
| nupdates           | 400      |
| policy_entropy     | 8.51     |
| policy_loss        | -0.115   |
| total_timesteps    | 102400   |
| value_loss         | 0.706    |
---------------------------------
Eval num_timesteps=110000, episode_reward=-2.53 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
Eval num_timesteps=120000, episode_reward=-2.51 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -2.84    |
| explained_variance | -0.00942 |
| fps                | 3307     |
| nupdates           | 500      |
| policy_entropy     | 8.51     |
| policy_loss        | 0.00511  |
| total_timesteps    | 128000   |
| value_loss         | 0.29     |
---------------------------------
Eval num_timesteps=130000, episode_reward=-2.50 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
Eval num_timesteps=140000, episode_reward=-2.49 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
Eval num_timesteps=150000, episode_reward=-2.50 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -2.96    |
| explained_variance | 0.0767   |
| fps                | 3312     |
| nupdates           | 600      |
| policy_entropy     | 8.51     |
| policy_loss        | -0.226   |
| total_timesteps    | 153600   |
| value_loss         | 0.915    |
---------------------------------
Eval num_timesteps=160000, episode_reward=-2.48 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
Eval num_timesteps=170000, episode_reward=-2.49 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -2.89    |
| explained_variance | -0.044   |
| fps                | 3339     |
| nupdates           | 700      |
| policy_entropy     | 8.51     |
| policy_loss        | -0.0879  |
| total_timesteps    | 179200   |
| value_loss         | 0.643    |
---------------------------------
Eval num_timesteps=180000, episode_reward=-2.47 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
Eval num_timesteps=190000, episode_reward=-2.46 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
Saving to logs/train_0.2M_widowx_reacher-v5_SONIC/acktr/widowx_reacher-v5_2
pybullet build time: May 18 2020 02:46:26
