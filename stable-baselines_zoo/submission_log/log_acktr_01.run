--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   n295
  Local device: hfi1_0
--------------------------------------------------------------------------
WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/acktr/acktr.py:181: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/acktr/acktr.py:223: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/acktr/kfac.py:973: The name tf.train.MomentumOptimizer is deprecated. Please use tf.compat.v1.train.MomentumOptimizer instead.

========== widowx_reacher-v7 ==========
Seed: 1
OrderedDict([('ent_coef', 0.0),
             ('gamma', 0.99),
             ('learning_rate', 0.06),
             ('lr_schedule', 'constant'),
             ('n_envs', 8),
             ('n_steps', 16),
             ('n_timesteps', 2000000.0),
             ('normalize', True),
             ('policy', 'MlpPolicy')])
Using 8 environments
Overwriting n_timesteps with n=500000
Normalizing input and reward
Creating test environment
TRAINING ENV TYPE :  <stable_baselines.common.vec_env.vec_normalize.VecNormalize object at 0x7f10aaa78780>
Normalization activated: {'norm_reward': False}
EVAL ENV TYPE :  <stable_baselines.common.vec_env.vec_normalize.VecNormalize object at 0x7f10a83ec4a8>
Log path: logs/train_0.5M_widowx_reacher-v7_KAY/acktr/widowx_reacher-v7_2
---------------------------------
| explained_variance | 0.0133   |
| fps                | 110      |
| nupdates           | 1        |
| policy_entropy     | 8.51     |
| policy_loss        | -0.0241  |
| total_timesteps    | 128      |
| value_loss         | 8.5      |
---------------------------------
Eval num_timesteps=10000, episode_reward=-1.04 +/- 0.71
Episode length: 100.00 +/- 0.00
New best mean reward!
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -2.05    |
| explained_variance | -0.031   |
| fps                | 2229     |
| nupdates           | 100      |
| policy_entropy     | 8.4      |
| policy_loss        | -0.101   |
| total_timesteps    | 12800    |
| value_loss         | 0.208    |
---------------------------------
Eval num_timesteps=20000, episode_reward=-2.07 +/- 1.09
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.58    |
| explained_variance | 0.0997   |
| fps                | 2588     |
| nupdates           | 200      |
| policy_entropy     | 8.37     |
| policy_loss        | -0.0253  |
| total_timesteps    | 25600    |
| value_loss         | 0.171    |
---------------------------------
Eval num_timesteps=30000, episode_reward=-2.62 +/- 1.81
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.47    |
| explained_variance | 0.15     |
| fps                | 2742     |
| nupdates           | 300      |
| policy_entropy     | 8.34     |
| policy_loss        | 0.00176  |
| total_timesteps    | 38400    |
| value_loss         | 0.0707   |
---------------------------------
Eval num_timesteps=40000, episode_reward=-1.22 +/- 0.90
Episode length: 100.00 +/- 0.00
Eval num_timesteps=50000, episode_reward=-1.54 +/- 0.49
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.5     |
| explained_variance | 0.128    |
| fps                | 2769     |
| nupdates           | 400      |
| policy_entropy     | 8.32     |
| policy_loss        | -0.069   |
| total_timesteps    | 51200    |
| value_loss         | 0.0873   |
---------------------------------
Eval num_timesteps=60000, episode_reward=-1.49 +/- 0.58
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.54    |
| explained_variance | -0.00284 |
| fps                | 2832     |
| nupdates           | 500      |
| policy_entropy     | 8.28     |
| policy_loss        | 0.0993   |
| total_timesteps    | 64000    |
| value_loss         | 0.362    |
---------------------------------
Eval num_timesteps=70000, episode_reward=-1.35 +/- 0.91
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.5     |
| explained_variance | -0.0616  |
| fps                | 2877     |
| nupdates           | 600      |
| policy_entropy     | 8.31     |
| policy_loss        | -0.268   |
| total_timesteps    | 76800    |
| value_loss         | 0.655    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-1.17 +/- 0.65
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.65    |
| explained_variance | -0.283   |
| fps                | 2910     |
| nupdates           | 700      |
| policy_entropy     | 8.35     |
| policy_loss        | -0.212   |
| total_timesteps    | 89600    |
| value_loss         | 0.647    |
---------------------------------
Eval num_timesteps=90000, episode_reward=-1.42 +/- 0.53
Episode length: 100.00 +/- 0.00
Eval num_timesteps=100000, episode_reward=-1.30 +/- 0.80
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.86    |
| explained_variance | 0.131    |
| fps                | 2902     |
| nupdates           | 800      |
| policy_entropy     | 8.37     |
| policy_loss        | -0.129   |
| total_timesteps    | 102400   |
| value_loss         | 0.235    |
---------------------------------
Eval num_timesteps=110000, episode_reward=-1.13 +/- 0.71
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.48    |
| explained_variance | -0.0274  |
| fps                | 2923     |
| nupdates           | 900      |
| policy_entropy     | 8.36     |
| policy_loss        | -0.0803  |
| total_timesteps    | 115200   |
| value_loss         | 0.16     |
---------------------------------
Eval num_timesteps=120000, episode_reward=-1.06 +/- 0.74
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.55    |
| explained_variance | -0.0255  |
| fps                | 2941     |
| nupdates           | 1000     |
| policy_entropy     | 8.35     |
| policy_loss        | 0.324    |
| total_timesteps    | 128000   |
| value_loss         | 0.0628   |
---------------------------------
Eval num_timesteps=130000, episode_reward=-1.62 +/- 0.72
Episode length: 100.00 +/- 0.00
Eval num_timesteps=140000, episode_reward=-1.64 +/- 0.62
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.55    |
| explained_variance | 0.119    |
| fps                | 2929     |
| nupdates           | 1100     |
| policy_entropy     | 8.32     |
| policy_loss        | -0.0961  |
| total_timesteps    | 140800   |
| value_loss         | 0.122    |
---------------------------------
Eval num_timesteps=150000, episode_reward=-1.98 +/- 0.36
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.48    |
| explained_variance | 0.425    |
| fps                | 2942     |
| nupdates           | 1200     |
| policy_entropy     | 8.32     |
| policy_loss        | 0.0952   |
| total_timesteps    | 153600   |
| value_loss         | 0.0271   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-1.11 +/- 0.92
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.5     |
| explained_variance | -0.0757  |
| fps                | 2953     |
| nupdates           | 1300     |
| policy_entropy     | 8.3      |
| policy_loss        | 0.00867  |
| total_timesteps    | 166400   |
| value_loss         | 0.0553   |
---------------------------------
Eval num_timesteps=170000, episode_reward=-1.10 +/- 0.99
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.76    |
| explained_variance | 0.00485  |
| fps                | 2961     |
| nupdates           | 1400     |
| policy_entropy     | 8.29     |
| policy_loss        | 0.0109   |
| total_timesteps    | 179200   |
| value_loss         | 0.184    |
---------------------------------
Eval num_timesteps=180000, episode_reward=-1.72 +/- 0.89
Episode length: 100.00 +/- 0.00
Eval num_timesteps=190000, episode_reward=-1.09 +/- 0.17
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.42    |
| explained_variance | 0.309    |
| fps                | 2953     |
| nupdates           | 1500     |
| policy_entropy     | 8.27     |
| policy_loss        | -0.143   |
| total_timesteps    | 192000   |
| value_loss         | 0.028    |
---------------------------------
Eval num_timesteps=200000, episode_reward=-1.69 +/- 0.15
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.6     |
| explained_variance | 0.384    |
| fps                | 2962     |
| nupdates           | 1600     |
| policy_entropy     | 8.25     |
| policy_loss        | 0.102    |
| total_timesteps    | 204800   |
| value_loss         | 0.0643   |
---------------------------------
Eval num_timesteps=210000, episode_reward=-2.36 +/- 0.84
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.48    |
| explained_variance | 0.262    |
| fps                | 2970     |
| nupdates           | 1700     |
| policy_entropy     | 8.29     |
| policy_loss        | 0.0294   |
| total_timesteps    | 217600   |
| value_loss         | 0.0546   |
---------------------------------
Eval num_timesteps=220000, episode_reward=-2.07 +/- 0.70
Episode length: 100.00 +/- 0.00
Eval num_timesteps=230000, episode_reward=-1.54 +/- 0.60
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.52    |
| explained_variance | 0.456    |
| fps                | 2962     |
| nupdates           | 1800     |
| policy_entropy     | 8.29     |
| policy_loss        | -0.11    |
| total_timesteps    | 230400   |
| value_loss         | 0.032    |
---------------------------------
Eval num_timesteps=240000, episode_reward=-1.92 +/- 0.69
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.69    |
| explained_variance | 0.36     |
| fps                | 2969     |
| nupdates           | 1900     |
| policy_entropy     | 8.23     |
| policy_loss        | -0.133   |
| total_timesteps    | 243200   |
| value_loss         | 0.0714   |
---------------------------------
Eval num_timesteps=250000, episode_reward=-1.63 +/- 0.23
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.63    |
| explained_variance | -0.432   |
| fps                | 2975     |
| nupdates           | 2000     |
| policy_entropy     | 8.25     |
| policy_loss        | -0.0475  |
| total_timesteps    | 256000   |
| value_loss         | 0.121    |
---------------------------------
Eval num_timesteps=260000, episode_reward=-1.82 +/- 0.95
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.43    |
| explained_variance | 0.211    |
| fps                | 2981     |
| nupdates           | 2100     |
| policy_entropy     | 8.27     |
| policy_loss        | 0.0121   |
| total_timesteps    | 268800   |
| value_loss         | 0.0659   |
---------------------------------
Eval num_timesteps=270000, episode_reward=-1.46 +/- 0.64
Episode length: 100.00 +/- 0.00
Eval num_timesteps=280000, episode_reward=-1.35 +/- 1.01
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.73    |
| explained_variance | -0.104   |
| fps                | 2974     |
| nupdates           | 2200     |
| policy_entropy     | 8.27     |
| policy_loss        | 0.16     |
| total_timesteps    | 281600   |
| value_loss         | 0.232    |
---------------------------------
Eval num_timesteps=290000, episode_reward=-1.34 +/- 0.68
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.91    |
| explained_variance | -0.158   |
| fps                | 2980     |
| nupdates           | 2300     |
| policy_entropy     | 8.25     |
| policy_loss        | -0.049   |
| total_timesteps    | 294400   |
| value_loss         | 0.348    |
---------------------------------
Eval num_timesteps=300000, episode_reward=-1.53 +/- 0.79
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.53    |
| explained_variance | -3.1     |
| fps                | 2985     |
| nupdates           | 2400     |
| policy_entropy     | 8.24     |
| policy_loss        | 0.0988   |
| total_timesteps    | 307200   |
| value_loss         | 0.343    |
---------------------------------
Eval num_timesteps=310000, episode_reward=-2.26 +/- 0.87
Episode length: 100.00 +/- 0.00
Eval num_timesteps=320000, episode_reward=-2.50 +/- 0.79
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.56    |
| explained_variance | -0.122   |
| fps                | 2979     |
| nupdates           | 2500     |
| policy_entropy     | 8.25     |
| policy_loss        | 0.0957   |
| total_timesteps    | 320000   |
| value_loss         | 0.178    |
---------------------------------
Eval num_timesteps=330000, episode_reward=-1.11 +/- 0.63
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.52    |
| explained_variance | -0.189   |
| fps                | 2985     |
| nupdates           | 2600     |
| policy_entropy     | 8.22     |
| policy_loss        | -0.113   |
| total_timesteps    | 332800   |
| value_loss         | 0.068    |
---------------------------------
Eval num_timesteps=340000, episode_reward=-1.91 +/- 0.58
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.7     |
| explained_variance | -0.985   |
| fps                | 2989     |
| nupdates           | 2700     |
| policy_entropy     | 8.18     |
| policy_loss        | -0.0755  |
| total_timesteps    | 345600   |
| value_loss         | 0.171    |
---------------------------------
Eval num_timesteps=350000, episode_reward=-1.38 +/- 0.44
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.75    |
| explained_variance | -0.0491  |
| fps                | 2994     |
| nupdates           | 2800     |
| policy_entropy     | 8.16     |
| policy_loss        | -0.0491  |
| total_timesteps    | 358400   |
| value_loss         | 0.125    |
---------------------------------
Eval num_timesteps=360000, episode_reward=-1.45 +/- 0.34
Episode length: 100.00 +/- 0.00
Eval num_timesteps=370000, episode_reward=-0.99 +/- 0.50
Episode length: 100.00 +/- 0.00
New best mean reward!
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.74    |
| explained_variance | -0.706   |
| fps                | 2988     |
| nupdates           | 2900     |
| policy_entropy     | 8.17     |
| policy_loss        | -0.116   |
| total_timesteps    | 371200   |
| value_loss         | 0.183    |
---------------------------------
Eval num_timesteps=380000, episode_reward=-1.88 +/- 0.57
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.78    |
| explained_variance | 0.0541   |
| fps                | 2992     |
| nupdates           | 3000     |
| policy_entropy     | 8.19     |
| policy_loss        | -0.0661  |
| total_timesteps    | 384000   |
| value_loss         | 0.313    |
---------------------------------
Eval num_timesteps=390000, episode_reward=-1.72 +/- 0.78
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.63    |
| explained_variance | -0.611   |
| fps                | 2995     |
| nupdates           | 3100     |
| policy_entropy     | 8.2      |
| policy_loss        | 0.184    |
| total_timesteps    | 396800   |
| value_loss         | 0.342    |
---------------------------------
Eval num_timesteps=400000, episode_reward=-1.61 +/- 1.06
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.82    |
| explained_variance | -2.96    |
| fps                | 2998     |
| nupdates           | 3200     |
| policy_entropy     | 8.19     |
| policy_loss        | 0.185    |
| total_timesteps    | 409600   |
| value_loss         | 0.768    |
---------------------------------
Eval num_timesteps=410000, episode_reward=-0.77 +/- 0.34
Episode length: 100.00 +/- 0.00
New best mean reward!
Eval num_timesteps=420000, episode_reward=-1.71 +/- 0.87
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.41    |
| explained_variance | -1       |
| fps                | 2994     |
| nupdates           | 3300     |
| policy_entropy     | 8.2      |
| policy_loss        | 0.034    |
| total_timesteps    | 422400   |
| value_loss         | 0.374    |
---------------------------------
Eval num_timesteps=430000, episode_reward=-1.09 +/- 1.07
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.53    |
| explained_variance | -0.00783 |
| fps                | 2998     |
| nupdates           | 3400     |
| policy_entropy     | 8.21     |
| policy_loss        | 0.0506   |
| total_timesteps    | 435200   |
| value_loss         | 0.637    |
---------------------------------
Eval num_timesteps=440000, episode_reward=-1.47 +/- 0.27
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.59    |
| explained_variance | -0.184   |
| fps                | 3002     |
| nupdates           | 3500     |
| policy_entropy     | 8.2      |
| policy_loss        | -0.0579  |
| total_timesteps    | 448000   |
| value_loss         | 0.853    |
---------------------------------
Eval num_timesteps=450000, episode_reward=-1.03 +/- 0.51
Episode length: 100.00 +/- 0.00
Eval num_timesteps=460000, episode_reward=-2.10 +/- 0.50
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.59    |
| explained_variance | -0.007   |
| fps                | 2998     |
| nupdates           | 3600     |
| policy_entropy     | 8.2      |
| policy_loss        | -0.0545  |
| total_timesteps    | 460800   |
| value_loss         | 0.769    |
---------------------------------
Eval num_timesteps=470000, episode_reward=-1.71 +/- 0.67
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.33    |
| explained_variance | -0.473   |
| fps                | 3002     |
| nupdates           | 3700     |
| policy_entropy     | 8.19     |
| policy_loss        | 0.0895   |
| total_timesteps    | 473600   |
| value_loss         | 0.706    |
---------------------------------
Eval num_timesteps=480000, episode_reward=-1.65 +/- 0.86
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.71    |
| explained_variance | -0.54    |
| fps                | 3005     |
| nupdates           | 3800     |
| policy_entropy     | 8.17     |
| policy_loss        | 0.0815   |
| total_timesteps    | 486400   |
| value_loss         | 0.401    |
---------------------------------
Eval num_timesteps=490000, episode_reward=-2.28 +/- 0.90
Episode length: 100.00 +/- 0.00
---------------------------------
| ep_len_mean        | 100      |
| ep_reward_mean     | -1.65    |
| explained_variance | -0.00771 |
| fps                | 3008     |
| nupdates           | 3900     |
| policy_entropy     | 8.13     |
| policy_loss        | -0.135   |
| total_timesteps    | 499200   |
| value_loss         | 0.117    |
---------------------------------
Saving to logs/train_0.5M_widowx_reacher-v7_KAY/acktr/widowx_reacher-v7_2
pybullet build time: May 18 2020 02:46:26
