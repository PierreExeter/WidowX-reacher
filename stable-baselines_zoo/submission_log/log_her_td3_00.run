--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   n295
  Local device: hfi1_0
--------------------------------------------------------------------------
WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/td3/td3.py:131: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/td3/policies.py:124: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/common/tf_layers.py:57: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/td3/td3.py:194: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/td3/td3.py:226: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/td3/td3.py:240: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

========== widowx_reacher-v8 ==========
Seed: 0
OrderedDict([('batch_size', 256),
             ('buffer_size', 1000000),
             ('gamma', 0.95),
             ('goal_selection_strategy', 'future'),
             ('learning_rate', 0.001),
             ('learning_starts', 1000),
             ('model_class', 'td3'),
             ('n_sampled_goal', 4),
             ('n_timesteps', 25000.0),
             ('policy', 'MlpPolicy')])
Using 1 environments
Overwriting n_timesteps with n=500000
Creating test environment
TRAINING ENV TYPE :  <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f889b8d0668>
Not replacing HERGoalEnvWrapper env by a DummyVecEnv
EVAL ENV TYPE :  <stable_baselines.her.utils.HERGoalEnvWrapper object at 0x7f8899244048>
Log path: logs/train_0.5M_widowx_reacher-v7_KAY/her/widowx_reacher-v8_1
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -2.5          |
| episodes                | 100           |
| eplenmean               | 100           |
| fps                     | 285           |
| mean 100 episode reward | -2.5          |
| n_updates               | 9000          |
| qf1_loss                | 0.00018437764 |
| qf2_loss                | 0.00017513023 |
| time_elapsed            | 34            |
| total timesteps         | 9900          |
-------------------------------------------
Eval num_timesteps=10000, episode_reward=-3.56 +/- 1.87
Episode length: 100.00 +/- 0.00
New best mean reward!
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -1.58         |
| episodes                | 200           |
| eplenmean               | 100           |
| fps                     | 271           |
| mean 100 episode reward | -1.6          |
| n_updates               | 19000         |
| qf1_loss                | 0.00020814867 |
| qf2_loss                | 0.00021336958 |
| time_elapsed            | 73            |
| total timesteps         | 19900         |
-------------------------------------------
Eval num_timesteps=20000, episode_reward=-1.48 +/- 1.16
Episode length: 100.00 +/- 0.00
New best mean reward!
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -1.34         |
| episodes                | 300           |
| eplenmean               | 100           |
| fps                     | 267           |
| mean 100 episode reward | -1.3          |
| n_updates               | 29000         |
| qf1_loss                | 0.00019031917 |
| qf2_loss                | 0.0001766838  |
| time_elapsed            | 111           |
| total timesteps         | 29900         |
-------------------------------------------
Eval num_timesteps=30000, episode_reward=-0.63 +/- 0.36
Episode length: 100.00 +/- 0.00
New best mean reward!
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -1.12         |
| episodes                | 400           |
| eplenmean               | 100           |
| fps                     | 264           |
| mean 100 episode reward | -1.1          |
| n_updates               | 39000         |
| qf1_loss                | 0.00019554095 |
| qf2_loss                | 0.00018662261 |
| time_elapsed            | 150           |
| total timesteps         | 39900         |
-------------------------------------------
Eval num_timesteps=40000, episode_reward=-1.89 +/- 0.97
Episode length: 100.00 +/- 0.00
--------------------------------------------
| current_lr              | 0.001          |
| ep_rewmean              | -0.718         |
| episodes                | 500            |
| eplenmean               | 100            |
| fps                     | 263            |
| mean 100 episode reward | -0.7           |
| n_updates               | 49000          |
| qf1_loss                | 0.000104902356 |
| qf2_loss                | 9.301923e-05   |
| time_elapsed            | 189            |
| total timesteps         | 49900          |
--------------------------------------------
Eval num_timesteps=50000, episode_reward=-3.60 +/- 4.29
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.709        |
| episodes                | 600           |
| eplenmean               | 100           |
| fps                     | 262           |
| mean 100 episode reward | -0.7          |
| n_updates               | 59000         |
| qf1_loss                | 0.00013866698 |
| qf2_loss                | 0.00012793479 |
| time_elapsed            | 228           |
| total timesteps         | 59900         |
-------------------------------------------
Eval num_timesteps=60000, episode_reward=-3.76 +/- 1.41
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.462        |
| episodes                | 700           |
| eplenmean               | 100           |
| fps                     | 261           |
| mean 100 episode reward | -0.5          |
| n_updates               | 69000         |
| qf1_loss                | 9.905015e-05  |
| qf2_loss                | 9.1122965e-05 |
| time_elapsed            | 267           |
| total timesteps         | 69900         |
-------------------------------------------
Eval num_timesteps=70000, episode_reward=-4.20 +/- 3.33
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.381        |
| episodes                | 800           |
| eplenmean               | 100           |
| fps                     | 261           |
| mean 100 episode reward | -0.4          |
| n_updates               | 79000         |
| qf1_loss                | 6.204567e-05  |
| qf2_loss                | 5.6793193e-05 |
| time_elapsed            | 305           |
| total timesteps         | 79900         |
-------------------------------------------
Eval num_timesteps=80000, episode_reward=-3.52 +/- 2.98
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.493        |
| episodes                | 900           |
| eplenmean               | 100           |
| fps                     | 261           |
| mean 100 episode reward | -0.5          |
| n_updates               | 89000         |
| qf1_loss                | 6.5689645e-05 |
| qf2_loss                | 5.996122e-05  |
| time_elapsed            | 343           |
| total timesteps         | 89900         |
-------------------------------------------
Eval num_timesteps=90000, episode_reward=-0.44 +/- 0.31
Episode length: 100.00 +/- 0.00
New best mean reward!
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.462        |
| episodes                | 1000          |
| eplenmean               | 100           |
| fps                     | 261           |
| mean 100 episode reward | -0.5          |
| n_updates               | 99000         |
| qf1_loss                | 6.4472064e-05 |
| qf2_loss                | 5.876408e-05  |
| time_elapsed            | 382           |
| total timesteps         | 99900         |
-------------------------------------------
Eval num_timesteps=100000, episode_reward=-1.93 +/- 1.43
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.353        |
| episodes                | 1100          |
| eplenmean               | 100           |
| fps                     | 260           |
| mean 100 episode reward | -0.4          |
| n_updates               | 109000        |
| qf1_loss                | 5.1998755e-05 |
| qf2_loss                | 5.0204842e-05 |
| time_elapsed            | 421           |
| total timesteps         | 109900        |
-------------------------------------------
Eval num_timesteps=110000, episode_reward=-7.21 +/- 1.89
Episode length: 100.00 +/- 0.00
------------------------------------------
| current_lr              | 0.001        |
| ep_rewmean              | -0.368       |
| episodes                | 1200         |
| eplenmean               | 100          |
| fps                     | 260          |
| mean 100 episode reward | -0.4         |
| n_updates               | 119000       |
| qf1_loss                | 7.045704e-05 |
| qf2_loss                | 6.661706e-05 |
| time_elapsed            | 460          |
| total timesteps         | 119900       |
------------------------------------------
Eval num_timesteps=120000, episode_reward=-3.44 +/- 2.61
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.321        |
| episodes                | 1300          |
| eplenmean               | 100           |
| fps                     | 259           |
| mean 100 episode reward | -0.3          |
| n_updates               | 129000        |
| qf1_loss                | 6.8942056e-05 |
| qf2_loss                | 6.5852626e-05 |
| time_elapsed            | 499           |
| total timesteps         | 129900        |
-------------------------------------------
Eval num_timesteps=130000, episode_reward=-1.95 +/- 2.97
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.33         |
| episodes                | 1400          |
| eplenmean               | 100           |
| fps                     | 259           |
| mean 100 episode reward | -0.3          |
| n_updates               | 139000        |
| qf1_loss                | 5.7363603e-05 |
| qf2_loss                | 5.6689627e-05 |
| time_elapsed            | 539           |
| total timesteps         | 139900        |
-------------------------------------------
Eval num_timesteps=140000, episode_reward=-1.62 +/- 0.78
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.27         |
| episodes                | 1500          |
| eplenmean               | 100           |
| fps                     | 259           |
| mean 100 episode reward | -0.3          |
| n_updates               | 149000        |
| qf1_loss                | 5.4338332e-05 |
| qf2_loss                | 5.1193998e-05 |
| time_elapsed            | 578           |
| total timesteps         | 149900        |
-------------------------------------------
Eval num_timesteps=150000, episode_reward=-3.26 +/- 0.99
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.261        |
| episodes                | 1600          |
| eplenmean               | 100           |
| fps                     | 258           |
| mean 100 episode reward | -0.3          |
| n_updates               | 159000        |
| qf1_loss                | 3.0553558e-05 |
| qf2_loss                | 2.7204691e-05 |
| time_elapsed            | 617           |
| total timesteps         | 159900        |
-------------------------------------------
Eval num_timesteps=160000, episode_reward=-1.98 +/- 0.72
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.281        |
| episodes                | 1700          |
| eplenmean               | 100           |
| fps                     | 258           |
| mean 100 episode reward | -0.3          |
| n_updates               | 169000        |
| qf1_loss                | 4.3414944e-05 |
| qf2_loss                | 4.301543e-05  |
| time_elapsed            | 656           |
| total timesteps         | 169900        |
-------------------------------------------
Eval num_timesteps=170000, episode_reward=-2.31 +/- 1.18
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.337        |
| episodes                | 1800          |
| eplenmean               | 100           |
| fps                     | 258           |
| mean 100 episode reward | -0.3          |
| n_updates               | 179000        |
| qf1_loss                | 6.0895803e-05 |
| qf2_loss                | 5.4761796e-05 |
| time_elapsed            | 696           |
| total timesteps         | 179900        |
-------------------------------------------
Eval num_timesteps=180000, episode_reward=-1.91 +/- 0.79
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.307        |
| episodes                | 1900          |
| eplenmean               | 100           |
| fps                     | 258           |
| mean 100 episode reward | -0.3          |
| n_updates               | 189000        |
| qf1_loss                | 6.743253e-05  |
| qf2_loss                | 6.0684877e-05 |
| time_elapsed            | 735           |
| total timesteps         | 189900        |
-------------------------------------------
Eval num_timesteps=190000, episode_reward=-1.83 +/- 0.53
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.293        |
| episodes                | 2000          |
| eplenmean               | 100           |
| fps                     | 258           |
| mean 100 episode reward | -0.3          |
| n_updates               | 199000        |
| qf1_loss                | 5.5050114e-05 |
| qf2_loss                | 5.2922733e-05 |
| time_elapsed            | 774           |
| total timesteps         | 199900        |
-------------------------------------------
Eval num_timesteps=200000, episode_reward=-2.21 +/- 1.35
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.269        |
| episodes                | 2100          |
| eplenmean               | 100           |
| fps                     | 257           |
| mean 100 episode reward | -0.3          |
| n_updates               | 209000        |
| qf1_loss                | 5.8355927e-05 |
| qf2_loss                | 6.0878927e-05 |
| time_elapsed            | 813           |
| total timesteps         | 209900        |
-------------------------------------------
Eval num_timesteps=210000, episode_reward=-2.39 +/- 2.54
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.282        |
| episodes                | 2200          |
| eplenmean               | 100           |
| fps                     | 257           |
| mean 100 episode reward | -0.3          |
| n_updates               | 219000        |
| qf1_loss                | 2.2978125e-05 |
| qf2_loss                | 1.9236766e-05 |
| time_elapsed            | 853           |
| total timesteps         | 219900        |
-------------------------------------------
Eval num_timesteps=220000, episode_reward=-1.79 +/- 1.68
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.293        |
| episodes                | 2300          |
| eplenmean               | 100           |
| fps                     | 257           |
| mean 100 episode reward | -0.3          |
| n_updates               | 229000        |
| qf1_loss                | 3.6624755e-05 |
| qf2_loss                | 3.6306497e-05 |
| time_elapsed            | 893           |
| total timesteps         | 229900        |
-------------------------------------------
Eval num_timesteps=230000, episode_reward=-5.43 +/- 4.44
Episode length: 100.00 +/- 0.00
--------------------------------------------
| current_lr              | 0.001          |
| ep_rewmean              | -0.261         |
| episodes                | 2400           |
| eplenmean               | 100            |
| fps                     | 257            |
| mean 100 episode reward | -0.3           |
| n_updates               | 239000         |
| qf1_loss                | 1.604451e-05   |
| qf2_loss                | 1.37032885e-05 |
| time_elapsed            | 931            |
| total timesteps         | 239900         |
--------------------------------------------
Eval num_timesteps=240000, episode_reward=-1.93 +/- 1.18
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.25         |
| episodes                | 2500          |
| eplenmean               | 100           |
| fps                     | 257           |
| mean 100 episode reward | -0.2          |
| n_updates               | 249000        |
| qf1_loss                | 1.8489038e-05 |
| qf2_loss                | 1.516026e-05  |
| time_elapsed            | 969           |
| total timesteps         | 249900        |
-------------------------------------------
Eval num_timesteps=250000, episode_reward=-3.68 +/- 4.99
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.267        |
| episodes                | 2600          |
| eplenmean               | 100           |
| fps                     | 257           |
| mean 100 episode reward | -0.3          |
| n_updates               | 259000        |
| qf1_loss                | 1.317839e-05  |
| qf2_loss                | 1.2591784e-05 |
| time_elapsed            | 1007          |
| total timesteps         | 259900        |
-------------------------------------------
Eval num_timesteps=260000, episode_reward=-1.48 +/- 0.84
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.203        |
| episodes                | 2700          |
| eplenmean               | 100           |
| fps                     | 258           |
| mean 100 episode reward | -0.2          |
| n_updates               | 269000        |
| qf1_loss                | 1.4029064e-05 |
| qf2_loss                | 1.3163971e-05 |
| time_elapsed            | 1045          |
| total timesteps         | 269900        |
-------------------------------------------
Eval num_timesteps=270000, episode_reward=-4.80 +/- 4.31
Episode length: 100.00 +/- 0.00
------------------------------------------
| current_lr              | 0.001        |
| ep_rewmean              | -0.269       |
| episodes                | 2800         |
| eplenmean               | 100          |
| fps                     | 258          |
| mean 100 episode reward | -0.3         |
| n_updates               | 279000       |
| qf1_loss                | 8.354391e-06 |
| qf2_loss                | 8.308638e-06 |
| time_elapsed            | 1083         |
| total timesteps         | 279900       |
------------------------------------------
Eval num_timesteps=280000, episode_reward=-3.04 +/- 3.85
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.243        |
| episodes                | 2900          |
| eplenmean               | 100           |
| fps                     | 258           |
| mean 100 episode reward | -0.2          |
| n_updates               | 289000        |
| qf1_loss                | 1.3360805e-05 |
| qf2_loss                | 1.3990445e-05 |
| time_elapsed            | 1121          |
| total timesteps         | 289900        |
-------------------------------------------
Eval num_timesteps=290000, episode_reward=-1.61 +/- 2.21
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.261        |
| episodes                | 3000          |
| eplenmean               | 100           |
| fps                     | 258           |
| mean 100 episode reward | -0.3          |
| n_updates               | 299000        |
| qf1_loss                | 5.222566e-06  |
| qf2_loss                | 4.2646034e-06 |
| time_elapsed            | 1158          |
| total timesteps         | 299900        |
-------------------------------------------
Eval num_timesteps=300000, episode_reward=-4.95 +/- 2.36
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.255        |
| episodes                | 3100          |
| eplenmean               | 100           |
| fps                     | 258           |
| mean 100 episode reward | -0.3          |
| n_updates               | 309000        |
| qf1_loss                | 8.079368e-06  |
| qf2_loss                | 7.9099555e-06 |
| time_elapsed            | 1198          |
| total timesteps         | 309900        |
-------------------------------------------
Eval num_timesteps=310000, episode_reward=-2.69 +/- 1.84
Episode length: 100.00 +/- 0.00
-----------------------------------------
| current_lr              | 0.001       |
| ep_rewmean              | -0.223      |
| episodes                | 3200        |
| eplenmean               | 100         |
| fps                     | 258         |
| mean 100 episode reward | -0.2        |
| n_updates               | 319000      |
| qf1_loss                | 7.08063e-05 |
| qf2_loss                | 7.61475e-05 |
| time_elapsed            | 1237        |
| total timesteps         | 319900      |
-----------------------------------------
Eval num_timesteps=320000, episode_reward=-2.56 +/- 2.03
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.234        |
| episodes                | 3300          |
| eplenmean               | 100           |
| fps                     | 258           |
| mean 100 episode reward | -0.2          |
| n_updates               | 329000        |
| qf1_loss                | 9.224533e-06  |
| qf2_loss                | 1.1005999e-05 |
| time_elapsed            | 1277          |
| total timesteps         | 329900        |
-------------------------------------------
Eval num_timesteps=330000, episode_reward=-3.48 +/- 3.49
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.262        |
| episodes                | 3400          |
| eplenmean               | 100           |
| fps                     | 257           |
| mean 100 episode reward | -0.3          |
| n_updates               | 339000        |
| qf1_loss                | 3.1529366e-06 |
| qf2_loss                | 2.9607918e-06 |
| time_elapsed            | 1318          |
| total timesteps         | 339900        |
-------------------------------------------
Eval num_timesteps=340000, episode_reward=-1.55 +/- 2.07
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.227        |
| episodes                | 3500          |
| eplenmean               | 100           |
| fps                     | 257           |
| mean 100 episode reward | -0.2          |
| n_updates               | 349000        |
| qf1_loss                | 1.7818986e-05 |
| qf2_loss                | 1.6892274e-05 |
| time_elapsed            | 1358          |
| total timesteps         | 349900        |
-------------------------------------------
Eval num_timesteps=350000, episode_reward=-2.38 +/- 0.73
Episode length: 100.00 +/- 0.00
------------------------------------------
| current_lr              | 0.001        |
| ep_rewmean              | -0.328       |
| episodes                | 3600         |
| eplenmean               | 100          |
| fps                     | 257          |
| mean 100 episode reward | -0.3         |
| n_updates               | 359000       |
| qf1_loss                | 6.728699e-06 |
| qf2_loss                | 6.792493e-06 |
| time_elapsed            | 1398         |
| total timesteps         | 359900       |
------------------------------------------
Eval num_timesteps=360000, episode_reward=-2.07 +/- 1.28
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.276        |
| episodes                | 3700          |
| eplenmean               | 100           |
| fps                     | 257           |
| mean 100 episode reward | -0.3          |
| n_updates               | 369000        |
| qf1_loss                | 1.1635578e-05 |
| qf2_loss                | 1.2322392e-05 |
| time_elapsed            | 1438          |
| total timesteps         | 369900        |
-------------------------------------------
Eval num_timesteps=370000, episode_reward=-0.72 +/- 0.35
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.229        |
| episodes                | 3800          |
| eplenmean               | 100           |
| fps                     | 256           |
| mean 100 episode reward | -0.2          |
| n_updates               | 379000        |
| qf1_loss                | 1.2823032e-05 |
| qf2_loss                | 1.2469613e-05 |
| time_elapsed            | 1478          |
| total timesteps         | 379900        |
-------------------------------------------
Eval num_timesteps=380000, episode_reward=-2.65 +/- 2.29
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.3          |
| episodes                | 3900          |
| eplenmean               | 100           |
| fps                     | 256           |
| mean 100 episode reward | -0.3          |
| n_updates               | 389000        |
| qf1_loss                | 7.805278e-06  |
| qf2_loss                | 6.5581035e-06 |
| time_elapsed            | 1517          |
| total timesteps         | 389900        |
-------------------------------------------
Eval num_timesteps=390000, episode_reward=-4.69 +/- 5.07
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.219        |
| episodes                | 4000          |
| eplenmean               | 100           |
| fps                     | 256           |
| mean 100 episode reward | -0.2          |
| n_updates               | 399000        |
| qf1_loss                | 5.7638654e-06 |
| qf2_loss                | 4.994897e-06  |
| time_elapsed            | 1557          |
| total timesteps         | 399900        |
-------------------------------------------
Eval num_timesteps=400000, episode_reward=-4.09 +/- 3.45
Episode length: 100.00 +/- 0.00
------------------------------------------
| current_lr              | 0.001        |
| ep_rewmean              | -0.258       |
| episodes                | 4100         |
| eplenmean               | 100          |
| fps                     | 256          |
| mean 100 episode reward | -0.3         |
| n_updates               | 409000       |
| qf1_loss                | 7.755328e-06 |
| qf2_loss                | 8.056264e-06 |
| time_elapsed            | 1596         |
| total timesteps         | 409900       |
------------------------------------------
Eval num_timesteps=410000, episode_reward=-2.33 +/- 3.53
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.292        |
| episodes                | 4200          |
| eplenmean               | 100           |
| fps                     | 256           |
| mean 100 episode reward | -0.3          |
| n_updates               | 419000        |
| qf1_loss                | 7.5708026e-06 |
| qf2_loss                | 7.65923e-06   |
| time_elapsed            | 1635          |
| total timesteps         | 419900        |
-------------------------------------------
Eval num_timesteps=420000, episode_reward=-2.12 +/- 1.55
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.263        |
| episodes                | 4300          |
| eplenmean               | 100           |
| fps                     | 256           |
| mean 100 episode reward | -0.3          |
| n_updates               | 429000        |
| qf1_loss                | 5.7040575e-06 |
| qf2_loss                | 4.5643355e-06 |
| time_elapsed            | 1674          |
| total timesteps         | 429900        |
-------------------------------------------
Eval num_timesteps=430000, episode_reward=-2.10 +/- 1.70
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.235        |
| episodes                | 4400          |
| eplenmean               | 100           |
| fps                     | 256           |
| mean 100 episode reward | -0.2          |
| n_updates               | 439000        |
| qf1_loss                | 6.716389e-06  |
| qf2_loss                | 6.6334264e-06 |
| time_elapsed            | 1714          |
| total timesteps         | 439900        |
-------------------------------------------
Eval num_timesteps=440000, episode_reward=-1.18 +/- 0.75
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.218        |
| episodes                | 4500          |
| eplenmean               | 100           |
| fps                     | 256           |
| mean 100 episode reward | -0.2          |
| n_updates               | 449000        |
| qf1_loss                | 4.809571e-06  |
| qf2_loss                | 5.1078555e-06 |
| time_elapsed            | 1754          |
| total timesteps         | 449900        |
-------------------------------------------
Eval num_timesteps=450000, episode_reward=-3.35 +/- 2.39
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.207        |
| episodes                | 4600          |
| eplenmean               | 100           |
| fps                     | 256           |
| mean 100 episode reward | -0.2          |
| n_updates               | 459000        |
| qf1_loss                | 1.0029116e-05 |
| qf2_loss                | 9.403136e-06  |
| time_elapsed            | 1793          |
| total timesteps         | 459900        |
-------------------------------------------
Eval num_timesteps=460000, episode_reward=-9.78 +/- 4.20
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.185        |
| episodes                | 4700          |
| eplenmean               | 100           |
| fps                     | 256           |
| mean 100 episode reward | -0.2          |
| n_updates               | 469000        |
| qf1_loss                | 3.833438e-06  |
| qf2_loss                | 3.5939313e-06 |
| time_elapsed            | 1831          |
| total timesteps         | 469900        |
-------------------------------------------
Eval num_timesteps=470000, episode_reward=-3.50 +/- 4.00
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.235        |
| episodes                | 4800          |
| eplenmean               | 100           |
| fps                     | 256           |
| mean 100 episode reward | -0.2          |
| n_updates               | 479000        |
| qf1_loss                | 6.548966e-06  |
| qf2_loss                | 6.6386697e-06 |
| time_elapsed            | 1869          |
| total timesteps         | 479900        |
-------------------------------------------
Eval num_timesteps=480000, episode_reward=-1.47 +/- 1.65
Episode length: 100.00 +/- 0.00
------------------------------------------
| current_lr              | 0.001        |
| ep_rewmean              | -0.207       |
| episodes                | 4900         |
| eplenmean               | 100          |
| fps                     | 256          |
| mean 100 episode reward | -0.2         |
| n_updates               | 489000       |
| qf1_loss                | 5.99087e-06  |
| qf2_loss                | 5.291008e-06 |
| time_elapsed            | 1907         |
| total timesteps         | 489900       |
------------------------------------------
Eval num_timesteps=490000, episode_reward=-1.82 +/- 1.47
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.227        |
| episodes                | 5000          |
| eplenmean               | 100           |
| fps                     | 256           |
| mean 100 episode reward | -0.2          |
| n_updates               | 499000        |
| qf1_loss                | 6.2743343e-06 |
| qf2_loss                | 5.336499e-06  |
| time_elapsed            | 1945          |
| total timesteps         | 499900        |
-------------------------------------------
Eval num_timesteps=500000, episode_reward=-6.44 +/- 4.23
Episode length: 100.00 +/- 0.00
Saving to logs/train_0.5M_widowx_reacher-v7_KAY/her/widowx_reacher-v8_1
pybullet build time: May 18 2020 02:46:26
