WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/sac/sac.py:141: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/sac/policies.py:194: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_layers.py:57: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/sac/sac.py:232: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/sac/sac.py:294: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/sac/sac.py:314: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

========== widowx_reacher-v5 ==========
Seed: 1
OrderedDict([('batch_size', 64),
             ('buffer_size', 50000),
             ('ent_coef', 'auto'),
             ('gamma', 0.99),
             ('gradient_steps', 1),
             ('learning_rate', 0.0003),
             ('learning_starts', 100),
             ('n_timesteps', 10000),
             ('policy', 'MlpPolicy'),
             ('policy_kwargs', 'None'),
             ('random_exploration', 0.0),
             ('target_entropy', 'auto'),
             ('target_update_interval', 1),
             ('tau', 0.005),
             ('train_freq', 1)])
Using 1 environments
Creating test environment
TRAINING ENV TYPE :  <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f6971bf3828>
EVAL ENV TYPE :  <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f6971c0df98>
Log path: logs/train_10K_widowx_reacher-v5/sac/widowx_reacher-v5_2
------------------------------------------
| current_lr              | 0.0003       |
| ent_coef                | 0.9415331    |
| ent_coef_loss           | -0.6064856   |
| entropy                 | 7.7051606    |
| ep_rewmean              | -4.14        |
| episodes                | 4            |
| eplenmean               | 100          |
| fps                     | 415          |
| mean 100 episode reward | -4.1         |
| n_updates               | 201          |
| policy_loss             | -5.09515     |
| qf1_loss                | 0.0013267937 |
| qf2_loss                | 0.0019588657 |
| time_elapsed            | 0            |
| total timesteps         | 300          |
| value_loss              | 0.12735456   |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ent_coef                | 0.8349651    |
| ent_coef_loss           | -1.8155855   |
| entropy                 | 7.900057     |
| ep_rewmean              | -3.88        |
| episodes                | 8            |
| eplenmean               | 100          |
| fps                     | 448          |
| mean 100 episode reward | -3.9         |
| n_updates               | 601          |
| policy_loss             | -10.085836   |
| qf1_loss                | 0.0094257295 |
| qf2_loss                | 0.013442297  |
| time_elapsed            | 1            |
| total timesteps         | 700          |
| value_loss              | 0.058582094  |
------------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.74055064 |
| ent_coef_loss           | -3.026888  |
| entropy                 | 7.910057   |
| ep_rewmean              | -3.63      |
| episodes                | 12         |
| eplenmean               | 100        |
| fps                     | 449        |
| mean 100 episode reward | -3.6       |
| n_updates               | 1001       |
| policy_loss             | -14.829444 |
| qf1_loss                | 2.135171   |
| qf2_loss                | 2.185663   |
| time_elapsed            | 2          |
| total timesteps         | 1100       |
| value_loss              | 0.08566352 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.65691966 |
| ent_coef_loss           | -4.22283   |
| entropy                 | 7.992236   |
| ep_rewmean              | -3.36      |
| episodes                | 16         |
| eplenmean               | 100        |
| fps                     | 444        |
| mean 100 episode reward | -3.4       |
| n_updates               | 1401       |
| policy_loss             | -19.139935 |
| qf1_loss                | 2.0329537  |
| qf2_loss                | 2.0672522  |
| time_elapsed            | 3          |
| total timesteps         | 1500       |
| value_loss              | 0.13728334 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.5828367  |
| ent_coef_loss           | -5.3726263 |
| entropy                 | 8.051565   |
| ep_rewmean              | -3.18      |
| episodes                | 20         |
| eplenmean               | 100        |
| fps                     | 445        |
| mean 100 episode reward | -3.2       |
| n_updates               | 1801       |
| policy_loss             | -22.694197 |
| qf1_loss                | 5.7752457  |
| qf2_loss                | 5.855791   |
| time_elapsed            | 4          |
| total timesteps         | 1900       |
| value_loss              | 0.14535742 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.5171906  |
| ent_coef_loss           | -6.6652126 |
| entropy                 | 7.957123   |
| ep_rewmean              | -3.08      |
| episodes                | 24         |
| eplenmean               | 100        |
| fps                     | 449        |
| mean 100 episode reward | -3.1       |
| n_updates               | 2201       |
| policy_loss             | -25.32156  |
| qf1_loss                | 0.2629618  |
| qf2_loss                | 0.26813424 |
| time_elapsed            | 5          |
| total timesteps         | 2300       |
| value_loss              | 0.11282693 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.458922   |
| ent_coef_loss           | -7.8435936 |
| entropy                 | 8.038906   |
| ep_rewmean              | -3.07      |
| episodes                | 28         |
| eplenmean               | 100        |
| fps                     | 452        |
| mean 100 episode reward | -3.1       |
| n_updates               | 2601       |
| policy_loss             | -28.481293 |
| qf1_loss                | 7.3412213  |
| qf2_loss                | 7.3880267  |
| time_elapsed            | 5          |
| total timesteps         | 2700       |
| value_loss              | 0.17657225 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.40726417 |
| ent_coef_loss           | -9.002935  |
| entropy                 | 8.215914   |
| ep_rewmean              | -3.07      |
| episodes                | 32         |
| eplenmean               | 100        |
| fps                     | 453        |
| mean 100 episode reward | -3.1       |
| n_updates               | 3001       |
| policy_loss             | -30.040737 |
| qf1_loss                | 5.671461   |
| qf2_loss                | 5.54208    |
| time_elapsed            | 6          |
| total timesteps         | 3100       |
| value_loss              | 0.15680578 |
----------------------------------------
-----------------------------------------
| current_lr              | 0.0003      |
| ent_coef                | 0.36162362  |
| ent_coef_loss           | -10.0299015 |
| entropy                 | 8.039727    |
| ep_rewmean              | -3.2        |
| episodes                | 36          |
| eplenmean               | 100         |
| fps                     | 454         |
| mean 100 episode reward | -3.2        |
| n_updates               | 3401        |
| policy_loss             | -31.783415  |
| qf1_loss                | 6.671872    |
| qf2_loss                | 6.5691905   |
| time_elapsed            | 7           |
| total timesteps         | 3500        |
| value_loss              | 0.15863562  |
-----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.32119012 |
| ent_coef_loss           | -11.221558 |
| entropy                 | 8.146827   |
| ep_rewmean              | -3.14      |
| episodes                | 40         |
| eplenmean               | 100        |
| fps                     | 453        |
| mean 100 episode reward | -3.1       |
| n_updates               | 3801       |
| policy_loss             | -32.813828 |
| qf1_loss                | 5.903595   |
| qf2_loss                | 6.0738053  |
| time_elapsed            | 8          |
| total timesteps         | 3900       |
| value_loss              | 0.2510454  |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.2852859  |
| ent_coef_loss           | -12.261347 |
| entropy                 | 8.09624    |
| ep_rewmean              | -3.16      |
| episodes                | 44         |
| eplenmean               | 100        |
| fps                     | 455        |
| mean 100 episode reward | -3.2       |
| n_updates               | 4201       |
| policy_loss             | -33.67913  |
| qf1_loss                | 6.2274175  |
| qf2_loss                | 5.316034   |
| time_elapsed            | 9          |
| total timesteps         | 4300       |
| value_loss              | 0.2512707  |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.25370803 |
| ent_coef_loss           | -13.359509 |
| entropy                 | 8.251619   |
| ep_rewmean              | -3.14      |
| episodes                | 48         |
| eplenmean               | 100        |
| fps                     | 452        |
| mean 100 episode reward | -3.1       |
| n_updates               | 4601       |
| policy_loss             | -34.161545 |
| qf1_loss                | 6.4030714  |
| qf2_loss                | 6.682552   |
| time_elapsed            | 10         |
| total timesteps         | 4700       |
| value_loss              | 0.18677235 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.22574243 |
| ent_coef_loss           | -13.210163 |
| entropy                 | 7.7638645  |
| ep_rewmean              | -3.17      |
| episodes                | 52         |
| eplenmean               | 100        |
| fps                     | 452        |
| mean 100 episode reward | -3.2       |
| n_updates               | 5001       |
| policy_loss             | -35.194256 |
| qf1_loss                | 7.90292    |
| qf2_loss                | 8.165975   |
| time_elapsed            | 11         |
| total timesteps         | 5100       |
| value_loss              | 0.204492   |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.2013475  |
| ent_coef_loss           | -14.576975 |
| entropy                 | 7.8131275  |
| ep_rewmean              | -3.21      |
| episodes                | 56         |
| eplenmean               | 100        |
| fps                     | 452        |
| mean 100 episode reward | -3.2       |
| n_updates               | 5401       |
| policy_loss             | -35.231155 |
| qf1_loss                | 0.6175094  |
| qf2_loss                | 0.57187545 |
| time_elapsed            | 12         |
| total timesteps         | 5500       |
| value_loss              | 0.17242068 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.18043734 |
| ent_coef_loss           | -13.791358 |
| entropy                 | 7.4000998  |
| ep_rewmean              | -3.28      |
| episodes                | 60         |
| eplenmean               | 100        |
| fps                     | 452        |
| mean 100 episode reward | -3.3       |
| n_updates               | 5801       |
| policy_loss             | -35.159393 |
| qf1_loss                | 0.23665631 |
| qf2_loss                | 0.2515372  |
| time_elapsed            | 13         |
| total timesteps         | 5900       |
| value_loss              | 0.26332942 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.16256066 |
| ent_coef_loss           | -10.875545 |
| entropy                 | 6.7281914  |
| ep_rewmean              | -3.29      |
| episodes                | 64         |
| eplenmean               | 100        |
| fps                     | 450        |
| mean 100 episode reward | -3.3       |
| n_updates               | 6201       |
| policy_loss             | -36.07204  |
| qf1_loss                | 0.15527377 |
| qf2_loss                | 0.07833073 |
| time_elapsed            | 13         |
| total timesteps         | 6300       |
| value_loss              | 0.8519777  |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.14732    |
| ent_coef_loss           | -10.623145 |
| entropy                 | 6.3170853  |
| ep_rewmean              | -3.25      |
| episodes                | 68         |
| eplenmean               | 100        |
| fps                     | 451        |
| mean 100 episode reward | -3.2       |
| n_updates               | 6601       |
| policy_loss             | -35.470417 |
| qf1_loss                | 0.14548633 |
| qf2_loss                | 0.13202982 |
| time_elapsed            | 14         |
| total timesteps         | 6700       |
| value_loss              | 0.16562247 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.13577236 |
| ent_coef_loss           | -9.335459  |
| entropy                 | 5.4984827  |
| ep_rewmean              | -3.2       |
| episodes                | 72         |
| eplenmean               | 100        |
| fps                     | 451        |
| mean 100 episode reward | -3.2       |
| n_updates               | 7001       |
| policy_loss             | -34.750458 |
| qf1_loss                | 0.15719701 |
| qf2_loss                | 0.1210603  |
| time_elapsed            | 15         |
| total timesteps         | 7100       |
| value_loss              | 0.16516957 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.12590961 |
| ent_coef_loss           | -6.7931213 |
| entropy                 | 4.66852    |
| ep_rewmean              | -3.17      |
| episodes                | 76         |
| eplenmean               | 100        |
| fps                     | 451        |
| mean 100 episode reward | -3.2       |
| n_updates               | 7401       |
| policy_loss             | -34.174732 |
| qf1_loss                | 5.318397   |
| qf2_loss                | 5.4392447  |
| time_elapsed            | 16         |
| total timesteps         | 7500       |
| value_loss              | 0.14036487 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.11542741 |
| ent_coef_loss           | -8.029001  |
| entropy                 | 5.0883307  |
| ep_rewmean              | -3.13      |
| episodes                | 80         |
| eplenmean               | 100        |
| fps                     | 451        |
| mean 100 episode reward | -3.1       |
| n_updates               | 7801       |
| policy_loss             | -31.727682 |
| qf1_loss                | 3.1813543  |
| qf2_loss                | 2.5662854  |
| time_elapsed            | 17         |
| total timesteps         | 7900       |
| value_loss              | 0.11875224 |
----------------------------------------
-----------------------------------------
| current_lr              | 0.0003      |
| ent_coef                | 0.10421662  |
| ent_coef_loss           | -12.336117  |
| entropy                 | 4.963955    |
| ep_rewmean              | -3.08       |
| episodes                | 84          |
| eplenmean               | 100         |
| fps                     | 451         |
| mean 100 episode reward | -3.1        |
| n_updates               | 8201        |
| policy_loss             | -33.02516   |
| qf1_loss                | 0.16035882  |
| qf2_loss                | 0.0808835   |
| time_elapsed            | 18          |
| total timesteps         | 8300        |
| value_loss              | 0.075602956 |
-----------------------------------------
-----------------------------------------
| current_lr              | 0.0003      |
| ent_coef                | 0.09379252  |
| ent_coef_loss           | -11.416938  |
| entropy                 | 4.4152403   |
| ep_rewmean              | -3.04       |
| episodes                | 88          |
| eplenmean               | 100         |
| fps                     | 451         |
| mean 100 episode reward | -3          |
| n_updates               | 8601        |
| policy_loss             | -32.6249    |
| qf1_loss                | 0.14945507  |
| qf2_loss                | 0.078351215 |
| time_elapsed            | 19          |
| total timesteps         | 8700        |
| value_loss              | 0.10273171  |
-----------------------------------------
-----------------------------------------
| current_lr              | 0.0003      |
| ent_coef                | 0.08509854  |
| ent_coef_loss           | -9.670866   |
| entropy                 | 4.1185713   |
| ep_rewmean              | -3.03       |
| episodes                | 92          |
| eplenmean               | 100         |
| fps                     | 450         |
| mean 100 episode reward | -3          |
| n_updates               | 9001        |
| policy_loss             | -31.785397  |
| qf1_loss                | 0.15915889  |
| qf2_loss                | 0.092100926 |
| time_elapsed            | 20          |
| total timesteps         | 9100        |
| value_loss              | 0.12374142  |
-----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.07721495 |
| ent_coef_loss           | -9.533482  |
| entropy                 | 3.7499511  |
| ep_rewmean              | -3.09      |
| episodes                | 96         |
| eplenmean               | 100        |
| fps                     | 450        |
| mean 100 episode reward | -3.1       |
| n_updates               | 9401       |
| policy_loss             | -31.465569 |
| qf1_loss                | 0.15797263 |
| qf2_loss                | 0.10093367 |
| time_elapsed            | 21         |
| total timesteps         | 9500       |
| value_loss              | 0.12481297 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.07065326 |
| ent_coef_loss           | -8.545372  |
| entropy                 | 3.4155512  |
| ep_rewmean              | -3.11      |
| episodes                | 100        |
| eplenmean               | 100        |
| fps                     | 450        |
| mean 100 episode reward | -3.1       |
| n_updates               | 9801       |
| policy_loss             | -31.81572  |
| qf1_loss                | 6.998705   |
| qf2_loss                | 7.460523   |
| time_elapsed            | 21         |
| total timesteps         | 9900       |
| value_loss              | 0.23575461 |
----------------------------------------
/home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/callbacks.py:285: UserWarning: Training and eval env are not of the same type<stable_baselines.common.base_class._UnvecWrapper object at 0x7f6971c13128> != <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f6971c0df98>
  "{} != {}".format(self.training_env, self.eval_env))
Eval num_timesteps=10000, episode_reward=-2.60 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
Saving to logs/train_10K_widowx_reacher-v5/sac/widowx_reacher-v5_2
pybullet build time: May 18 2020 02:46:26
