WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/sac/sac.py:141: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/sac/policies.py:194: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_layers.py:57: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/sac/sac.py:254: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/sac/sac.py:294: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/sac/sac.py:314: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

========== widowx_reacher-v5 ==========
Seed: 1
OrderedDict([('batch_size', 32),
             ('buffer_size', 100000),
             ('ent_coef', 0.0001),
             ('gamma', 0.95),
             ('gradient_steps', 1),
             ('learning_rate', 0.0014704926021044404),
             ('learning_starts', 0),
             ('n_timesteps', 60000.0),
             ('policy', 'MlpPolicy'),
             ('policy_kwargs', 'dict(layers=[256, 256])'),
             ('train_freq', 1)])
Using 1 environments
Overwriting n_timesteps with n=10000
Creating test environment
TRAINING ENV TYPE :  <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7fab946201d0>
EVAL ENV TYPE :  <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7fab94624cc0>
Log path: logs/train_0.01M_widowx_reacher-v5/sac/widowx_reacher-v5_2
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -14.806944    |
| ep_rewmean              | -2.74         |
| episodes                | 4             |
| eplenmean               | 100           |
| fps                     | 305           |
| mean 100 episode reward | -2.7          |
| n_updates               | 269           |
| policy_loss             | 0.14025486    |
| qf1_loss                | 7.675673e-06  |
| qf2_loss                | 1.6919936e-05 |
| time_elapsed            | 0             |
| total timesteps         | 300           |
| value_loss              | 2.5629599e-05 |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -8.991549     |
| ep_rewmean              | -1.73         |
| episodes                | 8             |
| eplenmean               | 100           |
| fps                     | 349           |
| mean 100 episode reward | -1.7          |
| n_updates               | 669           |
| policy_loss             | 0.14271224    |
| qf1_loss                | 2.8834882e-05 |
| qf2_loss                | 6.0110084e-05 |
| time_elapsed            | 1             |
| total timesteps         | 700           |
| value_loss              | 6.980142e-05  |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -7.7299013    |
| ep_rewmean              | -1.76         |
| episodes                | 12            |
| eplenmean               | 100           |
| fps                     | 362           |
| mean 100 episode reward | -1.8          |
| n_updates               | 1069          |
| policy_loss             | 0.17542881    |
| qf1_loss                | 2.7208966e-05 |
| qf2_loss                | 4.8152277e-05 |
| time_elapsed            | 3             |
| total timesteps         | 1100          |
| value_loss              | 3.0123938e-05 |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -4.7837014    |
| ep_rewmean              | -1.88         |
| episodes                | 16            |
| eplenmean               | 100           |
| fps                     | 368           |
| mean 100 episode reward | -1.9          |
| n_updates               | 1469          |
| policy_loss             | 0.20627625    |
| qf1_loss                | 6.114188e-05  |
| qf2_loss                | 5.3843338e-05 |
| time_elapsed            | 4             |
| total timesteps         | 1500          |
| value_loss              | 0.00018104694 |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -5.2609906    |
| ep_rewmean              | -1.76         |
| episodes                | 20            |
| eplenmean               | 100           |
| fps                     | 367           |
| mean 100 episode reward | -1.8          |
| n_updates               | 1869          |
| policy_loss             | 0.22409758    |
| qf1_loss                | 3.7583348e-05 |
| qf2_loss                | 5.438185e-05  |
| time_elapsed            | 5             |
| total timesteps         | 1900          |
| value_loss              | 0.00014494808 |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -7.199691     |
| ep_rewmean              | -1.56         |
| episodes                | 24            |
| eplenmean               | 100           |
| fps                     | 371           |
| mean 100 episode reward | -1.6          |
| n_updates               | 2269          |
| policy_loss             | 0.21238914    |
| qf1_loss                | 4.0359148e-05 |
| qf2_loss                | 3.8224356e-05 |
| time_elapsed            | 6             |
| total timesteps         | 2300          |
| value_loss              | 0.00016633458 |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -3.979167     |
| ep_rewmean              | -1.43         |
| episodes                | 28            |
| eplenmean               | 100           |
| fps                     | 370           |
| mean 100 episode reward | -1.4          |
| n_updates               | 2669          |
| policy_loss             | 0.2075179     |
| qf1_loss                | 2.0711854e-05 |
| qf2_loss                | 7.5366142e-06 |
| time_elapsed            | 7             |
| total timesteps         | 2700          |
| value_loss              | 3.767448e-05  |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -4.0001082    |
| ep_rewmean              | -1.28         |
| episodes                | 32            |
| eplenmean               | 100           |
| fps                     | 369           |
| mean 100 episode reward | -1.3          |
| n_updates               | 3069          |
| policy_loss             | 0.20490861    |
| qf1_loss                | 3.2146672e-05 |
| qf2_loss                | 4.8317113e-05 |
| time_elapsed            | 8             |
| total timesteps         | 3100          |
| value_loss              | 2.2987744e-05 |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -3.890114     |
| ep_rewmean              | -1.22         |
| episodes                | 36            |
| eplenmean               | 100           |
| fps                     | 368           |
| mean 100 episode reward | -1.2          |
| n_updates               | 3469          |
| policy_loss             | 0.17857337    |
| qf1_loss                | 1.2896509e-05 |
| qf2_loss                | 1.2513167e-05 |
| time_elapsed            | 9             |
| total timesteps         | 3500          |
| value_loss              | 3.9261344e-05 |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -4.3145895    |
| ep_rewmean              | -1.12         |
| episodes                | 40            |
| eplenmean               | 100           |
| fps                     | 368           |
| mean 100 episode reward | -1.1          |
| n_updates               | 3869          |
| policy_loss             | 0.1596503     |
| qf1_loss                | 2.1410262e-05 |
| qf2_loss                | 2.0543292e-05 |
| time_elapsed            | 10            |
| total timesteps         | 3900          |
| value_loss              | 6.319023e-05  |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -4.8559084    |
| ep_rewmean              | -1.04         |
| episodes                | 44            |
| eplenmean               | 100           |
| fps                     | 368           |
| mean 100 episode reward | -1            |
| n_updates               | 4269          |
| policy_loss             | 0.16348012    |
| qf1_loss                | 1.8584691e-05 |
| qf2_loss                | 1.1646842e-05 |
| time_elapsed            | 11            |
| total timesteps         | 4300          |
| value_loss              | 4.444582e-05  |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -4.689503     |
| ep_rewmean              | -0.971        |
| episodes                | 48            |
| eplenmean               | 100           |
| fps                     | 368           |
| mean 100 episode reward | -1            |
| n_updates               | 4669          |
| policy_loss             | 0.14356996    |
| qf1_loss                | 1.6662309e-05 |
| qf2_loss                | 9.036237e-06  |
| time_elapsed            | 12            |
| total timesteps         | 4700          |
| value_loss              | 6.031496e-05  |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -4.2193484    |
| ep_rewmean              | -0.939        |
| episodes                | 52            |
| eplenmean               | 100           |
| fps                     | 367           |
| mean 100 episode reward | -0.9          |
| n_updates               | 5069          |
| policy_loss             | 0.13927865    |
| qf1_loss                | 5.4689855e-05 |
| qf2_loss                | 4.839855e-05  |
| time_elapsed            | 13            |
| total timesteps         | 5100          |
| value_loss              | 2.045884e-05  |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -4.291621     |
| ep_rewmean              | -0.888        |
| episodes                | 56            |
| eplenmean               | 100           |
| fps                     | 363           |
| mean 100 episode reward | -0.9          |
| n_updates               | 5469          |
| policy_loss             | 0.12573895    |
| qf1_loss                | 1.5820493e-05 |
| qf2_loss                | 1.20739e-05   |
| time_elapsed            | 15            |
| total timesteps         | 5500          |
| value_loss              | 3.244418e-05  |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -2.5930724    |
| ep_rewmean              | -0.861        |
| episodes                | 60            |
| eplenmean               | 100           |
| fps                     | 364           |
| mean 100 episode reward | -0.9          |
| n_updates               | 5869          |
| policy_loss             | 0.13743696    |
| qf1_loss                | 1.6432858e-05 |
| qf2_loss                | 1.7169292e-05 |
| time_elapsed            | 16            |
| total timesteps         | 5900          |
| value_loss              | 1.5716698e-05 |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -1.4262471    |
| ep_rewmean              | -0.819        |
| episodes                | 64            |
| eplenmean               | 100           |
| fps                     | 363           |
| mean 100 episode reward | -0.8          |
| n_updates               | 6269          |
| policy_loss             | 0.13093504    |
| qf1_loss                | 0.00015351704 |
| qf2_loss                | 9.17947e-05   |
| time_elapsed            | 17            |
| total timesteps         | 6300          |
| value_loss              | 5.3756587e-05 |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -1.790025     |
| ep_rewmean              | -0.786        |
| episodes                | 68            |
| eplenmean               | 100           |
| fps                     | 362           |
| mean 100 episode reward | -0.8          |
| n_updates               | 6669          |
| policy_loss             | 0.114980966   |
| qf1_loss                | 7.9604724e-05 |
| qf2_loss                | 5.1279414e-05 |
| time_elapsed            | 18            |
| total timesteps         | 6700          |
| value_loss              | 4.144172e-06  |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -2.2911544    |
| ep_rewmean              | -0.753        |
| episodes                | 72            |
| eplenmean               | 100           |
| fps                     | 361           |
| mean 100 episode reward | -0.8          |
| n_updates               | 7069          |
| policy_loss             | 0.08914764    |
| qf1_loss                | 1.0841941e-05 |
| qf2_loss                | 8.713667e-06  |
| time_elapsed            | 19            |
| total timesteps         | 7100          |
| value_loss              | 1.1707939e-05 |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -1.318919     |
| ep_rewmean              | -0.724        |
| episodes                | 76            |
| eplenmean               | 100           |
| fps                     | 362           |
| mean 100 episode reward | -0.7          |
| n_updates               | 7469          |
| policy_loss             | 0.121432975   |
| qf1_loss                | 1.8685594e-05 |
| qf2_loss                | 2.8974015e-05 |
| time_elapsed            | 20            |
| total timesteps         | 7500          |
| value_loss              | 1.8702964e-05 |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -1.1663904    |
| ep_rewmean              | -0.703        |
| episodes                | 80            |
| eplenmean               | 100           |
| fps                     | 362           |
| mean 100 episode reward | -0.7          |
| n_updates               | 7869          |
| policy_loss             | 0.09367761    |
| qf1_loss                | 3.3953736e-06 |
| qf2_loss                | 3.4753803e-06 |
| time_elapsed            | 21            |
| total timesteps         | 7900          |
| value_loss              | 2.743197e-06  |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -0.347353     |
| ep_rewmean              | -0.681        |
| episodes                | 84            |
| eplenmean               | 100           |
| fps                     | 363           |
| mean 100 episode reward | -0.7          |
| n_updates               | 8269          |
| policy_loss             | 0.08457795    |
| qf1_loss                | 1.0741847e-05 |
| qf2_loss                | 1.3445464e-05 |
| time_elapsed            | 22            |
| total timesteps         | 8300          |
| value_loss              | 9.928568e-06  |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | 0.35955745    |
| ep_rewmean              | -0.665        |
| episodes                | 88            |
| eplenmean               | 100           |
| fps                     | 363           |
| mean 100 episode reward | -0.7          |
| n_updates               | 8669          |
| policy_loss             | 0.09139344    |
| qf1_loss                | 4.6292816e-06 |
| qf2_loss                | 7.0211586e-06 |
| time_elapsed            | 23            |
| total timesteps         | 8700          |
| value_loss              | 4.416484e-06  |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | 0.31238577    |
| ep_rewmean              | -0.657        |
| episodes                | 92            |
| eplenmean               | 100           |
| fps                     | 364           |
| mean 100 episode reward | -0.7          |
| n_updates               | 9069          |
| policy_loss             | 0.08377789    |
| qf1_loss                | 2.3262543e-05 |
| qf2_loss                | 1.5481903e-05 |
| time_elapsed            | 24            |
| total timesteps         | 9100          |
| value_loss              | 2.3515815e-05 |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -0.79806376   |
| ep_rewmean              | -0.653        |
| episodes                | 96            |
| eplenmean               | 100           |
| fps                     | 363           |
| mean 100 episode reward | -0.7          |
| n_updates               | 9469          |
| policy_loss             | 0.078816265   |
| qf1_loss                | 5.1838848e-05 |
| qf2_loss                | 4.887618e-05  |
| time_elapsed            | 26            |
| total timesteps         | 9500          |
| value_loss              | 2.0324933e-06 |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -1.153149     |
| ep_rewmean              | -0.657        |
| episodes                | 100           |
| eplenmean               | 100           |
| fps                     | 364           |
| mean 100 episode reward | -0.7          |
| n_updates               | 9869          |
| policy_loss             | 0.083674334   |
| qf1_loss                | 2.4894912e-06 |
| qf2_loss                | 2.9848204e-06 |
| time_elapsed            | 27            |
| total timesteps         | 9900          |
| value_loss              | 1.4740929e-05 |
-------------------------------------------
/home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/callbacks.py:285: UserWarning: Training and eval env are not of the same type<stable_baselines.common.base_class._UnvecWrapper object at 0x7fab94624d30> != <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7fab94624cc0>
  "{} != {}".format(self.training_env, self.eval_env))
Eval num_timesteps=10000, episode_reward=-0.71 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
Saving to logs/train_0.01M_widowx_reacher-v5/sac/widowx_reacher-v5_2
pybullet build time: May 18 2020 02:46:26
