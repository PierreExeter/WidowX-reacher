--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   n295
  Local device: hfi1_0
--------------------------------------------------------------------------
WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/ddpg/ddpg.py:332: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/ddpg/policies.py:134: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/ddpg/policies.py:136: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/ddpg/ddpg.py:412: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/ddpg/ddpg.py:452: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

========== widowx_reacher-v7 ==========
Seed: 1
OrderedDict([('memory_limit', 50000),
             ('n_timesteps', 200000.0),
             ('noise_std', 0.1),
             ('noise_type', 'ornstein-uhlenbeck'),
             ('policy', 'MlpPolicy')])
Using 1 environments
Overwriting n_timesteps with n=500000
Creating test environment
TRAINING ENV TYPE :  <Monitor<TimeLimit<WidowxEnv<widowx_reacher-v7>>>>
EVAL ENV TYPE :  <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7fa26c8601d0>
Applying ornstein-uhlenbeck noise with std 0.1
Log path: logs/train_0.5M_widowx_reacher-v7_KAY/ddpg/widowx_reacher-v7_2
Eval num_timesteps=10000, episode_reward=-4.09 +/- 3.17
Episode length: 100.00 +/- 0.00
New best mean reward!
--------------------------------------
| reference_Q_mean        | -0.0697  |
| reference_Q_std         | 0.0432   |
| reference_action_mean   | -0.124   |
| reference_action_std    | 0.467    |
| reference_actor_Q_mean  | -0.0218  |
| reference_actor_Q_std   | 0.0723   |
| rollout/Q_mean          | -0.0459  |
| rollout/actions_mean    | -0.207   |
| rollout/actions_std     | 0.358    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 100      |
| rollout/return          | -3.2     |
| rollout/return_history  | -3.2     |
| total/duration          | 25.8     |
| total/episodes          | 100      |
| total/epochs            | 1        |
| total/steps             | 9998     |
| total/steps_per_second  | 387      |
| train/loss_actor        | 0.0154   |
| train/loss_critic       | 0.000149 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=20000, episode_reward=-1.99 +/- 0.88
Episode length: 100.00 +/- 0.00
New best mean reward!
--------------------------------------
| reference_Q_mean        | 0.0143   |
| reference_Q_std         | 0.0862   |
| reference_action_mean   | -0.171   |
| reference_action_std    | 0.294    |
| reference_actor_Q_mean  | 0.0529   |
| reference_actor_Q_std   | 0.0887   |
| rollout/Q_mean          | 0.00291  |
| rollout/actions_mean    | -0.203   |
| rollout/actions_std     | 0.334    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 200      |
| rollout/return          | -3.22    |
| rollout/return_history  | -3.23    |
| total/duration          | 52       |
| total/episodes          | 200      |
| total/epochs            | 1        |
| total/steps             | 19998    |
| total/steps_per_second  | 385      |
| train/loss_actor        | -0.063   |
| train/loss_critic       | 0.000504 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=30000, episode_reward=-3.59 +/- 0.89
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 0.0468   |
| reference_Q_std         | 0.0958   |
| reference_action_mean   | -0.165   |
| reference_action_std    | 0.347    |
| reference_actor_Q_mean  | 0.0914   |
| reference_actor_Q_std   | 0.117    |
| rollout/Q_mean          | 0.0126   |
| rollout/actions_mean    | -0.226   |
| rollout/actions_std     | 0.319    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 300      |
| rollout/return          | -3.35    |
| rollout/return_history  | -3.62    |
| total/duration          | 78.1     |
| total/episodes          | 300      |
| total/epochs            | 1        |
| total/steps             | 29998    |
| total/steps_per_second  | 384      |
| train/loss_actor        | -0.0406  |
| train/loss_critic       | 0.000507 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=40000, episode_reward=-1.45 +/- 0.34
Episode length: 100.00 +/- 0.00
New best mean reward!
--------------------------------------
| reference_Q_mean        | 0.0867   |
| reference_Q_std         | 0.122    |
| reference_action_mean   | 0.0498   |
| reference_action_std    | 0.582    |
| reference_actor_Q_mean  | 0.16     |
| reference_actor_Q_std   | 0.136    |
| rollout/Q_mean          | 0.0558   |
| rollout/actions_mean    | -0.22    |
| rollout/actions_std     | 0.383    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 400      |
| rollout/return          | -3.06    |
| rollout/return_history  | -2.21    |
| total/duration          | 104      |
| total/episodes          | 400      |
| total/epochs            | 1        |
| total/steps             | 39998    |
| total/steps_per_second  | 385      |
| train/loss_actor        | -0.214   |
| train/loss_critic       | 0.00131  |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=50000, episode_reward=-0.83 +/- 0.38
Episode length: 100.00 +/- 0.00
New best mean reward!
--------------------------------------
| reference_Q_mean        | 0.181    |
| reference_Q_std         | 0.171    |
| reference_action_mean   | -0.0248  |
| reference_action_std    | 0.524    |
| reference_actor_Q_mean  | 0.23     |
| reference_actor_Q_std   | 0.19     |
| rollout/Q_mean          | 0.176    |
| rollout/actions_mean    | -0.209   |
| rollout/actions_std     | 0.51     |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 500      |
| rollout/return          | -2.8     |
| rollout/return_history  | -1.75    |
| total/duration          | 130      |
| total/episodes          | 500      |
| total/epochs            | 1        |
| total/steps             | 49998    |
| total/steps_per_second  | 385      |
| train/loss_actor        | -0.398   |
| train/loss_critic       | 0.00235  |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=60000, episode_reward=-1.82 +/- 0.39
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 0.307    |
| reference_Q_std         | 0.258    |
| reference_action_mean   | -0.0447  |
| reference_action_std    | 0.496    |
| reference_actor_Q_mean  | 0.374    |
| reference_actor_Q_std   | 0.281    |
| rollout/Q_mean          | 0.259    |
| rollout/actions_mean    | -0.195   |
| rollout/actions_std     | 0.584    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 600      |
| rollout/return          | -2.76    |
| rollout/return_history  | -2.54    |
| total/duration          | 156      |
| total/episodes          | 600      |
| total/epochs            | 1        |
| total/steps             | 59998    |
| total/steps_per_second  | 385      |
| train/loss_actor        | -0.565   |
| train/loss_critic       | 0.00395  |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=70000, episode_reward=-2.57 +/- 1.14
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 0.288    |
| reference_Q_std         | 0.305    |
| reference_action_mean   | -0.185   |
| reference_action_std    | 0.491    |
| reference_actor_Q_mean  | 0.295    |
| reference_actor_Q_std   | 0.335    |
| rollout/Q_mean          | 0.31     |
| rollout/actions_mean    | -0.189   |
| rollout/actions_std     | 0.619    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 700      |
| rollout/return          | -2.65    |
| rollout/return_history  | -1.97    |
| total/duration          | 182      |
| total/episodes          | 700      |
| total/epochs            | 1        |
| total/steps             | 69998    |
| total/steps_per_second  | 385      |
| train/loss_actor        | -0.551   |
| train/loss_critic       | 0.00243  |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=80000, episode_reward=-1.22 +/- 1.01
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 0.367    |
| reference_Q_std         | 0.343    |
| reference_action_mean   | -0.42    |
| reference_action_std    | 0.658    |
| reference_actor_Q_mean  | 0.424    |
| reference_actor_Q_std   | 0.398    |
| rollout/Q_mean          | 0.456    |
| rollout/actions_mean    | -0.209   |
| rollout/actions_std     | 0.644    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 800      |
| rollout/return          | -2.56    |
| rollout/return_history  | -1.93    |
| total/duration          | 208      |
| total/episodes          | 800      |
| total/epochs            | 1        |
| total/steps             | 79998    |
| total/steps_per_second  | 385      |
| train/loss_actor        | -0.683   |
| train/loss_critic       | 0.00606  |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=90000, episode_reward=-5.42 +/- 1.26
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 0.43     |
| reference_Q_std         | 0.428    |
| reference_action_mean   | -0.353   |
| reference_action_std    | 0.786    |
| reference_actor_Q_mean  | 0.519    |
| reference_actor_Q_std   | 0.418    |
| rollout/Q_mean          | 0.617    |
| rollout/actions_mean    | -0.232   |
| rollout/actions_std     | 0.671    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 900      |
| rollout/return          | -2.58    |
| rollout/return_history  | -2.74    |
| total/duration          | 234      |
| total/episodes          | 900      |
| total/epochs            | 1        |
| total/steps             | 89998    |
| total/steps_per_second  | 385      |
| train/loss_actor        | -0.928   |
| train/loss_critic       | 0.0115   |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=100000, episode_reward=-1.83 +/- 0.76
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 0.567    |
| reference_Q_std         | 0.386    |
| reference_action_mean   | -0.497   |
| reference_action_std    | 0.798    |
| reference_actor_Q_mean  | 0.664    |
| reference_actor_Q_std   | 0.404    |
| rollout/Q_mean          | 0.738    |
| rollout/actions_mean    | -0.255   |
| rollout/actions_std     | 0.693    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1e+03    |
| rollout/return          | -2.55    |
| rollout/return_history  | -2.32    |
| total/duration          | 260      |
| total/episodes          | 1e+03    |
| total/epochs            | 1        |
| total/steps             | 99998    |
| total/steps_per_second  | 384      |
| train/loss_actor        | -1.12    |
| train/loss_critic       | 0.0146   |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=110000, episode_reward=-1.59 +/- 0.49
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 0.594    |
| reference_Q_std         | 0.402    |
| reference_action_mean   | -0.436   |
| reference_action_std    | 0.853    |
| reference_actor_Q_mean  | 0.702    |
| reference_actor_Q_std   | 0.385    |
| rollout/Q_mean          | 0.841    |
| rollout/actions_mean    | -0.288   |
| rollout/actions_std     | 0.706    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1.1e+03  |
| rollout/return          | -2.46    |
| rollout/return_history  | -1.5     |
| total/duration          | 286      |
| total/episodes          | 1.1e+03  |
| total/epochs            | 1        |
| total/steps             | 109998   |
| total/steps_per_second  | 384      |
| train/loss_actor        | -1.36    |
| train/loss_critic       | 0.0162   |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=120000, episode_reward=-1.85 +/- 1.25
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 0.702    |
| reference_Q_std         | 0.347    |
| reference_action_mean   | -0.48    |
| reference_action_std    | 0.788    |
| reference_actor_Q_mean  | 0.821    |
| reference_actor_Q_std   | 0.301    |
| rollout/Q_mean          | 0.912    |
| rollout/actions_mean    | -0.315   |
| rollout/actions_std     | 0.715    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1.2e+03  |
| rollout/return          | -2.37    |
| rollout/return_history  | -1.47    |
| total/duration          | 313      |
| total/episodes          | 1.2e+03  |
| total/epochs            | 1        |
| total/steps             | 119998   |
| total/steps_per_second  | 383      |
| train/loss_actor        | -1.44    |
| train/loss_critic       | 0.0149   |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=130000, episode_reward=-2.22 +/- 0.94
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 0.79     |
| reference_Q_std         | 0.292    |
| reference_action_mean   | -0.463   |
| reference_action_std    | 0.805    |
| reference_actor_Q_mean  | 0.92     |
| reference_actor_Q_std   | 0.257    |
| rollout/Q_mean          | 0.959    |
| rollout/actions_mean    | -0.339   |
| rollout/actions_std     | 0.721    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1.3e+03  |
| rollout/return          | -2.31    |
| rollout/return_history  | -1.52    |
| total/duration          | 340      |
| total/episodes          | 1.3e+03  |
| total/epochs            | 1        |
| total/steps             | 129998   |
| total/steps_per_second  | 382      |
| train/loss_actor        | -1.35    |
| train/loss_critic       | 0.0121   |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=140000, episode_reward=-1.09 +/- 0.76
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 0.791    |
| reference_Q_std         | 0.263    |
| reference_action_mean   | -0.484   |
| reference_action_std    | 0.795    |
| reference_actor_Q_mean  | 0.926    |
| reference_actor_Q_std   | 0.231    |
| rollout/Q_mean          | 0.985    |
| rollout/actions_mean    | -0.36    |
| rollout/actions_std     | 0.726    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1.4e+03  |
| rollout/return          | -2.25    |
| rollout/return_history  | -1.46    |
| total/duration          | 367      |
| total/episodes          | 1.4e+03  |
| total/epochs            | 1        |
| total/steps             | 139998   |
| total/steps_per_second  | 381      |
| train/loss_actor        | -1.22    |
| train/loss_critic       | 0.00627  |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=150000, episode_reward=-0.93 +/- 0.26
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 0.838    |
| reference_Q_std         | 0.28     |
| reference_action_mean   | -0.465   |
| reference_action_std    | 0.813    |
| reference_actor_Q_mean  | 0.95     |
| reference_actor_Q_std   | 0.225    |
| rollout/Q_mean          | 0.996    |
| rollout/actions_mean    | -0.378   |
| rollout/actions_std     | 0.73     |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1.5e+03  |
| rollout/return          | -2.2     |
| rollout/return_history  | -1.6     |
| total/duration          | 394      |
| total/episodes          | 1.5e+03  |
| total/epochs            | 1        |
| total/steps             | 149998   |
| total/steps_per_second  | 381      |
| train/loss_actor        | -1.06    |
| train/loss_critic       | 0.00572  |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=160000, episode_reward=-0.72 +/- 0.55
Episode length: 100.00 +/- 0.00
New best mean reward!
--------------------------------------
| reference_Q_mean        | 0.794    |
| reference_Q_std         | 0.281    |
| reference_action_mean   | -0.336   |
| reference_action_std    | 0.94     |
| reference_actor_Q_mean  | 0.872    |
| reference_actor_Q_std   | 0.242    |
| rollout/Q_mean          | 0.995    |
| rollout/actions_mean    | -0.393   |
| rollout/actions_std     | 0.733    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1.6e+03  |
| rollout/return          | -2.16    |
| rollout/return_history  | -1.52    |
| total/duration          | 421      |
| total/episodes          | 1.6e+03  |
| total/epochs            | 1        |
| total/steps             | 159998   |
| total/steps_per_second  | 380      |
| train/loss_actor        | -0.885   |
| train/loss_critic       | 0.00282  |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=170000, episode_reward=-1.58 +/- 0.61
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 0.765    |
| reference_Q_std         | 0.282    |
| reference_action_mean   | -0.418   |
| reference_action_std    | 0.899    |
| reference_actor_Q_mean  | 0.819    |
| reference_actor_Q_std   | 0.251    |
| rollout/Q_mean          | 0.984    |
| rollout/actions_mean    | -0.407   |
| rollout/actions_std     | 0.736    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1.7e+03  |
| rollout/return          | -2.13    |
| rollout/return_history  | -1.54    |
| total/duration          | 449      |
| total/episodes          | 1.7e+03  |
| total/epochs            | 1        |
| total/steps             | 169998   |
| total/steps_per_second  | 379      |
| train/loss_actor        | -0.722   |
| train/loss_critic       | 0.00179  |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=180000, episode_reward=-1.29 +/- 0.66
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 0.712    |
| reference_Q_std         | 0.265    |
| reference_action_mean   | -0.435   |
| reference_action_std    | 0.895    |
| reference_actor_Q_mean  | 0.745    |
| reference_actor_Q_std   | 0.239    |
| rollout/Q_mean          | 0.964    |
| rollout/actions_mean    | -0.42    |
| rollout/actions_std     | 0.738    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1.8e+03  |
| rollout/return          | -2.08    |
| rollout/return_history  | -1.35    |
| total/duration          | 477      |
| total/episodes          | 1.8e+03  |
| total/epochs            | 1        |
| total/steps             | 179998   |
| total/steps_per_second  | 378      |
| train/loss_actor        | -0.554   |
| train/loss_critic       | 0.000667 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=190000, episode_reward=-1.02 +/- 0.45
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 0.642    |
| reference_Q_std         | 0.247    |
| reference_action_mean   | -0.489   |
| reference_action_std    | 0.864    |
| reference_actor_Q_mean  | 0.671    |
| reference_actor_Q_std   | 0.221    |
| rollout/Q_mean          | 0.939    |
| rollout/actions_mean    | -0.432   |
| rollout/actions_std     | 0.739    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1.9e+03  |
| rollout/return          | -2.06    |
| rollout/return_history  | -1.69    |
| total/duration          | 500      |
| total/episodes          | 1.9e+03  |
| total/epochs            | 1        |
| total/steps             | 189998   |
| total/steps_per_second  | 380      |
| train/loss_actor        | -0.398   |
| train/loss_critic       | 0.000264 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=200000, episode_reward=-1.73 +/- 0.80
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 0.54     |
| reference_Q_std         | 0.227    |
| reference_action_mean   | -0.461   |
| reference_action_std    | 0.882    |
| reference_actor_Q_mean  | 0.548    |
| reference_actor_Q_std   | 0.203    |
| rollout/Q_mean          | 0.909    |
| rollout/actions_mean    | -0.442   |
| rollout/actions_std     | 0.74     |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 2e+03    |
| rollout/return          | -2.03    |
| rollout/return_history  | -1.4     |
| total/duration          | 523      |
| total/episodes          | 2e+03    |
| total/epochs            | 1        |
| total/steps             | 199998   |
| total/steps_per_second  | 382      |
| train/loss_actor        | -0.263   |
| train/loss_critic       | 9.53e-05 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=210000, episode_reward=-1.23 +/- 0.35
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 0.418    |
| reference_Q_std         | 0.213    |
| reference_action_mean   | -0.484   |
| reference_action_std    | 0.866    |
| reference_actor_Q_mean  | 0.418    |
| reference_actor_Q_std   | 0.179    |
| rollout/Q_mean          | 0.875    |
| rollout/actions_mean    | -0.452   |
| rollout/actions_std     | 0.741    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 2.1e+03  |
| rollout/return          | -2.01    |
| rollout/return_history  | -1.61    |
| total/duration          | 546      |
| total/episodes          | 2.1e+03  |
| total/epochs            | 1        |
| total/steps             | 209998   |
| total/steps_per_second  | 384      |
| train/loss_actor        | -0.13    |
| train/loss_critic       | 0.000105 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=220000, episode_reward=-1.90 +/- 1.02
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 0.288    |
| reference_Q_std         | 0.201    |
| reference_action_mean   | -0.487   |
| reference_action_std    | 0.865    |
| reference_actor_Q_mean  | 0.262    |
| reference_actor_Q_std   | 0.152    |
| rollout/Q_mean          | 0.838    |
| rollout/actions_mean    | -0.46    |
| rollout/actions_std     | 0.742    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 2.2e+03  |
| rollout/return          | -1.98    |
| rollout/return_history  | -1.35    |
| total/duration          | 570      |
| total/episodes          | 2.2e+03  |
| total/epochs            | 1        |
| total/steps             | 219998   |
| total/steps_per_second  | 386      |
| train/loss_actor        | -0.0146  |
| train/loss_critic       | 0.000206 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=230000, episode_reward=-5.65 +/- 1.91
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 0.141    |
| reference_Q_std         | 0.15     |
| reference_action_mean   | 0.404    |
| reference_action_std    | 0.879    |
| reference_actor_Q_mean  | 0.0661   |
| reference_actor_Q_std   | 0.161    |
| rollout/Q_mean          | 0.8      |
| rollout/actions_mean    | -0.465   |
| rollout/actions_std     | 0.744    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 2.3e+03  |
| rollout/return          | -1.96    |
| rollout/return_history  | -1.57    |
| total/duration          | 593      |
| total/episodes          | 2.3e+03  |
| total/epochs            | 1        |
| total/steps             | 229998   |
| total/steps_per_second  | 388      |
| train/loss_actor        | 0.0622   |
| train/loss_critic       | 0.0019   |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=240000, episode_reward=-1.34 +/- 0.66
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 0.216    |
| reference_Q_std         | 0.103    |
| reference_action_mean   | 0.23     |
| reference_action_std    | 0.958    |
| reference_actor_Q_mean  | 0.276    |
| reference_actor_Q_std   | 0.0804   |
| rollout/Q_mean          | 0.776    |
| rollout/actions_mean    | -0.44    |
| rollout/actions_std     | 0.762    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 2.4e+03  |
| rollout/return          | -2.04    |
| rollout/return_history  | -3.76    |
| total/duration          | 616      |
| total/episodes          | 2.4e+03  |
| total/epochs            | 1        |
| total/steps             | 239998   |
| total/steps_per_second  | 390      |
| train/loss_actor        | -0.0544  |
| train/loss_critic       | 0.000327 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=250000, episode_reward=-1.26 +/- 0.93
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 0.241    |
| reference_Q_std         | 0.102    |
| reference_action_mean   | 0.075    |
| reference_action_std    | 0.963    |
| reference_actor_Q_mean  | 0.33     |
| reference_actor_Q_std   | 0.108    |
| rollout/Q_mean          | 0.755    |
| rollout/actions_mean    | -0.439   |
| rollout/actions_std     | 0.763    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 2.5e+03  |
| rollout/return          | -2.02    |
| rollout/return_history  | -1.59    |
| total/duration          | 639      |
| total/episodes          | 2.5e+03  |
| total/epochs            | 1        |
| total/steps             | 249998   |
| total/steps_per_second  | 391      |
| train/loss_actor        | -0.0829  |
| train/loss_critic       | 0.000417 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=260000, episode_reward=-1.57 +/- 0.85
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 0.244    |
| reference_Q_std         | 0.122    |
| reference_action_mean   | -0.15    |
| reference_action_std    | 0.973    |
| reference_actor_Q_mean  | 0.44     |
| reference_actor_Q_std   | 0.155    |
| rollout/Q_mean          | 0.733    |
| rollout/actions_mean    | -0.438   |
| rollout/actions_std     | 0.764    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 2.6e+03  |
| rollout/return          | -2       |
| rollout/return_history  | -1.59    |
| total/duration          | 662      |
| total/episodes          | 2.6e+03  |
| total/epochs            | 1        |
| total/steps             | 259998   |
| total/steps_per_second  | 392      |
| train/loss_actor        | -0.0394  |
| train/loss_critic       | 0.000557 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=270000, episode_reward=-1.78 +/- 0.84
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | 0.145    |
| reference_Q_std         | 0.139    |
| reference_action_mean   | -0.119   |
| reference_action_std    | 0.985    |
| reference_actor_Q_mean  | 0.379    |
| reference_actor_Q_std   | 0.194    |
| rollout/Q_mean          | 0.709    |
| rollout/actions_mean    | -0.439   |
| rollout/actions_std     | 0.765    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 2.7e+03  |
| rollout/return          | -1.98    |
| rollout/return_history  | -1.5     |
| total/duration          | 686      |
| total/episodes          | 2.7e+03  |
| total/epochs            | 1        |
| total/steps             | 269998   |
| total/steps_per_second  | 394      |
| train/loss_actor        | 0.0279   |
| train/loss_critic       | 0.000591 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=280000, episode_reward=-1.25 +/- 0.56
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.0209  |
| reference_Q_std         | 0.0967   |
| reference_action_mean   | -0.0745  |
| reference_action_std    | 0.982    |
| reference_actor_Q_mean  | 0.171    |
| reference_actor_Q_std   | 0.155    |
| rollout/Q_mean          | 0.686    |
| rollout/actions_mean    | -0.439   |
| rollout/actions_std     | 0.765    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 2.8e+03  |
| rollout/return          | -1.97    |
| rollout/return_history  | -1.47    |
| total/duration          | 709      |
| total/episodes          | 2.8e+03  |
| total/epochs            | 1        |
| total/steps             | 279998   |
| total/steps_per_second  | 395      |
| train/loss_actor        | 0.061    |
| train/loss_critic       | 0.000523 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=290000, episode_reward=-2.37 +/- 1.14
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.0843  |
| reference_Q_std         | 0.0964   |
| reference_action_mean   | 0.0896   |
| reference_action_std    | 0.98     |
| reference_actor_Q_mean  | 0.0132   |
| reference_actor_Q_std   | 0.119    |
| rollout/Q_mean          | 0.661    |
| rollout/actions_mean    | -0.44    |
| rollout/actions_std     | 0.766    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 2.9e+03  |
| rollout/return          | -1.95    |
| rollout/return_history  | -1.45    |
| total/duration          | 733      |
| total/episodes          | 2.9e+03  |
| total/epochs            | 1        |
| total/steps             | 289998   |
| total/steps_per_second  | 396      |
| train/loss_actor        | 0.0493   |
| train/loss_critic       | 0.000116 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=300000, episode_reward=-1.89 +/- 1.02
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.132   |
| reference_Q_std         | 0.0869   |
| reference_action_mean   | -0.0643  |
| reference_action_std    | 0.985    |
| reference_actor_Q_mean  | -0.0258  |
| reference_actor_Q_std   | 0.104    |
| rollout/Q_mean          | 0.637    |
| rollout/actions_mean    | -0.44    |
| rollout/actions_std     | 0.766    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 3e+03    |
| rollout/return          | -1.93    |
| rollout/return_history  | -1.51    |
| total/duration          | 757      |
| total/episodes          | 3e+03    |
| total/epochs            | 1        |
| total/steps             | 299998   |
| total/steps_per_second  | 397      |
| train/loss_actor        | 0.101    |
| train/loss_critic       | 0.000211 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=310000, episode_reward=-1.59 +/- 0.53
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.174   |
| reference_Q_std         | 0.0819   |
| reference_action_mean   | -0.366   |
| reference_action_std    | 0.909    |
| reference_actor_Q_mean  | -0.0322  |
| reference_actor_Q_std   | 0.163    |
| rollout/Q_mean          | 0.613    |
| rollout/actions_mean    | -0.441   |
| rollout/actions_std     | 0.768    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 3.1e+03  |
| rollout/return          | -1.92    |
| rollout/return_history  | -1.52    |
| total/duration          | 780      |
| total/episodes          | 3.1e+03  |
| total/epochs            | 1        |
| total/steps             | 309998   |
| total/steps_per_second  | 397      |
| train/loss_actor        | 0.162    |
| train/loss_critic       | 0.000322 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=320000, episode_reward=-0.96 +/- 0.79
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.208   |
| reference_Q_std         | 0.0928   |
| reference_action_mean   | -0.224   |
| reference_action_std    | 0.941    |
| reference_actor_Q_mean  | -0.0744  |
| reference_actor_Q_std   | 0.175    |
| rollout/Q_mean          | 0.588    |
| rollout/actions_mean    | -0.441   |
| rollout/actions_std     | 0.768    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 3.2e+03  |
| rollout/return          | -1.91    |
| rollout/return_history  | -1.5     |
| total/duration          | 804      |
| total/episodes          | 3.2e+03  |
| total/epochs            | 1        |
| total/steps             | 319998   |
| total/steps_per_second  | 398      |
| train/loss_actor        | 0.214    |
| train/loss_critic       | 0.000478 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=330000, episode_reward=-1.07 +/- 0.53
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.248   |
| reference_Q_std         | 0.0973   |
| reference_action_mean   | -0.301   |
| reference_action_std    | 0.906    |
| reference_actor_Q_mean  | -0.0847  |
| reference_actor_Q_std   | 0.183    |
| rollout/Q_mean          | 0.563    |
| rollout/actions_mean    | -0.442   |
| rollout/actions_std     | 0.768    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 3.3e+03  |
| rollout/return          | -1.9     |
| rollout/return_history  | -1.57    |
| total/duration          | 827      |
| total/episodes          | 3.3e+03  |
| total/epochs            | 1        |
| total/steps             | 329998   |
| total/steps_per_second  | 399      |
| train/loss_actor        | 0.263    |
| train/loss_critic       | 0.000778 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=340000, episode_reward=-1.27 +/- 0.49
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.318   |
| reference_Q_std         | 0.104    |
| reference_action_mean   | -0.373   |
| reference_action_std    | 0.888    |
| reference_actor_Q_mean  | -0.146   |
| reference_actor_Q_std   | 0.182    |
| rollout/Q_mean          | 0.538    |
| rollout/actions_mean    | -0.442   |
| rollout/actions_std     | 0.768    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 3.4e+03  |
| rollout/return          | -1.88    |
| rollout/return_history  | -1.42    |
| total/duration          | 851      |
| total/episodes          | 3.4e+03  |
| total/epochs            | 1        |
| total/steps             | 339998   |
| total/steps_per_second  | 400      |
| train/loss_actor        | 0.311    |
| train/loss_critic       | 0.000847 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=350000, episode_reward=-2.04 +/- 0.96
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.381   |
| reference_Q_std         | 0.111    |
| reference_action_mean   | -0.373   |
| reference_action_std    | 0.884    |
| reference_actor_Q_mean  | -0.181   |
| reference_actor_Q_std   | 0.177    |
| rollout/Q_mean          | 0.513    |
| rollout/actions_mean    | -0.443   |
| rollout/actions_std     | 0.768    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 3.5e+03  |
| rollout/return          | -1.87    |
| rollout/return_history  | -1.52    |
| total/duration          | 875      |
| total/episodes          | 3.5e+03  |
| total/epochs            | 1        |
| total/steps             | 349998   |
| total/steps_per_second  | 400      |
| train/loss_actor        | 0.352    |
| train/loss_critic       | 0.00127  |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=360000, episode_reward=-5.77 +/- 1.14
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.221   |
| reference_Q_std         | 0.175    |
| reference_action_mean   | -0.0474  |
| reference_action_std    | 0.986    |
| reference_actor_Q_mean  | 0.0387   |
| reference_actor_Q_std   | 0.234    |
| rollout/Q_mean          | 0.488    |
| rollout/actions_mean    | -0.433   |
| rollout/actions_std     | 0.776    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 3.6e+03  |
| rollout/return          | -1.92    |
| rollout/return_history  | -3.6     |
| total/duration          | 899      |
| total/episodes          | 3.6e+03  |
| total/epochs            | 1        |
| total/steps             | 359998   |
| total/steps_per_second  | 401      |
| train/loss_actor        | 0.254    |
| train/loss_critic       | 0.000826 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=370000, episode_reward=-5.57 +/- 0.85
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.369   |
| reference_Q_std         | 0.101    |
| reference_action_mean   | -0.226   |
| reference_action_std    | 0.967    |
| reference_actor_Q_mean  | -0.0973  |
| reference_actor_Q_std   | 0.16     |
| rollout/Q_mean          | 0.47     |
| rollout/actions_mean    | -0.427   |
| rollout/actions_std     | 0.782    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 3.7e+03  |
| rollout/return          | -1.96    |
| rollout/return_history  | -3.49    |
| total/duration          | 923      |
| total/episodes          | 3.7e+03  |
| total/epochs            | 1        |
| total/steps             | 369998   |
| total/steps_per_second  | 401      |
| train/loss_actor        | 0.252    |
| train/loss_critic       | 0.000991 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=380000, episode_reward=-4.71 +/- 2.30
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.48    |
| reference_Q_std         | 0.0763   |
| reference_action_mean   | -0.265   |
| reference_action_std    | 0.953    |
| reference_actor_Q_mean  | -0.282   |
| reference_actor_Q_std   | 0.107    |
| rollout/Q_mean          | 0.454    |
| rollout/actions_mean    | -0.424   |
| rollout/actions_std     | 0.786    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 3.8e+03  |
| rollout/return          | -2       |
| rollout/return_history  | -3.5     |
| total/duration          | 947      |
| total/episodes          | 3.8e+03  |
| total/epochs            | 1        |
| total/steps             | 379998   |
| total/steps_per_second  | 401      |
| train/loss_actor        | 0.25     |
| train/loss_critic       | 0.00093  |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=390000, episode_reward=-4.99 +/- 1.89
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.533   |
| reference_Q_std         | 0.0882   |
| reference_action_mean   | -0.291   |
| reference_action_std    | 0.952    |
| reference_actor_Q_mean  | -0.363   |
| reference_actor_Q_std   | 0.102    |
| rollout/Q_mean          | 0.435    |
| rollout/actions_mean    | -0.422   |
| rollout/actions_std     | 0.79     |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 3.9e+03  |
| rollout/return          | -2.04    |
| rollout/return_history  | -3.48    |
| total/duration          | 971      |
| total/episodes          | 3.9e+03  |
| total/epochs            | 1        |
| total/steps             | 389998   |
| total/steps_per_second  | 402      |
| train/loss_actor        | 0.315    |
| train/loss_critic       | 0.00176  |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=400000, episode_reward=-4.00 +/- 2.38
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.54    |
| reference_Q_std         | 0.101    |
| reference_action_mean   | -0.257   |
| reference_action_std    | 0.956    |
| reference_actor_Q_mean  | -0.462   |
| reference_actor_Q_std   | 0.123    |
| rollout/Q_mean          | 0.415    |
| rollout/actions_mean    | -0.419   |
| rollout/actions_std     | 0.793    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 4e+03    |
| rollout/return          | -2.08    |
| rollout/return_history  | -3.47    |
| total/duration          | 995      |
| total/episodes          | 4e+03    |
| total/epochs            | 1        |
| total/steps             | 399998   |
| total/steps_per_second  | 402      |
| train/loss_actor        | 0.498    |
| train/loss_critic       | 0.00426  |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=410000, episode_reward=-4.83 +/- 1.90
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.525   |
| reference_Q_std         | 0.0998   |
| reference_action_mean   | 0.000139 |
| reference_action_std    | 1        |
| reference_actor_Q_mean  | -0.453   |
| reference_actor_Q_std   | 0.0889   |
| rollout/Q_mean          | 0.392    |
| rollout/actions_mean    | -0.416   |
| rollout/actions_std     | 0.797    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 4.1e+03  |
| rollout/return          | -2.12    |
| rollout/return_history  | -4.05    |
| total/duration          | 1.02e+03 |
| total/episodes          | 4.1e+03  |
| total/epochs            | 1        |
| total/steps             | 409998   |
| total/steps_per_second  | 402      |
| train/loss_actor        | 0.606    |
| train/loss_critic       | 0.00476  |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=420000, episode_reward=-2.60 +/- 0.86
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.445   |
| reference_Q_std         | 0.0508   |
| reference_action_mean   | -0.0889  |
| reference_action_std    | 0.978    |
| reference_actor_Q_mean  | -0.501   |
| reference_actor_Q_std   | 0.0739   |
| rollout/Q_mean          | 0.366    |
| rollout/actions_mean    | -0.411   |
| rollout/actions_std     | 0.8      |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 4.2e+03  |
| rollout/return          | -2.18    |
| rollout/return_history  | -4.55    |
| total/duration          | 1.04e+03 |
| total/episodes          | 4.2e+03  |
| total/epochs            | 1        |
| total/steps             | 419998   |
| total/steps_per_second  | 403      |
| train/loss_actor        | 0.724    |
| train/loss_critic       | 0.00632  |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=430000, episode_reward=-3.78 +/- 1.68
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.443   |
| reference_Q_std         | 0.0485   |
| reference_action_mean   | -0.23    |
| reference_action_std    | 0.948    |
| reference_actor_Q_mean  | -0.558   |
| reference_actor_Q_std   | 0.0871   |
| rollout/Q_mean          | 0.34     |
| rollout/actions_mean    | -0.406   |
| rollout/actions_std     | 0.803    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 4.3e+03  |
| rollout/return          | -2.22    |
| rollout/return_history  | -3.67    |
| total/duration          | 1.07e+03 |
| total/episodes          | 4.3e+03  |
| total/epochs            | 1        |
| total/steps             | 429998   |
| total/steps_per_second  | 403      |
| train/loss_actor        | 0.865    |
| train/loss_critic       | 0.00675  |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=440000, episode_reward=-2.32 +/- 1.05
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.458   |
| reference_Q_std         | 0.0604   |
| reference_action_mean   | -0.314   |
| reference_action_std    | 0.944    |
| reference_actor_Q_mean  | -0.633   |
| reference_actor_Q_std   | 0.095    |
| rollout/Q_mean          | 0.312    |
| rollout/actions_mean    | -0.403   |
| rollout/actions_std     | 0.807    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 4.4e+03  |
| rollout/return          | -2.25    |
| rollout/return_history  | -3.47    |
| total/duration          | 1.09e+03 |
| total/episodes          | 4.4e+03  |
| total/epochs            | 1        |
| total/steps             | 439998   |
| total/steps_per_second  | 403      |
| train/loss_actor        | 1.01     |
| train/loss_critic       | 0.01     |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=450000, episode_reward=-3.92 +/- 1.26
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.531   |
| reference_Q_std         | 0.087    |
| reference_action_mean   | -0.326   |
| reference_action_std    | 0.942    |
| reference_actor_Q_mean  | -0.744   |
| reference_actor_Q_std   | 0.121    |
| rollout/Q_mean          | 0.282    |
| rollout/actions_mean    | -0.401   |
| rollout/actions_std     | 0.809    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 4.5e+03  |
| rollout/return          | -2.28    |
| rollout/return_history  | -3.66    |
| total/duration          | 1.12e+03 |
| total/episodes          | 4.5e+03  |
| total/epochs            | 1        |
| total/steps             | 449998   |
| total/steps_per_second  | 403      |
| train/loss_actor        | 1.14     |
| train/loss_critic       | 0.0142   |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=460000, episode_reward=-2.96 +/- 1.05
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.66    |
| reference_Q_std         | 0.122    |
| reference_action_mean   | -0.309   |
| reference_action_std    | 0.945    |
| reference_actor_Q_mean  | -0.909   |
| reference_actor_Q_std   | 0.129    |
| rollout/Q_mean          | 0.251    |
| rollout/actions_mean    | -0.399   |
| rollout/actions_std     | 0.812    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 4.6e+03  |
| rollout/return          | -2.31    |
| rollout/return_history  | -3.57    |
| total/duration          | 1.14e+03 |
| total/episodes          | 4.6e+03  |
| total/epochs            | 1        |
| total/steps             | 459998   |
| total/steps_per_second  | 404      |
| train/loss_actor        | 1.25     |
| train/loss_critic       | 0.014    |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=470000, episode_reward=-3.56 +/- 1.89
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.826   |
| reference_Q_std         | 0.167    |
| reference_action_mean   | -0.321   |
| reference_action_std    | 0.942    |
| reference_actor_Q_mean  | -1.08    |
| reference_actor_Q_std   | 0.134    |
| rollout/Q_mean          | 0.218    |
| rollout/actions_mean    | -0.396   |
| rollout/actions_std     | 0.815    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 4.7e+03  |
| rollout/return          | -2.35    |
| rollout/return_history  | -4.23    |
| total/duration          | 1.16e+03 |
| total/episodes          | 4.7e+03  |
| total/epochs            | 1        |
| total/steps             | 469998   |
| total/steps_per_second  | 404      |
| train/loss_actor        | 1.33     |
| train/loss_critic       | 0.0172   |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=480000, episode_reward=-4.24 +/- 0.84
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.94    |
| reference_Q_std         | 0.191    |
| reference_action_mean   | -0.333   |
| reference_action_std    | 0.943    |
| reference_actor_Q_mean  | -1.19    |
| reference_actor_Q_std   | 0.125    |
| rollout/Q_mean          | 0.185    |
| rollout/actions_mean    | -0.394   |
| rollout/actions_std     | 0.818    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 4.8e+03  |
| rollout/return          | -2.37    |
| rollout/return_history  | -3.46    |
| total/duration          | 1.19e+03 |
| total/episodes          | 4.8e+03  |
| total/epochs            | 1        |
| total/steps             | 479998   |
| total/steps_per_second  | 404      |
| train/loss_actor        | 1.43     |
| train/loss_critic       | 0.0184   |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=490000, episode_reward=-4.04 +/- 1.99
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -1.01    |
| reference_Q_std         | 0.207    |
| reference_action_mean   | -0.333   |
| reference_action_std    | 0.943    |
| reference_actor_Q_mean  | -1.28    |
| reference_actor_Q_std   | 0.136    |
| rollout/Q_mean          | 0.151    |
| rollout/actions_mean    | -0.392   |
| rollout/actions_std     | 0.821    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 4.9e+03  |
| rollout/return          | -2.39    |
| rollout/return_history  | -3.23    |
| total/duration          | 1.21e+03 |
| total/episodes          | 4.9e+03  |
| total/epochs            | 1        |
| total/steps             | 489998   |
| total/steps_per_second  | 404      |
| train/loss_actor        | 1.51     |
| train/loss_critic       | 0.0209   |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=500000, episode_reward=-4.64 +/- 2.12
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -1.09    |
| reference_Q_std         | 0.214    |
| reference_action_mean   | -0.333   |
| reference_action_std    | 0.943    |
| reference_actor_Q_mean  | -1.36    |
| reference_actor_Q_std   | 0.143    |
| rollout/Q_mean          | 0.117    |
| rollout/actions_mean    | -0.39    |
| rollout/actions_std     | 0.823    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 5e+03    |
| rollout/return          | -2.41    |
| rollout/return_history  | -3.61    |
| total/duration          | 1.24e+03 |
| total/episodes          | 5e+03    |
| total/epochs            | 1        |
| total/steps             | 499998   |
| total/steps_per_second  | 404      |
| train/loss_actor        | 1.59     |
| train/loss_critic       | 0.0282   |
| train/param_noise_di... | 0        |
--------------------------------------

/ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/common/callbacks.py:285: UserWarning: Training and eval env are not of the same type<Monitor<TimeLimit<WidowxEnv<widowx_reacher-v7>>>> != <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7fa26c8601d0>
  "{} != {}".format(self.training_env, self.eval_env))
Saving to logs/train_0.5M_widowx_reacher-v7_KAY/ddpg/widowx_reacher-v7_2
pybullet build time: May 18 2020 02:46:26
