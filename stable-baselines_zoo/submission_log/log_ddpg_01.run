--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   n371
  Local device: hfi1_0
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Warning: Open MPI has detected that you are running in an environment with CUDA
devices present and that you are using Intel(r) Ompi-Path networking. However,
the environment variable PSM2_CUDA was not set, meaning that the PSM2 Omni-Path
networking library was not told how to handle CUDA support.

If your application uses CUDA buffers, you should set the environment variable
PSM2_CUDA to 1; otherwise, set it to 0. Setting the variable to the wrong value
can have performance implications on your application, or even cause it to
crash.

Since it was not set, Open MPI has defaulted to setting the PSM2_CUDA
environment variable to 1.

Local hostname: n371
--------------------------------------------------------------------------
WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/ddpg/ddpg.py:332: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/ddpg/policies.py:134: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/ddpg/policies.py:136: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/ddpg/ddpg.py:412: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/ddpg/ddpg.py:452: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

========== widowx_reacher-v5 ==========
Seed: 1
OrderedDict([('memory_limit', 50000),
             ('n_timesteps', 200000.0),
             ('noise_std', 0.1),
             ('noise_type', 'ornstein-uhlenbeck'),
             ('policy', 'MlpPolicy')])
Using 1 environments
Overwriting n_timesteps with n=500000
Creating test environment
TRAINING ENV TYPE :  <Monitor<TimeLimit<WidowxEnv<widowx_reacher-v5>>>>
EVAL ENV TYPE :  <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7ff00558c160>
Applying ornstein-uhlenbeck noise with std 0.1
Log path: logs/train_0.5M_widowx_reacher-v5_KAY/ddpg/widowx_reacher-v5_2
Eval num_timesteps=10000, episode_reward=-0.98 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
--------------------------------------
| reference_Q_mean        | -0.0395  |
| reference_Q_std         | 0.0159   |
| reference_action_mean   | -0.109   |
| reference_action_std    | 0.739    |
| reference_actor_Q_mean  | -0.0381  |
| reference_actor_Q_std   | 0.0163   |
| rollout/Q_mean          | -0.0286  |
| rollout/actions_mean    | -0.148   |
| rollout/actions_std     | 0.625    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 100      |
| rollout/return          | -0.618   |
| rollout/return_history  | -0.618   |
| total/duration          | 25.5     |
| total/episodes          | 100      |
| total/epochs            | 1        |
| total/steps             | 9998     |
| total/steps_per_second  | 393      |
| train/loss_actor        | 0.0371   |
| train/loss_critic       | 6.5e-06  |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=20000, episode_reward=-1.50 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.0456  |
| reference_Q_std         | 0.0312   |
| reference_action_mean   | 0.109    |
| reference_action_std    | 0.84     |
| reference_actor_Q_mean  | -0.0419  |
| reference_actor_Q_std   | 0.0318   |
| rollout/Q_mean          | -0.0318  |
| rollout/actions_mean    | -0.147   |
| rollout/actions_std     | 0.649    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 200      |
| rollout/return          | -0.518   |
| rollout/return_history  | -0.418   |
| total/duration          | 51.1     |
| total/episodes          | 200      |
| total/epochs            | 1        |
| total/steps             | 19998    |
| total/steps_per_second  | 391      |
| train/loss_actor        | 0.0408   |
| train/loss_critic       | 6.38e-06 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=30000, episode_reward=-1.25 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.0478  |
| reference_Q_std         | 0.0406   |
| reference_action_mean   | 0.0991   |
| reference_action_std    | 0.871    |
| reference_actor_Q_mean  | -0.0434  |
| reference_actor_Q_std   | 0.0405   |
| rollout/Q_mean          | -0.0371  |
| rollout/actions_mean    | -0.165   |
| rollout/actions_std     | 0.658    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 300      |
| rollout/return          | -0.476   |
| rollout/return_history  | -0.393   |
| total/duration          | 76.6     |
| total/episodes          | 300      |
| total/epochs            | 1        |
| total/steps             | 29998    |
| total/steps_per_second  | 392      |
| train/loss_actor        | 0.0442   |
| train/loss_critic       | 9.71e-06 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=40000, episode_reward=-4.96 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.0556  |
| reference_Q_std         | 0.0399   |
| reference_action_mean   | -0.406   |
| reference_action_std    | 0.841    |
| reference_actor_Q_mean  | -0.0553  |
| reference_actor_Q_std   | 0.0393   |
| rollout/Q_mean          | -0.0332  |
| rollout/actions_mean    | -0.206   |
| rollout/actions_std     | 0.712    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 400      |
| rollout/return          | -0.473   |
| rollout/return_history  | -0.464   |
| total/duration          | 102      |
| total/episodes          | 400      |
| total/epochs            | 1        |
| total/steps             | 39998    |
| total/steps_per_second  | 393      |
| train/loss_actor        | 0.0461   |
| train/loss_critic       | 2.08e-05 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=50000, episode_reward=-1.30 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.0612  |
| reference_Q_std         | 0.0393   |
| reference_action_mean   | 0.00131  |
| reference_action_std    | 0.894    |
| reference_actor_Q_mean  | -0.0563  |
| reference_actor_Q_std   | 0.0375   |
| rollout/Q_mean          | -0.0317  |
| rollout/actions_mean    | -0.216   |
| rollout/actions_std     | 0.733    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 500      |
| rollout/return          | -0.507   |
| rollout/return_history  | -0.641   |
| total/duration          | 127      |
| total/episodes          | 500      |
| total/epochs            | 1        |
| total/steps             | 49998    |
| total/steps_per_second  | 394      |
| train/loss_actor        | 0.0432   |
| train/loss_critic       | 2.73e-05 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=60000, episode_reward=-1.57 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.0623  |
| reference_Q_std         | 0.0363   |
| reference_action_mean   | -0.188   |
| reference_action_std    | 0.842    |
| reference_actor_Q_mean  | -0.0595  |
| reference_actor_Q_std   | 0.0338   |
| rollout/Q_mean          | -0.0315  |
| rollout/actions_mean    | -0.199   |
| rollout/actions_std     | 0.755    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 600      |
| rollout/return          | -0.499   |
| rollout/return_history  | -0.458   |
| total/duration          | 152      |
| total/episodes          | 600      |
| total/epochs            | 1        |
| total/steps             | 59998    |
| total/steps_per_second  | 394      |
| train/loss_actor        | 0.0427   |
| train/loss_critic       | 2.09e-05 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=70000, episode_reward=-1.45 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.0623  |
| reference_Q_std         | 0.0376   |
| reference_action_mean   | -0.0718  |
| reference_action_std    | 0.905    |
| reference_actor_Q_mean  | -0.0598  |
| reference_actor_Q_std   | 0.0354   |
| rollout/Q_mean          | -0.031   |
| rollout/actions_mean    | -0.205   |
| rollout/actions_std     | 0.765    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 700      |
| rollout/return          | -0.476   |
| rollout/return_history  | -0.341   |
| total/duration          | 178      |
| total/episodes          | 700      |
| total/epochs            | 1        |
| total/steps             | 69998    |
| total/steps_per_second  | 394      |
| train/loss_actor        | 0.0425   |
| train/loss_critic       | 2.34e-05 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=80000, episode_reward=-1.55 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.0617  |
| reference_Q_std         | 0.0409   |
| reference_action_mean   | -0.0565  |
| reference_action_std    | 0.947    |
| reference_actor_Q_mean  | -0.0587  |
| reference_actor_Q_std   | 0.0403   |
| rollout/Q_mean          | -0.0322  |
| rollout/actions_mean    | -0.191   |
| rollout/actions_std     | 0.778    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 800      |
| rollout/return          | -0.472   |
| rollout/return_history  | -0.446   |
| total/duration          | 204      |
| total/episodes          | 800      |
| total/epochs            | 1        |
| total/steps             | 79998    |
| total/steps_per_second  | 393      |
| train/loss_actor        | 0.0449   |
| train/loss_critic       | 3.07e-05 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=90000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.073   |
| reference_Q_std         | 0.0424   |
| reference_action_mean   | -0.0455  |
| reference_action_std    | 0.972    |
| reference_actor_Q_mean  | -0.0706  |
| reference_actor_Q_std   | 0.0413   |
| rollout/Q_mean          | -0.0322  |
| rollout/actions_mean    | -0.179   |
| rollout/actions_std     | 0.797    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 900      |
| rollout/return          | -0.475   |
| rollout/return_history  | -0.497   |
| total/duration          | 229      |
| total/episodes          | 900      |
| total/epochs            | 1        |
| total/steps             | 89998    |
| total/steps_per_second  | 393      |
| train/loss_actor        | 0.0475   |
| train/loss_critic       | 2.81e-05 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=100000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.0819  |
| reference_Q_std         | 0.0407   |
| reference_action_mean   | -0.362   |
| reference_action_std    | 0.912    |
| reference_actor_Q_mean  | -0.0792  |
| reference_actor_Q_std   | 0.0406   |
| rollout/Q_mean          | -0.0329  |
| rollout/actions_mean    | -0.183   |
| rollout/actions_std     | 0.813    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1e+03    |
| rollout/return          | -0.477   |
| rollout/return_history  | -0.498   |
| total/duration          | 255      |
| total/episodes          | 1e+03    |
| total/epochs            | 1        |
| total/steps             | 99998    |
| total/steps_per_second  | 393      |
| train/loss_actor        | 0.05     |
| train/loss_critic       | 4.02e-05 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=110000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.0885  |
| reference_Q_std         | 0.0397   |
| reference_action_mean   | -0.488   |
| reference_action_std    | 0.864    |
| reference_actor_Q_mean  | -0.0846  |
| reference_actor_Q_std   | 0.0384   |
| rollout/Q_mean          | -0.0344  |
| rollout/actions_mean    | -0.192   |
| rollout/actions_std     | 0.821    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1.1e+03  |
| rollout/return          | -0.475   |
| rollout/return_history  | -0.451   |
| total/duration          | 281      |
| total/episodes          | 1.1e+03  |
| total/epochs            | 1        |
| total/steps             | 109998   |
| total/steps_per_second  | 392      |
| train/loss_actor        | 0.0587   |
| train/loss_critic       | 4.2e-05  |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=120000, episode_reward=-1.08 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.0909  |
| reference_Q_std         | 0.0343   |
| reference_action_mean   | -0.44    |
| reference_action_std    | 0.888    |
| reference_actor_Q_mean  | -0.0874  |
| reference_actor_Q_std   | 0.0313   |
| rollout/Q_mean          | -0.0372  |
| rollout/actions_mean    | -0.202   |
| rollout/actions_std     | 0.829    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1.2e+03  |
| rollout/return          | -0.476   |
| rollout/return_history  | -0.488   |
| total/duration          | 305      |
| total/episodes          | 1.2e+03  |
| total/epochs            | 1        |
| total/steps             | 119998   |
| total/steps_per_second  | 393      |
| train/loss_actor        | 0.0728   |
| train/loss_critic       | 6.24e-05 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=130000, episode_reward=-1.48 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.1     |
| reference_Q_std         | 0.025    |
| reference_action_mean   | -0.372   |
| reference_action_std    | 0.913    |
| reference_actor_Q_mean  | -0.0978  |
| reference_actor_Q_std   | 0.0239   |
| rollout/Q_mean          | -0.0408  |
| rollout/actions_mean    | -0.213   |
| rollout/actions_std     | 0.836    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1.3e+03  |
| rollout/return          | -0.478   |
| rollout/return_history  | -0.495   |
| total/duration          | 329      |
| total/episodes          | 1.3e+03  |
| total/epochs            | 1        |
| total/steps             | 129998   |
| total/steps_per_second  | 395      |
| train/loss_actor        | 0.0858   |
| train/loss_critic       | 9.7e-05  |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=140000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.109   |
| reference_Q_std         | 0.024    |
| reference_action_mean   | -0.0856  |
| reference_action_std    | 0.987    |
| reference_actor_Q_mean  | -0.104   |
| reference_actor_Q_std   | 0.024    |
| rollout/Q_mean          | -0.0447  |
| rollout/actions_mean    | -0.21    |
| rollout/actions_std     | 0.845    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1.4e+03  |
| rollout/return          | -0.479   |
| rollout/return_history  | -0.503   |
| total/duration          | 355      |
| total/episodes          | 1.4e+03  |
| total/epochs            | 1        |
| total/steps             | 139998   |
| total/steps_per_second  | 394      |
| train/loss_actor        | 0.0976   |
| train/loss_critic       | 8.39e-05 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=150000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.108   |
| reference_Q_std         | 0.0298   |
| reference_action_mean   | -0.122   |
| reference_action_std    | 0.984    |
| reference_actor_Q_mean  | -0.105   |
| reference_actor_Q_std   | 0.0305   |
| rollout/Q_mean          | -0.0485  |
| rollout/actions_mean    | -0.211   |
| rollout/actions_std     | 0.853    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1.5e+03  |
| rollout/return          | -0.481   |
| rollout/return_history  | -0.498   |
| total/duration          | 381      |
| total/episodes          | 1.5e+03  |
| total/epochs            | 1        |
| total/steps             | 149998   |
| total/steps_per_second  | 393      |
| train/loss_actor        | 0.107    |
| train/loss_critic       | 0.00013  |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=160000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.131   |
| reference_Q_std         | 0.0395   |
| reference_action_mean   | -0.188   |
| reference_action_std    | 0.961    |
| reference_actor_Q_mean  | -0.13    |
| reference_actor_Q_std   | 0.0396   |
| rollout/Q_mean          | -0.0524  |
| rollout/actions_mean    | -0.208   |
| rollout/actions_std     | 0.859    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1.6e+03  |
| rollout/return          | -0.482   |
| rollout/return_history  | -0.507   |
| total/duration          | 408      |
| total/episodes          | 1.6e+03  |
| total/epochs            | 1        |
| total/steps             | 159998   |
| total/steps_per_second  | 392      |
| train/loss_actor        | 0.122    |
| train/loss_critic       | 0.000139 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=170000, episode_reward=-1.52 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.151   |
| reference_Q_std         | 0.0477   |
| reference_action_mean   | -0.423   |
| reference_action_std    | 0.889    |
| reference_actor_Q_mean  | -0.153   |
| reference_actor_Q_std   | 0.0494   |
| rollout/Q_mean          | -0.0522  |
| rollout/actions_mean    | -0.22    |
| rollout/actions_std     | 0.862    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1.7e+03  |
| rollout/return          | -0.484   |
| rollout/return_history  | -0.504   |
| total/duration          | 435      |
| total/episodes          | 1.7e+03  |
| total/epochs            | 1        |
| total/steps             | 169998   |
| total/steps_per_second  | 391      |
| train/loss_actor        | 0.112    |
| train/loss_critic       | 0.000147 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=180000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.188   |
| reference_Q_std         | 0.0895   |
| reference_action_mean   | -0.484   |
| reference_action_std    | 0.847    |
| reference_actor_Q_mean  | -0.19    |
| reference_actor_Q_std   | 0.0908   |
| rollout/Q_mean          | -0.0526  |
| rollout/actions_mean    | -0.239   |
| rollout/actions_std     | 0.862    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1.8e+03  |
| rollout/return          | -0.484   |
| rollout/return_history  | -0.5     |
| total/duration          | 462      |
| total/episodes          | 1.8e+03  |
| total/epochs            | 1        |
| total/steps             | 179998   |
| total/steps_per_second  | 389      |
| train/loss_actor        | 0.103    |
| train/loss_critic       | 9.59e-05 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=190000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.218   |
| reference_Q_std         | 0.134    |
| reference_action_mean   | -0.611   |
| reference_action_std    | 0.784    |
| reference_actor_Q_mean  | -0.222   |
| reference_actor_Q_std   | 0.137    |
| rollout/Q_mean          | -0.0536  |
| rollout/actions_mean    | -0.254   |
| rollout/actions_std     | 0.862    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1.9e+03  |
| rollout/return          | -0.485   |
| rollout/return_history  | -0.497   |
| total/duration          | 490      |
| total/episodes          | 1.9e+03  |
| total/epochs            | 1        |
| total/steps             | 189998   |
| total/steps_per_second  | 388      |
| train/loss_actor        | 0.0963   |
| train/loss_critic       | 9.98e-05 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=200000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.232   |
| reference_Q_std         | 0.161    |
| reference_action_mean   | -0.653   |
| reference_action_std    | 0.754    |
| reference_actor_Q_mean  | -0.239   |
| reference_actor_Q_std   | 0.164    |
| rollout/Q_mean          | -0.055   |
| rollout/actions_mean    | -0.274   |
| rollout/actions_std     | 0.86     |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 2e+03    |
| rollout/return          | -0.486   |
| rollout/return_history  | -0.498   |
| total/duration          | 516      |
| total/episodes          | 2e+03    |
| total/epochs            | 1        |
| total/steps             | 199998   |
| total/steps_per_second  | 387      |
| train/loss_actor        | 0.0956   |
| train/loss_critic       | 9.91e-05 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=210000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.225   |
| reference_Q_std         | 0.161    |
| reference_action_mean   | -0.654   |
| reference_action_std    | 0.753    |
| reference_actor_Q_mean  | -0.233   |
| reference_actor_Q_std   | 0.165    |
| rollout/Q_mean          | -0.0568  |
| rollout/actions_mean    | -0.29    |
| rollout/actions_std     | 0.858    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 2.1e+03  |
| rollout/return          | -0.486   |
| rollout/return_history  | -0.498   |
| total/duration          | 543      |
| total/episodes          | 2.1e+03  |
| total/epochs            | 1        |
| total/steps             | 209998   |
| total/steps_per_second  | 387      |
| train/loss_actor        | 0.0981   |
| train/loss_critic       | 9.55e-05 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=220000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.224   |
| reference_Q_std         | 0.162    |
| reference_action_mean   | -0.365   |
| reference_action_std    | 0.925    |
| reference_actor_Q_mean  | -0.232   |
| reference_actor_Q_std   | 0.164    |
| rollout/Q_mean          | -0.0589  |
| rollout/actions_mean    | -0.301   |
| rollout/actions_std     | 0.858    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 2.2e+03  |
| rollout/return          | -0.487   |
| rollout/return_history  | -0.499   |
| total/duration          | 570      |
| total/episodes          | 2.2e+03  |
| total/epochs            | 1        |
| total/steps             | 219998   |
| total/steps_per_second  | 386      |
| train/loss_actor        | 0.109    |
| train/loss_critic       | 0.000138 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=230000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.223   |
| reference_Q_std         | 0.149    |
| reference_action_mean   | -0.363   |
| reference_action_std    | 0.926    |
| reference_actor_Q_mean  | -0.229   |
| reference_actor_Q_std   | 0.152    |
| rollout/Q_mean          | -0.0614  |
| rollout/actions_mean    | -0.303   |
| rollout/actions_std     | 0.861    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 2.3e+03  |
| rollout/return          | -0.487   |
| rollout/return_history  | -0.497   |
| total/duration          | 596      |
| total/episodes          | 2.3e+03  |
| total/epochs            | 1        |
| total/steps             | 229998   |
| total/steps_per_second  | 386      |
| train/loss_actor        | 0.12     |
| train/loss_critic       | 0.000153 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=240000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.219   |
| reference_Q_std         | 0.131    |
| reference_action_mean   | -0.359   |
| reference_action_std    | 0.928    |
| reference_actor_Q_mean  | -0.223   |
| reference_actor_Q_std   | 0.133    |
| rollout/Q_mean          | -0.064   |
| rollout/actions_mean    | -0.304   |
| rollout/actions_std     | 0.863    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 2.4e+03  |
| rollout/return          | -0.488   |
| rollout/return_history  | -0.498   |
| total/duration          | 623      |
| total/episodes          | 2.4e+03  |
| total/epochs            | 1        |
| total/steps             | 239998   |
| total/steps_per_second  | 385      |
| train/loss_actor        | 0.13     |
| train/loss_critic       | 0.000195 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=250000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.221   |
| reference_Q_std         | 0.12     |
| reference_action_mean   | -0.356   |
| reference_action_std    | 0.93     |
| reference_actor_Q_mean  | -0.224   |
| reference_actor_Q_std   | 0.121    |
| rollout/Q_mean          | -0.0669  |
| rollout/actions_mean    | -0.304   |
| rollout/actions_std     | 0.865    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 2.5e+03  |
| rollout/return          | -0.488   |
| rollout/return_history  | -0.498   |
| total/duration          | 651      |
| total/episodes          | 2.5e+03  |
| total/epochs            | 1        |
| total/steps             | 249998   |
| total/steps_per_second  | 384      |
| train/loss_actor        | 0.139    |
| train/loss_critic       | 0.000206 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=260000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.231   |
| reference_Q_std         | 0.12     |
| reference_action_mean   | -0.355   |
| reference_action_std    | 0.931    |
| reference_actor_Q_mean  | -0.233   |
| reference_actor_Q_std   | 0.121    |
| rollout/Q_mean          | -0.0698  |
| rollout/actions_mean    | -0.305   |
| rollout/actions_std     | 0.868    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 2.6e+03  |
| rollout/return          | -0.489   |
| rollout/return_history  | -0.497   |
| total/duration          | 677      |
| total/episodes          | 2.6e+03  |
| total/epochs            | 1        |
| total/steps             | 259998   |
| total/steps_per_second  | 384      |
| train/loss_actor        | 0.148    |
| train/loss_critic       | 0.000204 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=270000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.251   |
| reference_Q_std         | 0.134    |
| reference_action_mean   | -0.476   |
| reference_action_std    | 0.84     |
| reference_actor_Q_mean  | -0.251   |
| reference_actor_Q_std   | 0.133    |
| rollout/Q_mean          | -0.0728  |
| rollout/actions_mean    | -0.307   |
| rollout/actions_std     | 0.869    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 2.7e+03  |
| rollout/return          | -0.489   |
| rollout/return_history  | -0.499   |
| total/duration          | 704      |
| total/episodes          | 2.7e+03  |
| total/epochs            | 1        |
| total/steps             | 269998   |
| total/steps_per_second  | 383      |
| train/loss_actor        | 0.155    |
| train/loss_critic       | 0.000199 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=280000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.264   |
| reference_Q_std         | 0.133    |
| reference_action_mean   | -0.428   |
| reference_action_std    | 0.896    |
| reference_actor_Q_mean  | -0.262   |
| reference_actor_Q_std   | 0.131    |
| rollout/Q_mean          | -0.0759  |
| rollout/actions_mean    | -0.316   |
| rollout/actions_std     | 0.868    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 2.8e+03  |
| rollout/return          | -0.489   |
| rollout/return_history  | -0.498   |
| total/duration          | 731      |
| total/episodes          | 2.8e+03  |
| total/epochs            | 1        |
| total/steps             | 279998   |
| total/steps_per_second  | 383      |
| train/loss_actor        | 0.162    |
| train/loss_critic       | 0.000263 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=290000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.276   |
| reference_Q_std         | 0.134    |
| reference_action_mean   | -0.414   |
| reference_action_std    | 0.904    |
| reference_actor_Q_mean  | -0.273   |
| reference_actor_Q_std   | 0.132    |
| rollout/Q_mean          | -0.079   |
| rollout/actions_mean    | -0.324   |
| rollout/actions_std     | 0.866    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 2.9e+03  |
| rollout/return          | -0.49    |
| rollout/return_history  | -0.497   |
| total/duration          | 754      |
| total/episodes          | 2.9e+03  |
| total/epochs            | 1        |
| total/steps             | 289998   |
| total/steps_per_second  | 384      |
| train/loss_actor        | 0.168    |
| train/loss_critic       | 0.000271 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=300000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.285   |
| reference_Q_std         | 0.132    |
| reference_action_mean   | -0.65    |
| reference_action_std    | 0.759    |
| reference_actor_Q_mean  | -0.282   |
| reference_actor_Q_std   | 0.131    |
| rollout/Q_mean          | -0.082   |
| rollout/actions_mean    | -0.335   |
| rollout/actions_std     | 0.864    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 3e+03    |
| rollout/return          | -0.49    |
| rollout/return_history  | -0.498   |
| total/duration          | 778      |
| total/episodes          | 3e+03    |
| total/epochs            | 1        |
| total/steps             | 299998   |
| total/steps_per_second  | 386      |
| train/loss_actor        | 0.172    |
| train/loss_critic       | 0.000307 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=310000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.309   |
| reference_Q_std         | 0.161    |
| reference_action_mean   | -0.651   |
| reference_action_std    | 0.758    |
| reference_actor_Q_mean  | -0.302   |
| reference_actor_Q_std   | 0.157    |
| rollout/Q_mean          | -0.085   |
| rollout/actions_mean    | -0.345   |
| rollout/actions_std     | 0.862    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 3.1e+03  |
| rollout/return          | -0.49    |
| rollout/return_history  | -0.498   |
| total/duration          | 802      |
| total/episodes          | 3.1e+03  |
| total/epochs            | 1        |
| total/steps             | 309998   |
| total/steps_per_second  | 387      |
| train/loss_actor        | 0.177    |
| train/loss_critic       | 0.00025  |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=320000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.326   |
| reference_Q_std         | 0.194    |
| reference_action_mean   | -0.652   |
| reference_action_std    | 0.756    |
| reference_actor_Q_mean  | -0.322   |
| reference_actor_Q_std   | 0.191    |
| rollout/Q_mean          | -0.0879  |
| rollout/actions_mean    | -0.354   |
| rollout/actions_std     | 0.86     |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 3.2e+03  |
| rollout/return          | -0.49    |
| rollout/return_history  | -0.498   |
| total/duration          | 826      |
| total/episodes          | 3.2e+03  |
| total/epochs            | 1        |
| total/steps             | 319998   |
| total/steps_per_second  | 388      |
| train/loss_actor        | 0.178    |
| train/loss_critic       | 0.000228 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=330000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.375   |
| reference_Q_std         | 0.271    |
| reference_action_mean   | -0.653   |
| reference_action_std    | 0.756    |
| reference_actor_Q_mean  | -0.371   |
| reference_actor_Q_std   | 0.268    |
| rollout/Q_mean          | -0.0907  |
| rollout/actions_mean    | -0.363   |
| rollout/actions_std     | 0.858    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 3.3e+03  |
| rollout/return          | -0.491   |
| rollout/return_history  | -0.498   |
| total/duration          | 850      |
| total/episodes          | 3.3e+03  |
| total/epochs            | 1        |
| total/steps             | 329998   |
| total/steps_per_second  | 388      |
| train/loss_actor        | 0.18     |
| train/loss_critic       | 0.000284 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=340000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.418   |
| reference_Q_std         | 0.345    |
| reference_action_mean   | -0.652   |
| reference_action_std    | 0.757    |
| reference_actor_Q_mean  | -0.414   |
| reference_actor_Q_std   | 0.344    |
| rollout/Q_mean          | -0.0933  |
| rollout/actions_mean    | -0.371   |
| rollout/actions_std     | 0.856    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 3.4e+03  |
| rollout/return          | -0.491   |
| rollout/return_history  | -0.499   |
| total/duration          | 874      |
| total/episodes          | 3.4e+03  |
| total/epochs            | 1        |
| total/steps             | 339998   |
| total/steps_per_second  | 389      |
| train/loss_actor        | 0.182    |
| train/loss_critic       | 0.00024  |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=350000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.449   |
| reference_Q_std         | 0.395    |
| reference_action_mean   | -0.353   |
| reference_action_std    | 0.932    |
| reference_actor_Q_mean  | -0.445   |
| reference_actor_Q_std   | 0.393    |
| rollout/Q_mean          | -0.0959  |
| rollout/actions_mean    | -0.372   |
| rollout/actions_std     | 0.857    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 3.5e+03  |
| rollout/return          | -0.491   |
| rollout/return_history  | -0.498   |
| total/duration          | 898      |
| total/episodes          | 3.5e+03  |
| total/epochs            | 1        |
| total/steps             | 349998   |
| total/steps_per_second  | 390      |
| train/loss_actor        | 0.183    |
| train/loss_critic       | 0.000305 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=360000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.466   |
| reference_Q_std         | 0.422    |
| reference_action_mean   | -0.348   |
| reference_action_std    | 0.934    |
| reference_actor_Q_mean  | -0.462   |
| reference_actor_Q_std   | 0.42     |
| rollout/Q_mean          | -0.0983  |
| rollout/actions_mean    | -0.371   |
| rollout/actions_std     | 0.858    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 3.6e+03  |
| rollout/return          | -0.491   |
| rollout/return_history  | -0.498   |
| total/duration          | 922      |
| total/episodes          | 3.6e+03  |
| total/epochs            | 1        |
| total/steps             | 359998   |
| total/steps_per_second  | 391      |
| train/loss_actor        | 0.185    |
| train/loss_critic       | 0.000286 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=370000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.479   |
| reference_Q_std         | 0.444    |
| reference_action_mean   | -0.348   |
| reference_action_std    | 0.934    |
| reference_actor_Q_mean  | -0.477   |
| reference_actor_Q_std   | 0.443    |
| rollout/Q_mean          | -0.101   |
| rollout/actions_mean    | -0.369   |
| rollout/actions_std     | 0.86     |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 3.7e+03  |
| rollout/return          | -0.491   |
| rollout/return_history  | -0.497   |
| total/duration          | 946      |
| total/episodes          | 3.7e+03  |
| total/epochs            | 1        |
| total/steps             | 369998   |
| total/steps_per_second  | 391      |
| train/loss_actor        | 0.188    |
| train/loss_critic       | 0.000221 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=380000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.49    |
| reference_Q_std         | 0.457    |
| reference_action_mean   | -0.348   |
| reference_action_std    | 0.935    |
| reference_actor_Q_mean  | -0.49    |
| reference_actor_Q_std   | 0.457    |
| rollout/Q_mean          | -0.103   |
| rollout/actions_mean    | -0.368   |
| rollout/actions_std     | 0.862    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 3.8e+03  |
| rollout/return          | -0.492   |
| rollout/return_history  | -0.499   |
| total/duration          | 970      |
| total/episodes          | 3.8e+03  |
| total/epochs            | 1        |
| total/steps             | 379998   |
| total/steps_per_second  | 392      |
| train/loss_actor        | 0.19     |
| train/loss_critic       | 0.000197 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=390000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.506   |
| reference_Q_std         | 0.481    |
| reference_action_mean   | -0.343   |
| reference_action_std    | 0.936    |
| reference_actor_Q_mean  | -0.507   |
| reference_actor_Q_std   | 0.481    |
| rollout/Q_mean          | -0.105   |
| rollout/actions_mean    | -0.367   |
| rollout/actions_std     | 0.863    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 3.9e+03  |
| rollout/return          | -0.492   |
| rollout/return_history  | -0.498   |
| total/duration          | 994      |
| total/episodes          | 3.9e+03  |
| total/epochs            | 1        |
| total/steps             | 389998   |
| total/steps_per_second  | 392      |
| train/loss_actor        | 0.191    |
| train/loss_critic       | 0.000214 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=400000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.536   |
| reference_Q_std         | 0.519    |
| reference_action_mean   | -0.654   |
| reference_action_std    | 0.755    |
| reference_actor_Q_mean  | -0.537   |
| reference_actor_Q_std   | 0.519    |
| rollout/Q_mean          | -0.107   |
| rollout/actions_mean    | -0.372   |
| rollout/actions_std     | 0.863    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 4e+03    |
| rollout/return          | -0.492   |
| rollout/return_history  | -0.498   |
| total/duration          | 1.02e+03 |
| total/episodes          | 4e+03    |
| total/epochs            | 1        |
| total/steps             | 399998   |
| total/steps_per_second  | 393      |
| train/loss_actor        | 0.191    |
| train/loss_critic       | 0.000268 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=410000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.547   |
| reference_Q_std         | 0.541    |
| reference_action_mean   | -0.654   |
| reference_action_std    | 0.756    |
| reference_actor_Q_mean  | -0.55    |
| reference_actor_Q_std   | 0.541    |
| rollout/Q_mean          | -0.109   |
| rollout/actions_mean    | -0.378   |
| rollout/actions_std     | 0.861    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 4.1e+03  |
| rollout/return          | -0.492   |
| rollout/return_history  | -0.498   |
| total/duration          | 1.04e+03 |
| total/episodes          | 4.1e+03  |
| total/epochs            | 1        |
| total/steps             | 409998   |
| total/steps_per_second  | 393      |
| train/loss_actor        | 0.191    |
| train/loss_critic       | 0.00025  |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=420000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.57    |
| reference_Q_std         | 0.589    |
| reference_action_mean   | -0.653   |
| reference_action_std    | 0.756    |
| reference_actor_Q_mean  | -0.577   |
| reference_actor_Q_std   | 0.59     |
| rollout/Q_mean          | -0.111   |
| rollout/actions_mean    | -0.385   |
| rollout/actions_std     | 0.859    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 4.2e+03  |
| rollout/return          | -0.492   |
| rollout/return_history  | -0.497   |
| total/duration          | 1.07e+03 |
| total/episodes          | 4.2e+03  |
| total/epochs            | 1        |
| total/steps             | 419998   |
| total/steps_per_second  | 394      |
| train/loss_actor        | 0.193    |
| train/loss_critic       | 0.000251 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=430000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.582   |
| reference_Q_std         | 0.634    |
| reference_action_mean   | -0.653   |
| reference_action_std    | 0.757    |
| reference_actor_Q_mean  | -0.593   |
| reference_actor_Q_std   | 0.635    |
| rollout/Q_mean          | -0.113   |
| rollout/actions_mean    | -0.391   |
| rollout/actions_std     | 0.857    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 4.3e+03  |
| rollout/return          | -0.492   |
| rollout/return_history  | -0.498   |
| total/duration          | 1.09e+03 |
| total/episodes          | 4.3e+03  |
| total/epochs            | 1        |
| total/steps             | 429998   |
| total/steps_per_second  | 394      |
| train/loss_actor        | 0.197    |
| train/loss_critic       | 0.0002   |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=440000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.632   |
| reference_Q_std         | 0.738    |
| reference_action_mean   | -0.652   |
| reference_action_std    | 0.757    |
| reference_actor_Q_mean  | -0.644   |
| reference_actor_Q_std   | 0.74     |
| rollout/Q_mean          | -0.115   |
| rollout/actions_mean    | -0.397   |
| rollout/actions_std     | 0.855    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 4.4e+03  |
| rollout/return          | -0.492   |
| rollout/return_history  | -0.498   |
| total/duration          | 1.11e+03 |
| total/episodes          | 4.4e+03  |
| total/epochs            | 1        |
| total/steps             | 439998   |
| total/steps_per_second  | 395      |
| train/loss_actor        | 0.2      |
| train/loss_critic       | 0.000248 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=450000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.714   |
| reference_Q_std         | 0.845    |
| reference_action_mean   | -0.355   |
| reference_action_std    | 0.933    |
| reference_actor_Q_mean  | -0.726   |
| reference_actor_Q_std   | 0.846    |
| rollout/Q_mean          | -0.117   |
| rollout/actions_mean    | -0.397   |
| rollout/actions_std     | 0.855    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 4.5e+03  |
| rollout/return          | -0.492   |
| rollout/return_history  | -0.497   |
| total/duration          | 1.14e+03 |
| total/episodes          | 4.5e+03  |
| total/epochs            | 1        |
| total/steps             | 449998   |
| total/steps_per_second  | 395      |
| train/loss_actor        | 0.204    |
| train/loss_critic       | 0.000286 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=460000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.753   |
| reference_Q_std         | 0.892    |
| reference_action_mean   | -0.35    |
| reference_action_std    | 0.934    |
| reference_actor_Q_mean  | -0.765   |
| reference_actor_Q_std   | 0.893    |
| rollout/Q_mean          | -0.119   |
| rollout/actions_mean    | -0.396   |
| rollout/actions_std     | 0.857    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 4.6e+03  |
| rollout/return          | -0.493   |
| rollout/return_history  | -0.498   |
| total/duration          | 1.16e+03 |
| total/episodes          | 4.6e+03  |
| total/epochs            | 1        |
| total/steps             | 459998   |
| total/steps_per_second  | 395      |
| train/loss_actor        | 0.206    |
| train/loss_critic       | 0.000256 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=470000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.76    |
| reference_Q_std         | 0.892    |
| reference_action_mean   | -0.351   |
| reference_action_std    | 0.934    |
| reference_actor_Q_mean  | -0.77    |
| reference_actor_Q_std   | 0.893    |
| rollout/Q_mean          | -0.121   |
| rollout/actions_mean    | -0.394   |
| rollout/actions_std     | 0.858    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 4.7e+03  |
| rollout/return          | -0.493   |
| rollout/return_history  | -0.498   |
| total/duration          | 1.19e+03 |
| total/episodes          | 4.7e+03  |
| total/epochs            | 1        |
| total/steps             | 469998   |
| total/steps_per_second  | 396      |
| train/loss_actor        | 0.206    |
| train/loss_critic       | 0.000296 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=480000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.746   |
| reference_Q_std         | 0.867    |
| reference_action_mean   | -0.349   |
| reference_action_std    | 0.935    |
| reference_actor_Q_mean  | -0.758   |
| reference_actor_Q_std   | 0.869    |
| rollout/Q_mean          | -0.123   |
| rollout/actions_mean    | -0.393   |
| rollout/actions_std     | 0.86     |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 4.8e+03  |
| rollout/return          | -0.493   |
| rollout/return_history  | -0.497   |
| total/duration          | 1.21e+03 |
| total/episodes          | 4.8e+03  |
| total/epochs            | 1        |
| total/steps             | 479998   |
| total/steps_per_second  | 396      |
| train/loss_actor        | 0.208    |
| train/loss_critic       | 0.000289 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=490000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.732   |
| reference_Q_std         | 0.841    |
| reference_action_mean   | -0.653   |
| reference_action_std    | 0.756    |
| reference_actor_Q_mean  | -0.743   |
| reference_actor_Q_std   | 0.842    |
| rollout/Q_mean          | -0.125   |
| rollout/actions_mean    | -0.393   |
| rollout/actions_std     | 0.86     |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 4.9e+03  |
| rollout/return          | -0.493   |
| rollout/return_history  | -0.498   |
| total/duration          | 1.24e+03 |
| total/episodes          | 4.9e+03  |
| total/epochs            | 1        |
| total/steps             | 489998   |
| total/steps_per_second  | 396      |
| train/loss_actor        | 0.208    |
| train/loss_critic       | 0.000308 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=500000, episode_reward=-1.53 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| reference_Q_mean        | -0.711   |
| reference_Q_std         | 0.805    |
| reference_action_mean   | -0.653   |
| reference_action_std    | 0.756    |
| reference_actor_Q_mean  | -0.724   |
| reference_actor_Q_std   | 0.806    |
| rollout/Q_mean          | -0.126   |
| rollout/actions_mean    | -0.398   |
| rollout/actions_std     | 0.859    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 5e+03    |
| rollout/return          | -0.493   |
| rollout/return_history  | -0.498   |
| total/duration          | 1.26e+03 |
| total/episodes          | 5e+03    |
| total/epochs            | 1        |
| total/steps             | 499998   |
| total/steps_per_second  | 396      |
| train/loss_actor        | 0.21     |
| train/loss_critic       | 0.00039  |
| train/param_noise_di... | 0        |
--------------------------------------

/ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/common/callbacks.py:285: UserWarning: Training and eval env are not of the same type<Monitor<TimeLimit<WidowxEnv<widowx_reacher-v5>>>> != <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7ff00558c160>
  "{} != {}".format(self.training_env, self.eval_env))
Saving to logs/train_0.5M_widowx_reacher-v5_KAY/ddpg/widowx_reacher-v5_2
pybullet build time: May 18 2020 02:46:26
