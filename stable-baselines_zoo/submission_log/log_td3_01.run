WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/td3/td3.py:131: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/td3/policies.py:124: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_layers.py:57: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/td3/td3.py:194: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/td3/td3.py:226: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/td3/td3.py:240: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

========== widowx_reacher-v5 ==========
Seed: 1
OrderedDict([('batch_size', 32),
             ('buffer_size', 10000),
             ('env_wrapper', 'utils.wrappers.TimeFeatureWrapper'),
             ('gamma', 0.9999),
             ('gradient_steps', 10),
             ('learning_rate', 0.0002375898086559317),
             ('learning_starts', 10000),
             ('n_timesteps', 1000000.0),
             ('noise_std', 0.5757724244213915),
             ('noise_type', 'normal'),
             ('policy', 'MlpPolicy'),
             ('policy_kwargs', 'dict(layers=[400, 300])'),
             ('train_freq', 10)])
Using 1 environments
Overwriting n_timesteps with n=10000
Creating test environment
TRAINING ENV TYPE :  <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f16ec050128>
EVAL ENV TYPE :  <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f16ec054ef0>
Applying normal noise with std 0.5757724244213915
Log path: logs/train_0.01M_widowx_reacher-v5/td3/widowx_reacher-v5_2
--------------------------------------
| current_lr              | 0.000238 |
| ep_rewmean              | -2.78    |
| episodes                | 4        |
| eplenmean               | 100      |
| fps                     | 5324     |
| mean 100 episode reward | -2.8     |
| n_updates               | 0        |
| time_elapsed            | 0        |
| total timesteps         | 300      |
--------------------------------------
--------------------------------------
| current_lr              | 0.000238 |
| ep_rewmean              | -2.91    |
| episodes                | 8        |
| eplenmean               | 100      |
| fps                     | 5274     |
| mean 100 episode reward | -2.9     |
| n_updates               | 0        |
| time_elapsed            | 0        |
| total timesteps         | 700      |
--------------------------------------
--------------------------------------
| current_lr              | 0.000238 |
| ep_rewmean              | -3.03    |
| episodes                | 12       |
| eplenmean               | 100      |
| fps                     | 5275     |
| mean 100 episode reward | -3       |
| n_updates               | 0        |
| time_elapsed            | 0        |
| total timesteps         | 1100     |
--------------------------------------
--------------------------------------
| current_lr              | 0.000238 |
| ep_rewmean              | -3.19    |
| episodes                | 16       |
| eplenmean               | 100      |
| fps                     | 5274     |
| mean 100 episode reward | -3.2     |
| n_updates               | 0        |
| time_elapsed            | 0        |
| total timesteps         | 1500     |
--------------------------------------
--------------------------------------
| current_lr              | 0.000238 |
| ep_rewmean              | -3.3     |
| episodes                | 20       |
| eplenmean               | 100      |
| fps                     | 5291     |
| mean 100 episode reward | -3.3     |
| n_updates               | 0        |
| time_elapsed            | 0        |
| total timesteps         | 1900     |
--------------------------------------
--------------------------------------
| current_lr              | 0.000238 |
| ep_rewmean              | -3.23    |
| episodes                | 24       |
| eplenmean               | 100      |
| fps                     | 5309     |
| mean 100 episode reward | -3.2     |
| n_updates               | 0        |
| time_elapsed            | 0        |
| total timesteps         | 2300     |
--------------------------------------
--------------------------------------
| current_lr              | 0.000238 |
| ep_rewmean              | -3.23    |
| episodes                | 28       |
| eplenmean               | 100      |
| fps                     | 5320     |
| mean 100 episode reward | -3.2     |
| n_updates               | 0        |
| time_elapsed            | 0        |
| total timesteps         | 2700     |
--------------------------------------
--------------------------------------
| current_lr              | 0.000238 |
| ep_rewmean              | -3.21    |
| episodes                | 32       |
| eplenmean               | 100      |
| fps                     | 5323     |
| mean 100 episode reward | -3.2     |
| n_updates               | 0        |
| time_elapsed            | 0        |
| total timesteps         | 3100     |
--------------------------------------
--------------------------------------
| current_lr              | 0.000238 |
| ep_rewmean              | -3.12    |
| episodes                | 36       |
| eplenmean               | 100      |
| fps                     | 5327     |
| mean 100 episode reward | -3.1     |
| n_updates               | 0        |
| time_elapsed            | 0        |
| total timesteps         | 3500     |
--------------------------------------
--------------------------------------
| current_lr              | 0.000238 |
| ep_rewmean              | -3.23    |
| episodes                | 40       |
| eplenmean               | 100      |
| fps                     | 5331     |
| mean 100 episode reward | -3.2     |
| n_updates               | 0        |
| time_elapsed            | 0        |
| total timesteps         | 3900     |
--------------------------------------
--------------------------------------
| current_lr              | 0.000238 |
| ep_rewmean              | -3.17    |
| episodes                | 44       |
| eplenmean               | 100      |
| fps                     | 5335     |
| mean 100 episode reward | -3.2     |
| n_updates               | 0        |
| time_elapsed            | 0        |
| total timesteps         | 4300     |
--------------------------------------
--------------------------------------
| current_lr              | 0.000238 |
| ep_rewmean              | -3.16    |
| episodes                | 48       |
| eplenmean               | 100      |
| fps                     | 5338     |
| mean 100 episode reward | -3.2     |
| n_updates               | 0        |
| time_elapsed            | 0        |
| total timesteps         | 4700     |
--------------------------------------
--------------------------------------
| current_lr              | 0.000238 |
| ep_rewmean              | -3.13    |
| episodes                | 52       |
| eplenmean               | 100      |
| fps                     | 5341     |
| mean 100 episode reward | -3.1     |
| n_updates               | 0        |
| time_elapsed            | 0        |
| total timesteps         | 5100     |
--------------------------------------
--------------------------------------
| current_lr              | 0.000238 |
| ep_rewmean              | -3.13    |
| episodes                | 56       |
| eplenmean               | 100      |
| fps                     | 5343     |
| mean 100 episode reward | -3.1     |
| n_updates               | 0        |
| time_elapsed            | 1        |
| total timesteps         | 5500     |
--------------------------------------
--------------------------------------
| current_lr              | 0.000238 |
| ep_rewmean              | -3.09    |
| episodes                | 60       |
| eplenmean               | 100      |
| fps                     | 5341     |
| mean 100 episode reward | -3.1     |
| n_updates               | 0        |
| time_elapsed            | 1        |
| total timesteps         | 5900     |
--------------------------------------
--------------------------------------
| current_lr              | 0.000238 |
| ep_rewmean              | -3.15    |
| episodes                | 64       |
| eplenmean               | 100      |
| fps                     | 5339     |
| mean 100 episode reward | -3.2     |
| n_updates               | 0        |
| time_elapsed            | 1        |
| total timesteps         | 6300     |
--------------------------------------
--------------------------------------
| current_lr              | 0.000238 |
| ep_rewmean              | -3.13    |
| episodes                | 68       |
| eplenmean               | 100      |
| fps                     | 5340     |
| mean 100 episode reward | -3.1     |
| n_updates               | 0        |
| time_elapsed            | 1        |
| total timesteps         | 6700     |
--------------------------------------
--------------------------------------
| current_lr              | 0.000238 |
| ep_rewmean              | -3.12    |
| episodes                | 72       |
| eplenmean               | 100      |
| fps                     | 5334     |
| mean 100 episode reward | -3.1     |
| n_updates               | 0        |
| time_elapsed            | 1        |
| total timesteps         | 7100     |
--------------------------------------
--------------------------------------
| current_lr              | 0.000238 |
| ep_rewmean              | -3.13    |
| episodes                | 76       |
| eplenmean               | 100      |
| fps                     | 5330     |
| mean 100 episode reward | -3.1     |
| n_updates               | 0        |
| time_elapsed            | 1        |
| total timesteps         | 7500     |
--------------------------------------
--------------------------------------
| current_lr              | 0.000238 |
| ep_rewmean              | -3.14    |
| episodes                | 80       |
| eplenmean               | 100      |
| fps                     | 5328     |
| mean 100 episode reward | -3.1     |
| n_updates               | 0        |
| time_elapsed            | 1        |
| total timesteps         | 7900     |
--------------------------------------
--------------------------------------
| current_lr              | 0.000238 |
| ep_rewmean              | -3.11    |
| episodes                | 84       |
| eplenmean               | 100      |
| fps                     | 5327     |
| mean 100 episode reward | -3.1     |
| n_updates               | 0        |
| time_elapsed            | 1        |
| total timesteps         | 8300     |
--------------------------------------
--------------------------------------
| current_lr              | 0.000238 |
| ep_rewmean              | -3.09    |
| episodes                | 88       |
| eplenmean               | 100      |
| fps                     | 5324     |
| mean 100 episode reward | -3.1     |
| n_updates               | 0        |
| time_elapsed            | 1        |
| total timesteps         | 8700     |
--------------------------------------
--------------------------------------
| current_lr              | 0.000238 |
| ep_rewmean              | -3.11    |
| episodes                | 92       |
| eplenmean               | 100      |
| fps                     | 5322     |
| mean 100 episode reward | -3.1     |
| n_updates               | 0        |
| time_elapsed            | 1        |
| total timesteps         | 9100     |
--------------------------------------
--------------------------------------
| current_lr              | 0.000238 |
| ep_rewmean              | -3.11    |
| episodes                | 96       |
| eplenmean               | 100      |
| fps                     | 5320     |
| mean 100 episode reward | -3.1     |
| n_updates               | 0        |
| time_elapsed            | 1        |
| total timesteps         | 9500     |
--------------------------------------
--------------------------------------
| current_lr              | 0.000238 |
| ep_rewmean              | -3.09    |
| episodes                | 100      |
| eplenmean               | 100      |
| fps                     | 5319     |
| mean 100 episode reward | -3.1     |
| n_updates               | 0        |
| time_elapsed            | 1        |
| total timesteps         | 9900     |
--------------------------------------
/home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/callbacks.py:285: UserWarning: Training and eval env are not of the same type<stable_baselines.common.base_class._UnvecWrapper object at 0x7f16ec057400> != <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f16ec054ef0>
  "{} != {}".format(self.training_env, self.eval_env))
Eval num_timesteps=10000, episode_reward=-3.65 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
Saving to logs/train_0.01M_widowx_reacher-v5/td3/widowx_reacher-v5_2
pybullet build time: May 18 2020 02:46:26
