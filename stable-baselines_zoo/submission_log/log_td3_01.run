WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/td3/td3.py:131: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/td3/policies.py:124: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_layers.py:57: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/td3/td3.py:194: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/td3/td3.py:226: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/td3/td3.py:240: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

========== widowx_reacher-v5 ==========
Seed: 1
OrderedDict([('batch_size', 128),
             ('buffer_size', 50000),
             ('gamma', 0.99),
             ('gradient_steps', 100),
             ('learning_rate', 0.0003),
             ('learning_starts', 100),
             ('n_timesteps', 10000),
             ('policy', 'MlpPolicy'),
             ('policy_delay', 2),
             ('policy_kwargs', 'None'),
             ('random_exploration', 0.0),
             ('target_noise_clip', 0.5),
             ('target_policy_noise', 0.2),
             ('tau', 0.005),
             ('train_freq', 100)])
Using 1 environments
Creating test environment
TRAINING ENV TYPE :  <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7fe2a0581518>
EVAL ENV TYPE :  <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7fe2a05a1e80>
Log path: logs/train_10K_widowx_reacher-v5/td3/widowx_reacher-v5_2
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -3.97        |
| episodes                | 4            |
| eplenmean               | 100          |
| fps                     | 496          |
| mean 100 episode reward | -4           |
| n_updates               | 200          |
| qf1_loss                | 0.0012291026 |
| qf2_loss                | 0.0013379299 |
| time_elapsed            | 0            |
| total timesteps         | 300          |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -4.86        |
| episodes                | 8            |
| eplenmean               | 100          |
| fps                     | 564          |
| mean 100 episode reward | -4.9         |
| n_updates               | 600          |
| qf1_loss                | 0.0006776762 |
| qf2_loss                | 0.000693499  |
| time_elapsed            | 1            |
| total timesteps         | 700          |
------------------------------------------
-----------------------------------------
| current_lr              | 0.0003      |
| ep_rewmean              | -4.13       |
| episodes                | 12          |
| eplenmean               | 100         |
| fps                     | 584         |
| mean 100 episode reward | -4.1        |
| n_updates               | 1000        |
| qf1_loss                | 0.000968174 |
| qf2_loss                | 0.001013669 |
| time_elapsed            | 1           |
| total timesteps         | 1100        |
-----------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -3.71        |
| episodes                | 16           |
| eplenmean               | 100          |
| fps                     | 597          |
| mean 100 episode reward | -3.7         |
| n_updates               | 1400         |
| qf1_loss                | 0.0010153541 |
| qf2_loss                | 0.0010063665 |
| time_elapsed            | 2            |
| total timesteps         | 1500         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -3.27        |
| episodes                | 20           |
| eplenmean               | 100          |
| fps                     | 600          |
| mean 100 episode reward | -3.3         |
| n_updates               | 1800         |
| qf1_loss                | 0.0007698252 |
| qf2_loss                | 0.000769398  |
| time_elapsed            | 3            |
| total timesteps         | 1900         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -2.93        |
| episodes                | 24           |
| eplenmean               | 100          |
| fps                     | 601          |
| mean 100 episode reward | -2.9         |
| n_updates               | 2200         |
| qf1_loss                | 0.0010781814 |
| qf2_loss                | 0.0012311082 |
| time_elapsed            | 3            |
| total timesteps         | 2300         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -2.72        |
| episodes                | 28           |
| eplenmean               | 100          |
| fps                     | 602          |
| mean 100 episode reward | -2.7         |
| n_updates               | 2600         |
| qf1_loss                | 0.0013873052 |
| qf2_loss                | 0.0014456416 |
| time_elapsed            | 4            |
| total timesteps         | 2700         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -2.8         |
| episodes                | 32           |
| eplenmean               | 100          |
| fps                     | 603          |
| mean 100 episode reward | -2.8         |
| n_updates               | 3000         |
| qf1_loss                | 0.0009886994 |
| qf2_loss                | 0.0009786821 |
| time_elapsed            | 5            |
| total timesteps         | 3100         |
------------------------------------------
-------------------------------------------
| current_lr              | 0.0003        |
| ep_rewmean              | -2.71         |
| episodes                | 36            |
| eplenmean               | 100           |
| fps                     | 604           |
| mean 100 episode reward | -2.7          |
| n_updates               | 3400          |
| qf1_loss                | 0.000865299   |
| qf2_loss                | 0.00088634324 |
| time_elapsed            | 5             |
| total timesteps         | 3500          |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.0003        |
| ep_rewmean              | -2.61         |
| episodes                | 40            |
| eplenmean               | 100           |
| fps                     | 606           |
| mean 100 episode reward | -2.6          |
| n_updates               | 3800          |
| qf1_loss                | 0.00092018227 |
| qf2_loss                | 0.00092354626 |
| time_elapsed            | 6             |
| total timesteps         | 3900          |
-------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -2.51        |
| episodes                | 44           |
| eplenmean               | 100          |
| fps                     | 606          |
| mean 100 episode reward | -2.5         |
| n_updates               | 4200         |
| qf1_loss                | 0.0009242387 |
| qf2_loss                | 0.0009368621 |
| time_elapsed            | 7            |
| total timesteps         | 4300         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -2.36        |
| episodes                | 48           |
| eplenmean               | 100          |
| fps                     | 606          |
| mean 100 episode reward | -2.4         |
| n_updates               | 4600         |
| qf1_loss                | 0.0011010711 |
| qf2_loss                | 0.0011058128 |
| time_elapsed            | 7            |
| total timesteps         | 4700         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -2.19        |
| episodes                | 52           |
| eplenmean               | 100          |
| fps                     | 605          |
| mean 100 episode reward | -2.2         |
| n_updates               | 5000         |
| qf1_loss                | 0.001164746  |
| qf2_loss                | 0.0011789046 |
| time_elapsed            | 8            |
| total timesteps         | 5100         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -2.04        |
| episodes                | 56           |
| eplenmean               | 100          |
| fps                     | 605          |
| mean 100 episode reward | -2           |
| n_updates               | 5400         |
| qf1_loss                | 0.0014245643 |
| qf2_loss                | 0.001428666  |
| time_elapsed            | 9            |
| total timesteps         | 5500         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -1.91        |
| episodes                | 60           |
| eplenmean               | 100          |
| fps                     | 605          |
| mean 100 episode reward | -1.9         |
| n_updates               | 5800         |
| qf1_loss                | 0.0015993941 |
| qf2_loss                | 0.0015976195 |
| time_elapsed            | 9            |
| total timesteps         | 5900         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -1.79        |
| episodes                | 64           |
| eplenmean               | 100          |
| fps                     | 605          |
| mean 100 episode reward | -1.8         |
| n_updates               | 6200         |
| qf1_loss                | 0.001338702  |
| qf2_loss                | 0.0013544449 |
| time_elapsed            | 10           |
| total timesteps         | 6300         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -1.7         |
| episodes                | 68           |
| eplenmean               | 100          |
| fps                     | 605          |
| mean 100 episode reward | -1.7         |
| n_updates               | 6600         |
| qf1_loss                | 0.0015631248 |
| qf2_loss                | 0.0015807047 |
| time_elapsed            | 11           |
| total timesteps         | 6700         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -1.62        |
| episodes                | 72           |
| eplenmean               | 100          |
| fps                     | 604          |
| mean 100 episode reward | -1.6         |
| n_updates               | 7000         |
| qf1_loss                | 0.0014002263 |
| qf2_loss                | 0.0014005571 |
| time_elapsed            | 11           |
| total timesteps         | 7100         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -1.55        |
| episodes                | 76           |
| eplenmean               | 100          |
| fps                     | 604          |
| mean 100 episode reward | -1.5         |
| n_updates               | 7400         |
| qf1_loss                | 0.0015970565 |
| qf2_loss                | 0.0015898283 |
| time_elapsed            | 12           |
| total timesteps         | 7500         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -1.49        |
| episodes                | 80           |
| eplenmean               | 100          |
| fps                     | 605          |
| mean 100 episode reward | -1.5         |
| n_updates               | 7800         |
| qf1_loss                | 0.0016873936 |
| qf2_loss                | 0.0016867225 |
| time_elapsed            | 13           |
| total timesteps         | 7900         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -1.43        |
| episodes                | 84           |
| eplenmean               | 100          |
| fps                     | 605          |
| mean 100 episode reward | -1.4         |
| n_updates               | 8200         |
| qf1_loss                | 0.0017093511 |
| qf2_loss                | 0.0017174877 |
| time_elapsed            | 13           |
| total timesteps         | 8300         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -1.39        |
| episodes                | 88           |
| eplenmean               | 100          |
| fps                     | 604          |
| mean 100 episode reward | -1.4         |
| n_updates               | 8600         |
| qf1_loss                | 0.0015442463 |
| qf2_loss                | 0.0015583228 |
| time_elapsed            | 14           |
| total timesteps         | 8700         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -1.35        |
| episodes                | 92           |
| eplenmean               | 100          |
| fps                     | 604          |
| mean 100 episode reward | -1.3         |
| n_updates               | 9000         |
| qf1_loss                | 0.0016860488 |
| qf2_loss                | 0.001702441  |
| time_elapsed            | 15           |
| total timesteps         | 9100         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -1.31        |
| episodes                | 96           |
| eplenmean               | 100          |
| fps                     | 604          |
| mean 100 episode reward | -1.3         |
| n_updates               | 9400         |
| qf1_loss                | 0.0015896321 |
| qf2_loss                | 0.0015940133 |
| time_elapsed            | 15           |
| total timesteps         | 9500         |
------------------------------------------
------------------------------------------
| current_lr              | 0.0003       |
| ep_rewmean              | -1.27        |
| episodes                | 100          |
| eplenmean               | 100          |
| fps                     | 604          |
| mean 100 episode reward | -1.3         |
| n_updates               | 9800         |
| qf1_loss                | 0.0017939018 |
| qf2_loss                | 0.0017939226 |
| time_elapsed            | 16           |
| total timesteps         | 9900         |
------------------------------------------
/home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/callbacks.py:285: UserWarning: Training and eval env are not of the same type<stable_baselines.common.base_class._UnvecWrapper object at 0x7fe2a059a128> != <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7fe2a05a1e80>
  "{} != {}".format(self.training_env, self.eval_env))
Eval num_timesteps=10000, episode_reward=-0.37 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
Saving to logs/train_10K_widowx_reacher-v5/td3/widowx_reacher-v5_2
pybullet build time: May 18 2020 02:46:26
