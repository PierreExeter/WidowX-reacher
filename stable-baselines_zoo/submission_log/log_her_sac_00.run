WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/sac/sac.py:141: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/sac/policies.py:194: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_layers.py:57: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/sac/sac.py:232: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/sac/sac.py:294: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/sac/sac.py:314: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

========== widowx_reacher-v8 ==========
Seed: 0
OrderedDict([('batch_size', 256),
             ('buffer_size', 1000000),
             ('ent_coef', 'auto'),
             ('gamma', 0.95),
             ('goal_selection_strategy', 'future'),
             ('learning_rate', 0.001),
             ('learning_starts', 1000),
             ('model_class', 'sac'),
             ('n_sampled_goal', 4),
             ('n_timesteps', 20000.0),
             ('policy', 'MlpPolicy')])
Using 1 environments
Overwriting n_timesteps with n=200000
Creating test environment
TRAINING ENV TYPE :  <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7fc46cc292e8>
Not replacing HERGoalEnvWrapper env by a DummyVecEnv
EVAL ENV TYPE :  <stable_baselines.her.utils.HERGoalEnvWrapper object at 0x7fc46cbbde80>
Log path: logs/train_0.2M_widowx_reacher-v7_HER_SAC_G5/her/widowx_reacher-v8_1
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 0.0038744283  |
| ent_coef_loss           | -7.8960114    |
| entropy                 | 4.5527296     |
| ep_rewmean              | -1.78         |
| episodes                | 100           |
| eplenmean               | 100           |
| fps                     | 318           |
| mean 100 episode reward | -1.8          |
| n_updates               | 8901          |
| policy_loss             | -2.2188766    |
| qf1_loss                | 0.00040505943 |
| qf2_loss                | 0.0007966587  |
| time_elapsed            | 31            |
| total timesteps         | 9900          |
| value_loss              | 0.001238842   |
-------------------------------------------
Eval num_timesteps=10000, episode_reward=-3.58 +/- 2.34
Episode length: 100.00 +/- 0.00
New best mean reward!
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 0.00044197575 |
| ent_coef_loss           | 3.0299025     |
| entropy                 | 4.396968      |
| ep_rewmean              | -2.15         |
| episodes                | 200           |
| eplenmean               | 100           |
| fps                     | 299           |
| mean 100 episode reward | -2.1          |
| n_updates               | 18901         |
| policy_loss             | -0.009907201  |
| qf1_loss                | 3.8255963e-05 |
| qf2_loss                | 3.946825e-05  |
| time_elapsed            | 66            |
| total timesteps         | 19900         |
| value_loss              | 0.00017405555 |
-------------------------------------------
Eval num_timesteps=20000, episode_reward=-2.13 +/- 0.99
Episode length: 100.00 +/- 0.00
New best mean reward!
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 0.00018940521 |
| ent_coef_loss           | -0.85209      |
| entropy                 | 4.263155      |
| ep_rewmean              | -1.05         |
| episodes                | 300           |
| eplenmean               | 100           |
| fps                     | 295           |
| mean 100 episode reward | -1            |
| n_updates               | 28901         |
| policy_loss             | 0.09044172    |
| qf1_loss                | 4.214064e-05  |
| qf2_loss                | 3.9829203e-05 |
| time_elapsed            | 101           |
| total timesteps         | 29900         |
| value_loss              | 5.9843882e-05 |
-------------------------------------------
Eval num_timesteps=30000, episode_reward=-2.93 +/- 2.78
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 0.00012991337 |
| ent_coef_loss           | -5.734698     |
| entropy                 | 3.861161      |
| ep_rewmean              | -1.08         |
| episodes                | 400           |
| eplenmean               | 100           |
| fps                     | 294           |
| mean 100 episode reward | -1.1          |
| n_updates               | 38901         |
| policy_loss             | 0.082513675   |
| qf1_loss                | 1.6525199e-05 |
| qf2_loss                | 2.3630144e-05 |
| time_elapsed            | 135           |
| total timesteps         | 39900         |
| value_loss              | 0.00015082813 |
-------------------------------------------
Eval num_timesteps=40000, episode_reward=-1.54 +/- 0.59
Episode length: 100.00 +/- 0.00
New best mean reward!
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 7.480659e-05  |
| ent_coef_loss           | -4.0942855    |
| entropy                 | 3.7317426     |
| ep_rewmean              | -0.566        |
| episodes                | 500           |
| eplenmean               | 100           |
| fps                     | 293           |
| mean 100 episode reward | -0.6          |
| n_updates               | 48901         |
| policy_loss             | 0.05844155    |
| qf1_loss                | 2.9456858e-05 |
| qf2_loss                | 2.7582282e-05 |
| time_elapsed            | 170           |
| total timesteps         | 49900         |
| value_loss              | 1.6875661e-05 |
-------------------------------------------
Eval num_timesteps=50000, episode_reward=-2.44 +/- 2.13
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 5.9897455e-05 |
| ent_coef_loss           | -0.21086788   |
| entropy                 | 4.141836      |
| ep_rewmean              | -0.446        |
| episodes                | 600           |
| eplenmean               | 100           |
| fps                     | 291           |
| mean 100 episode reward | -0.4          |
| n_updates               | 58901         |
| policy_loss             | 0.046832222   |
| qf1_loss                | 2.9088747e-05 |
| qf2_loss                | 2.852414e-05  |
| time_elapsed            | 205           |
| total timesteps         | 59900         |
| value_loss              | 1.8317383e-05 |
-------------------------------------------
Eval num_timesteps=60000, episode_reward=-2.29 +/- 0.66
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 5.4225613e-05 |
| ent_coef_loss           | 0.011434555   |
| entropy                 | 4.4666567     |
| ep_rewmean              | -0.381        |
| episodes                | 700           |
| eplenmean               | 100           |
| fps                     | 290           |
| mean 100 episode reward | -0.4          |
| n_updates               | 68901         |
| policy_loss             | 0.038712513   |
| qf1_loss                | 8.848025e-06  |
| qf2_loss                | 8.682984e-06  |
| time_elapsed            | 240           |
| total timesteps         | 69900         |
| value_loss              | 1.2461847e-05 |
-------------------------------------------
Eval num_timesteps=70000, episode_reward=-2.16 +/- 1.86
Episode length: 100.00 +/- 0.00
--------------------------------------------
| current_lr              | 0.001          |
| ent_coef                | 4.4191776e-05  |
| ent_coef_loss           | -4.6025333     |
| entropy                 | 4.4035106      |
| ep_rewmean              | -0.274         |
| episodes                | 800            |
| eplenmean               | 100            |
| fps                     | 290            |
| mean 100 episode reward | -0.3           |
| n_updates               | 78901          |
| policy_loss             | 0.03633133     |
| qf1_loss                | 1.086794e-05   |
| qf2_loss                | 1.18500775e-05 |
| time_elapsed            | 275            |
| total timesteps         | 79900          |
| value_loss              | 1.12673315e-05 |
--------------------------------------------
Eval num_timesteps=80000, episode_reward=-3.51 +/- 3.09
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 4.788169e-05  |
| ent_coef_loss           | 5.976701      |
| entropy                 | 5.1146574     |
| ep_rewmean              | -0.276        |
| episodes                | 900           |
| eplenmean               | 100           |
| fps                     | 286           |
| mean 100 episode reward | -0.3          |
| n_updates               | 88901         |
| policy_loss             | 0.037136413   |
| qf1_loss                | 1.751871e-05  |
| qf2_loss                | 1.9558613e-05 |
| time_elapsed            | 313           |
| total timesteps         | 89900         |
| value_loss              | 8.372292e-06  |
-------------------------------------------
Eval num_timesteps=90000, episode_reward=-0.46 +/- 0.48
Episode length: 100.00 +/- 0.00
New best mean reward!
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 3.4549612e-05 |
| ent_coef_loss           | -4.5428305    |
| entropy                 | 4.526329      |
| ep_rewmean              | -0.345        |
| episodes                | 1000          |
| eplenmean               | 100           |
| fps                     | 285           |
| mean 100 episode reward | -0.3          |
| n_updates               | 98901         |
| policy_loss             | 0.02848504    |
| qf1_loss                | 4.665185e-06  |
| qf2_loss                | 6.7519472e-06 |
| time_elapsed            | 350           |
| total timesteps         | 99900         |
| value_loss              | 8.05524e-06   |
-------------------------------------------
Eval num_timesteps=100000, episode_reward=-1.14 +/- 0.36
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 3.0709376e-05 |
| ent_coef_loss           | 18.689053     |
| entropy                 | 4.130162      |
| ep_rewmean              | -0.317        |
| episodes                | 1100          |
| eplenmean               | 100           |
| fps                     | 283           |
| mean 100 episode reward | -0.3          |
| n_updates               | 108901        |
| policy_loss             | 0.030337464   |
| qf1_loss                | 5.148987e-06  |
| qf2_loss                | 9.757436e-06  |
| time_elapsed            | 387           |
| total timesteps         | 109900        |
| value_loss              | 9.935472e-06  |
-------------------------------------------
Eval num_timesteps=110000, episode_reward=-1.73 +/- 0.23
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 2.3696195e-05 |
| ent_coef_loss           | -5.572131     |
| entropy                 | 3.78799       |
| ep_rewmean              | -0.261        |
| episodes                | 1200          |
| eplenmean               | 100           |
| fps                     | 283           |
| mean 100 episode reward | -0.3          |
| n_updates               | 118901        |
| policy_loss             | 0.030311741   |
| qf1_loss                | 8.20378e-06   |
| qf2_loss                | 6.5665e-06    |
| time_elapsed            | 422           |
| total timesteps         | 119900        |
| value_loss              | 1.204513e-05  |
-------------------------------------------
Eval num_timesteps=120000, episode_reward=-1.40 +/- 0.84
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 2.3288496e-05 |
| ent_coef_loss           | 2.72012       |
| entropy                 | 4.629854      |
| ep_rewmean              | -0.27         |
| episodes                | 1300          |
| eplenmean               | 100           |
| fps                     | 283           |
| mean 100 episode reward | -0.3          |
| n_updates               | 128901        |
| policy_loss             | 0.027488293   |
| qf1_loss                | 0.0002088679  |
| qf2_loss                | 0.00023186389 |
| time_elapsed            | 457           |
| total timesteps         | 129900        |
| value_loss              | 3.716294e-06  |
-------------------------------------------
Eval num_timesteps=130000, episode_reward=-1.79 +/- 1.50
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 2.215982e-05  |
| ent_coef_loss           | -13.977443    |
| entropy                 | 4.507722      |
| ep_rewmean              | -0.286        |
| episodes                | 1400          |
| eplenmean               | 100           |
| fps                     | 284           |
| mean 100 episode reward | -0.3          |
| n_updates               | 138901        |
| policy_loss             | 0.026087957   |
| qf1_loss                | 4.620677e-06  |
| qf2_loss                | 4.6386103e-06 |
| time_elapsed            | 492           |
| total timesteps         | 139900        |
| value_loss              | 6.5017634e-06 |
-------------------------------------------
Eval num_timesteps=140000, episode_reward=-1.55 +/- 0.54
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 3.145853e-05  |
| ent_coef_loss           | -3.8427024    |
| entropy                 | 5.0644207     |
| ep_rewmean              | -0.279        |
| episodes                | 1500          |
| eplenmean               | 100           |
| fps                     | 284           |
| mean 100 episode reward | -0.3          |
| n_updates               | 148901        |
| policy_loss             | 0.025443133   |
| qf1_loss                | 8.3969535e-06 |
| qf2_loss                | 5.5767323e-06 |
| time_elapsed            | 527           |
| total timesteps         | 149900        |
| value_loss              | 1.4365274e-05 |
-------------------------------------------
Eval num_timesteps=150000, episode_reward=-3.17 +/- 1.12
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 2.7067892e-05 |
| ent_coef_loss           | 6.7874637     |
| entropy                 | 4.636467      |
| ep_rewmean              | -0.272        |
| episodes                | 1600          |
| eplenmean               | 100           |
| fps                     | 284           |
| mean 100 episode reward | -0.3          |
| n_updates               | 158901        |
| policy_loss             | 0.02795556    |
| qf1_loss                | 3.8625835e-06 |
| qf2_loss                | 4.3128575e-06 |
| time_elapsed            | 562           |
| total timesteps         | 159900        |
| value_loss              | 6.51637e-06   |
-------------------------------------------
Eval num_timesteps=160000, episode_reward=-1.73 +/- 0.57
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 2.2047676e-05 |
| ent_coef_loss           | -0.43073344   |
| entropy                 | 4.2927246     |
| ep_rewmean              | -0.23         |
| episodes                | 1700          |
| eplenmean               | 100           |
| fps                     | 284           |
| mean 100 episode reward | -0.2          |
| n_updates               | 168901        |
| policy_loss             | 0.024611328   |
| qf1_loss                | 6.3029793e-06 |
| qf2_loss                | 6.3812613e-06 |
| time_elapsed            | 598           |
| total timesteps         | 169900        |
| value_loss              | 7.2015396e-06 |
-------------------------------------------
Eval num_timesteps=170000, episode_reward=-1.71 +/- 0.51
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 2.0229016e-05 |
| ent_coef_loss           | 17.011948     |
| entropy                 | 3.8863688     |
| ep_rewmean              | -0.236        |
| episodes                | 1800          |
| eplenmean               | 100           |
| fps                     | 284           |
| mean 100 episode reward | -0.2          |
| n_updates               | 178901        |
| policy_loss             | 0.029020047   |
| qf1_loss                | 1.1936646e-05 |
| qf2_loss                | 1.4674578e-05 |
| time_elapsed            | 633           |
| total timesteps         | 179900        |
| value_loss              | 2.2157761e-05 |
-------------------------------------------
Eval num_timesteps=180000, episode_reward=-1.79 +/- 0.86
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 2.357695e-05  |
| ent_coef_loss           | 7.646205      |
| entropy                 | 4.7216263     |
| ep_rewmean              | -0.251        |
| episodes                | 1900          |
| eplenmean               | 100           |
| fps                     | 283           |
| mean 100 episode reward | -0.3          |
| n_updates               | 188901        |
| policy_loss             | 0.02880957    |
| qf1_loss                | 2.7387437e-06 |
| qf2_loss                | 2.3744608e-06 |
| time_elapsed            | 669           |
| total timesteps         | 189900        |
| value_loss              | 4.901687e-06  |
-------------------------------------------
Eval num_timesteps=190000, episode_reward=-2.09 +/- 0.75
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 2.3664668e-05 |
| ent_coef_loss           | 11.375385     |
| entropy                 | 4.596888      |
| ep_rewmean              | -0.278        |
| episodes                | 2000          |
| eplenmean               | 100           |
| fps                     | 283           |
| mean 100 episode reward | -0.3          |
| n_updates               | 198901        |
| policy_loss             | 0.024477892   |
| qf1_loss                | 5.3954445e-06 |
| qf2_loss                | 6.068395e-06  |
| time_elapsed            | 706           |
| total timesteps         | 199900        |
| value_loss              | 5.5948994e-06 |
-------------------------------------------
Eval num_timesteps=200000, episode_reward=-1.59 +/- 0.84
Episode length: 100.00 +/- 0.00
Saving to logs/train_0.2M_widowx_reacher-v7_HER_SAC_G5/her/widowx_reacher-v8_1
pybullet build time: May 18 2020 02:46:26
