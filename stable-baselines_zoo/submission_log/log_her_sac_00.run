[sonicgpu1.compute:26492] pml_ucx.c:285  Error: UCP worker does not support MPI_THREAD_MULTIPLE
WARNING:tensorflow:From /home/people/paumjaud/WidowX-reacher/stable-baselines/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING:tensorflow:From /home/people/paumjaud/WidowX-reacher/stable-baselines/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /home/people/paumjaud/WidowX-reacher/stable-baselines/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /home/people/paumjaud/WidowX-reacher/stable-baselines/stable_baselines/sac/sac.py:141: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /home/people/paumjaud/WidowX-reacher/stable-baselines/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/people/paumjaud/WidowX-reacher/stable-baselines/stable_baselines/sac/policies.py:194: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From /home/people/paumjaud/WidowX-reacher/stable-baselines/stable_baselines/common/tf_layers.py:57: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /home/people/paumjaud/.conda/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /home/people/paumjaud/WidowX-reacher/stable-baselines/stable_baselines/sac/sac.py:254: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /home/people/paumjaud/.conda/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /home/people/paumjaud/WidowX-reacher/stable-baselines/stable_baselines/sac/sac.py:294: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /home/people/paumjaud/WidowX-reacher/stable-baselines/stable_baselines/sac/sac.py:314: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

========== widowx_reacher-v6 ==========
Seed: 0
OrderedDict([('batch_size', 128),
             ('buffer_size', 1000000),
             ('ent_coef', 0.5),
             ('gamma', 0.98),
             ('goal_selection_strategy', 'future'),
             ('gradient_steps', 10),
             ('learning_rate', 0.00015721498327357606),
             ('learning_starts', 0),
             ('model_class', 'sac'),
             ('n_sampled_goal', 2),
             ('n_timesteps', 20000.0),
             ('policy', 'MlpPolicy'),
             ('policy_kwargs', 'dict(layers=[256, 256])'),
             ('random_exploration', 0.6415024561966096),
             ('train_freq', 10)])
Using 1 environments
Overwriting n_timesteps with n=200000
Creating test environment
TRAINING ENV TYPE :  <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f3649eff3c8>
Not replacing HERGoalEnvWrapper env by a DummyVecEnv
EVAL ENV TYPE :  <stable_baselines.her.utils.HERGoalEnvWrapper object at 0x7f3649f04da0>
Log path: logs/train_0.2M_widowx_reacher-v5_SONIC/her/widowx_reacher-v6_1
----------------------------------------
| current_lr              | 0.000157   |
| entropy                 | 7.650666   |
| ep_rewmean              | -2.98      |
| episodes                | 100        |
| eplenmean               | 100        |
| fps                     | 190        |
| mean 100 episode reward | -3         |
| n_updates               | 9810       |
| policy_loss             | -57.94289  |
| qf1_loss                | 3.4817677  |
| qf2_loss                | 3.47718    |
| time_elapsed            | 51         |
| total timesteps         | 9900       |
| value_loss              | 0.34438938 |
----------------------------------------
Eval num_timesteps=10000, episode_reward=-2.18 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
---------------------------------------
| current_lr              | 0.000157  |
| entropy                 | 7.2027726 |
| ep_rewmean              | -2.57     |
| episodes                | 200       |
| eplenmean               | 100       |
| fps                     | 190       |
| mean 100 episode reward | -2.6      |
| n_updates               | 19810     |
| policy_loss             | -76.52417 |
| qf1_loss                | 12.163524 |
| qf2_loss                | 12.054941 |
| time_elapsed            | 104       |
| total timesteps         | 19900     |
| value_loss              | 0.3883534 |
---------------------------------------
Eval num_timesteps=20000, episode_reward=-2.11 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
----------------------------------------
| current_lr              | 0.000157   |
| entropy                 | 7.4183283  |
| ep_rewmean              | -2.48      |
| episodes                | 300        |
| eplenmean               | 100        |
| fps                     | 192        |
| mean 100 episode reward | -2.5       |
| n_updates               | 29810      |
| policy_loss             | -81.42293  |
| qf1_loss                | 8.275309   |
| qf2_loss                | 8.200137   |
| time_elapsed            | 155        |
| total timesteps         | 29900      |
| value_loss              | 0.32607588 |
----------------------------------------
Eval num_timesteps=30000, episode_reward=-1.73 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
---------------------------------------
| current_lr              | 0.000157  |
| entropy                 | 7.4754357 |
| ep_rewmean              | -2.47     |
| episodes                | 400       |
| eplenmean               | 100       |
| fps                     | 192       |
| mean 100 episode reward | -2.5      |
| n_updates               | 39810     |
| policy_loss             | -83.07356 |
| qf1_loss                | 6.789571  |
| qf2_loss                | 6.676828  |
| time_elapsed            | 207       |
| total timesteps         | 39900     |
| value_loss              | 0.2860995 |
---------------------------------------
Eval num_timesteps=40000, episode_reward=-2.49 +/- 0.00
Episode length: 100.00 +/- 0.00
----------------------------------------
| current_lr              | 0.000157   |
| entropy                 | 7.584597   |
| ep_rewmean              | -2.58      |
| episodes                | 500        |
| eplenmean               | 100        |
| fps                     | 193        |
| mean 100 episode reward | -2.6       |
| n_updates               | 49810      |
| policy_loss             | -83.2068   |
| qf1_loss                | 14.5024    |
| qf2_loss                | 14.345011  |
| time_elapsed            | 258        |
| total timesteps         | 49900      |
| value_loss              | 0.39882198 |
----------------------------------------
Eval num_timesteps=50000, episode_reward=-2.85 +/- 0.00
Episode length: 100.00 +/- 0.00
----------------------------------------
| current_lr              | 0.000157   |
| entropy                 | 7.4189987  |
| ep_rewmean              | -2.6       |
| episodes                | 600        |
| eplenmean               | 100        |
| fps                     | 193        |
| mean 100 episode reward | -2.6       |
| n_updates               | 59810      |
| policy_loss             | -83.799805 |
| qf1_loss                | 4.8718615  |
| qf2_loss                | 4.8513823  |
| time_elapsed            | 309        |
| total timesteps         | 59900      |
| value_loss              | 0.2370787  |
----------------------------------------
Eval num_timesteps=60000, episode_reward=-3.26 +/- 0.00
Episode length: 100.00 +/- 0.00
----------------------------------------
| current_lr              | 0.000157   |
| entropy                 | 7.660165   |
| ep_rewmean              | -2.72      |
| episodes                | 700        |
| eplenmean               | 100        |
| fps                     | 193        |
| mean 100 episode reward | -2.7       |
| n_updates               | 69810      |
| policy_loss             | -83.271126 |
| qf1_loss                | 15.269304  |
| qf2_loss                | 15.150253  |
| time_elapsed            | 361        |
| total timesteps         | 69900      |
| value_loss              | 0.413601   |
----------------------------------------
Eval num_timesteps=70000, episode_reward=-1.35 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
---------------------------------------
| current_lr              | 0.000157  |
| entropy                 | 7.5976906 |
| ep_rewmean              | -2.72     |
| episodes                | 800       |
| eplenmean               | 100       |
| fps                     | 193       |
| mean 100 episode reward | -2.7      |
| n_updates               | 79810     |
| policy_loss             | -84.47732 |
| qf1_loss                | 3.2142346 |
| qf2_loss                | 3.1476634 |
| time_elapsed            | 412       |
| total timesteps         | 79900     |
| value_loss              | 0.287062  |
---------------------------------------
Eval num_timesteps=80000, episode_reward=-1.43 +/- 0.00
Episode length: 100.00 +/- 0.00
----------------------------------------
| current_lr              | 0.000157   |
| entropy                 | 7.6530814  |
| ep_rewmean              | -2.9       |
| episodes                | 900        |
| eplenmean               | 100        |
| fps                     | 194        |
| mean 100 episode reward | -2.9       |
| n_updates               | 89810      |
| policy_loss             | -85.379776 |
| qf1_loss                | 3.5026047  |
| qf2_loss                | 3.5420258  |
| time_elapsed            | 463        |
| total timesteps         | 89900      |
| value_loss              | 0.1890034  |
----------------------------------------
Eval num_timesteps=90000, episode_reward=-3.30 +/- 0.00
Episode length: 100.00 +/- 0.00
----------------------------------------
| current_lr              | 0.000157   |
| entropy                 | 7.584162   |
| ep_rewmean              | -2.74      |
| episodes                | 1000       |
| eplenmean               | 100        |
| fps                     | 194        |
| mean 100 episode reward | -2.7       |
| n_updates               | 99810      |
| policy_loss             | -85.23012  |
| qf1_loss                | 9.942179   |
| qf2_loss                | 9.8946     |
| time_elapsed            | 514        |
| total timesteps         | 99900      |
| value_loss              | 0.30958146 |
----------------------------------------
Eval num_timesteps=100000, episode_reward=-2.90 +/- 0.00
Episode length: 100.00 +/- 0.00
----------------------------------------
| current_lr              | 0.000157   |
| entropy                 | 7.6103463  |
| ep_rewmean              | -2.75      |
| episodes                | 1100       |
| eplenmean               | 100        |
| fps                     | 194        |
| mean 100 episode reward | -2.7       |
| n_updates               | 109810     |
| policy_loss             | -85.53046  |
| qf1_loss                | 1.5786395  |
| qf2_loss                | 1.5801381  |
| time_elapsed            | 565        |
| total timesteps         | 109900     |
| value_loss              | 0.17102464 |
----------------------------------------
Eval num_timesteps=110000, episode_reward=-3.06 +/- 0.00
Episode length: 100.00 +/- 0.00
----------------------------------------
| current_lr              | 0.000157   |
| entropy                 | 7.4766364  |
| ep_rewmean              | -2.73      |
| episodes                | 1200       |
| eplenmean               | 100        |
| fps                     | 194        |
| mean 100 episode reward | -2.7       |
| n_updates               | 119810     |
| policy_loss             | -85.906334 |
| qf1_loss                | 4.870927   |
| qf2_loss                | 4.89255    |
| time_elapsed            | 616        |
| total timesteps         | 119900     |
| value_loss              | 0.6422957  |
----------------------------------------
Eval num_timesteps=120000, episode_reward=-3.42 +/- 0.00
Episode length: 100.00 +/- 0.00
----------------------------------------
| current_lr              | 0.000157   |
| entropy                 | 7.5321817  |
| ep_rewmean              | -2.82      |
| episodes                | 1300       |
| eplenmean               | 100        |
| fps                     | 194        |
| mean 100 episode reward | -2.8       |
| n_updates               | 129810     |
| policy_loss             | -85.96692  |
| qf1_loss                | 1.9159466  |
| qf2_loss                | 1.8802754  |
| time_elapsed            | 666        |
| total timesteps         | 129900     |
| value_loss              | 0.14700842 |
----------------------------------------
Eval num_timesteps=130000, episode_reward=-3.59 +/- 0.00
Episode length: 100.00 +/- 0.00
----------------------------------------
| current_lr              | 0.000157   |
| entropy                 | 7.5604143  |
| ep_rewmean              | -2.77      |
| episodes                | 1400       |
| eplenmean               | 100        |
| fps                     | 195        |
| mean 100 episode reward | -2.8       |
| n_updates               | 139810     |
| policy_loss             | -85.889694 |
| qf1_loss                | 3.7005253  |
| qf2_loss                | 3.8436859  |
| time_elapsed            | 715        |
| total timesteps         | 139900     |
| value_loss              | 0.3600052  |
----------------------------------------
Eval num_timesteps=140000, episode_reward=-3.87 +/- 0.00
Episode length: 100.00 +/- 0.00
----------------------------------------
| current_lr              | 0.000157   |
| entropy                 | 7.606842   |
| ep_rewmean              | -2.76      |
| episodes                | 1500       |
| eplenmean               | 100        |
| fps                     | 195        |
| mean 100 episode reward | -2.8       |
| n_updates               | 149810     |
| policy_loss             | -86.95982  |
| qf1_loss                | 8.040381   |
| qf2_loss                | 8.041901   |
| time_elapsed            | 765        |
| total timesteps         | 149900     |
| value_loss              | 0.26337755 |
----------------------------------------
Eval num_timesteps=150000, episode_reward=-4.03 +/- 0.00
Episode length: 100.00 +/- 0.00
----------------------------------------
| current_lr              | 0.000157   |
| entropy                 | 7.5548487  |
| ep_rewmean              | -2.87      |
| episodes                | 1600       |
| eplenmean               | 100        |
| fps                     | 196        |
| mean 100 episode reward | -2.9       |
| n_updates               | 159810     |
| policy_loss             | -87.25752  |
| qf1_loss                | 3.3992171  |
| qf2_loss                | 3.4153488  |
| time_elapsed            | 815        |
| total timesteps         | 159900     |
| value_loss              | 0.29255834 |
----------------------------------------
Eval num_timesteps=160000, episode_reward=-3.07 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------------
| current_lr              | 0.000157  |
| entropy                 | 7.575012  |
| ep_rewmean              | -2.8      |
| episodes                | 1700      |
| eplenmean               | 100       |
| fps                     | 196       |
| mean 100 episode reward | -2.8      |
| n_updates               | 169810    |
| policy_loss             | -87.25184 |
| qf1_loss                | 5.31481   |
| qf2_loss                | 5.3058305 |
| time_elapsed            | 865       |
| total timesteps         | 169900    |
| value_loss              | 0.264974  |
---------------------------------------
Eval num_timesteps=170000, episode_reward=-2.69 +/- 0.00
Episode length: 100.00 +/- 0.00
----------------------------------------
| current_lr              | 0.000157   |
| entropy                 | 7.563119   |
| ep_rewmean              | -2.85      |
| episodes                | 1800       |
| eplenmean               | 100        |
| fps                     | 196        |
| mean 100 episode reward | -2.9       |
| n_updates               | 179810     |
| policy_loss             | -87.91759  |
| qf1_loss                | 3.48983    |
| qf2_loss                | 3.5175195  |
| time_elapsed            | 915        |
| total timesteps         | 179900     |
| value_loss              | 0.33602065 |
----------------------------------------
Eval num_timesteps=180000, episode_reward=-3.48 +/- 0.00
Episode length: 100.00 +/- 0.00
----------------------------------------
| current_lr              | 0.000157   |
| entropy                 | 7.6832404  |
| ep_rewmean              | -2.8       |
| episodes                | 1900       |
| eplenmean               | 100        |
| fps                     | 196        |
| mean 100 episode reward | -2.8       |
| n_updates               | 189810     |
| policy_loss             | -87.97962  |
| qf1_loss                | 8.749764   |
| qf2_loss                | 8.343321   |
| time_elapsed            | 965        |
| total timesteps         | 189900     |
| value_loss              | 0.18655965 |
----------------------------------------
Eval num_timesteps=190000, episode_reward=-3.58 +/- 0.00
Episode length: 100.00 +/- 0.00
----------------------------------------
| current_lr              | 0.000157   |
| entropy                 | 7.6385927  |
| ep_rewmean              | -2.87      |
| episodes                | 2000       |
| eplenmean               | 100        |
| fps                     | 196        |
| mean 100 episode reward | -2.9       |
| n_updates               | 199810     |
| policy_loss             | -87.808    |
| qf1_loss                | 12.186366  |
| qf2_loss                | 12.143181  |
| time_elapsed            | 1015       |
| total timesteps         | 199900     |
| value_loss              | 0.29872474 |
----------------------------------------
Eval num_timesteps=200000, episode_reward=-3.25 +/- 0.00
Episode length: 100.00 +/- 0.00
Saving to logs/train_0.2M_widowx_reacher-v5_SONIC/her/widowx_reacher-v6_1
pybullet build time: May 18 2020 02:46:26
