WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/ppo2/ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/ppo2/ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/ppo2/ppo2.py:242: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

========== widowx_reacher-v5 ==========
Seed: 0
OrderedDict([('cliprange', 0.2),
             ('ent_coef', 0.01),
             ('gamma', 0.99),
             ('lam', 0.95),
             ('learning_rate', 0.00025),
             ('max_grad_norm', 0.5),
             ('n_envs', 8),
             ('n_steps', 128),
             ('n_timesteps', 10000),
             ('nminibatches', 4),
             ('noptepochs', 4),
             ('normalize', True),
             ('policy', 'MlpPolicy'),
             ('policy_kwargs', 'None'),
             ('vf_coef', 0.5)])
Using 8 environments
Normalizing input and reward
Creating test environment
TRAINING ENV TYPE :  <stable_baselines.common.vec_env.vec_normalize.VecNormalize object at 0x7f15632ba908>
Normalization activated: {'norm_reward': False}
EVAL ENV TYPE :  <stable_baselines.common.vec_env.vec_normalize.VecNormalize object at 0x7f156325d898>
Log path: logs/train_10K_widowx_reacher-v5/ppo2/widowx_reacher-v5_1
--------------------------------------
| approxkl           | 0.00060165237 |
| clipfrac           | 0.0007324219  |
| ep_len_mean        | 100           |
| ep_reward_mean     | -2.27         |
| explained_variance | -0.0275       |
| fps                | 2434          |
| n_updates          | 1             |
| policy_entropy     | 8.514291      |
| policy_loss        | -0.0041925996 |
| serial_timesteps   | 128           |
| time_elapsed       | 1.34e-05      |
| total_timesteps    | 1024          |
| value_loss         | 1.3684096     |
--------------------------------------
--------------------------------------
| approxkl           | 0.00029855012 |
| clipfrac           | 0.0           |
| ep_len_mean        | 100           |
| ep_reward_mean     | -3.09         |
| explained_variance | 0.14          |
| fps                | 5826          |
| n_updates          | 2             |
| policy_entropy     | 8.516897      |
| policy_loss        | -0.0023030434 |
| serial_timesteps   | 256           |
| time_elapsed       | 0.421         |
| total_timesteps    | 2048          |
| value_loss         | 0.28507942    |
--------------------------------------
--------------------------------------
| approxkl           | 0.00037964998 |
| clipfrac           | 0.0           |
| ep_len_mean        | 100           |
| ep_reward_mean     | -3.07         |
| explained_variance | 0.504         |
| fps                | 6002          |
| n_updates          | 3             |
| policy_entropy     | 8.5204735     |
| policy_loss        | -0.0033324445 |
| serial_timesteps   | 384           |
| time_elapsed       | 0.597         |
| total_timesteps    | 3072          |
| value_loss         | 0.14038947    |
--------------------------------------
--------------------------------------
| approxkl           | 0.00034883188 |
| clipfrac           | 0.00024414062 |
| ep_len_mean        | 100           |
| ep_reward_mean     | -2.98         |
| explained_variance | 0.473         |
| fps                | 5926          |
| n_updates          | 4             |
| policy_entropy     | 8.520624      |
| policy_loss        | -0.003760699  |
| serial_timesteps   | 512           |
| time_elapsed       | 0.768         |
| total_timesteps    | 4096          |
| value_loss         | 0.10746294    |
--------------------------------------
--------------------------------------
| approxkl           | 0.0003377656  |
| clipfrac           | 0.00024414062 |
| ep_len_mean        | 100           |
| ep_reward_mean     | -3.01         |
| explained_variance | 0.559         |
| fps                | 5851          |
| n_updates          | 5             |
| policy_entropy     | 8.5280695     |
| policy_loss        | -0.0031507956 |
| serial_timesteps   | 640           |
| time_elapsed       | 0.941         |
| total_timesteps    | 5120          |
| value_loss         | 0.14551964    |
--------------------------------------
--------------------------------------
| approxkl           | 0.00033409678 |
| clipfrac           | 0.0           |
| ep_len_mean        | 100           |
| ep_reward_mean     | -2.98         |
| explained_variance | 0.707         |
| fps                | 6152          |
| n_updates          | 6             |
| policy_entropy     | 8.535789      |
| policy_loss        | -0.0025737516 |
| serial_timesteps   | 768           |
| time_elapsed       | 1.12          |
| total_timesteps    | 6144          |
| value_loss         | 0.13035327    |
--------------------------------------
--------------------------------------
| approxkl           | 0.00030661235 |
| clipfrac           | 0.00024414062 |
| ep_len_mean        | 100           |
| ep_reward_mean     | -3.01         |
| explained_variance | 0.718         |
| fps                | 6080          |
| n_updates          | 7             |
| policy_entropy     | 8.539864      |
| policy_loss        | -0.0035390328 |
| serial_timesteps   | 896           |
| time_elapsed       | 1.28          |
| total_timesteps    | 7168          |
| value_loss         | 0.08911987    |
--------------------------------------
--------------------------------------
| approxkl           | 0.0006587941  |
| clipfrac           | 0.001953125   |
| ep_len_mean        | 100           |
| ep_reward_mean     | -2.83         |
| explained_variance | 0.637         |
| fps                | 6100          |
| n_updates          | 8             |
| policy_entropy     | 8.539276      |
| policy_loss        | -0.0048664305 |
| serial_timesteps   | 1024          |
| time_elapsed       | 1.45          |
| total_timesteps    | 8192          |
| value_loss         | 0.06752059    |
--------------------------------------
-------------------------------------
| approxkl           | 0.0007584348 |
| clipfrac           | 0.0034179688 |
| ep_len_mean        | 100          |
| ep_reward_mean     | -2.78        |
| explained_variance | 0.683        |
| fps                | 5861         |
| n_updates          | 9            |
| policy_entropy     | 8.542133     |
| policy_loss        | -0.005749627 |
| serial_timesteps   | 1152         |
| time_elapsed       | 1.62         |
| total_timesteps    | 9216         |
| value_loss         | 0.09132564   |
-------------------------------------
Saving to logs/train_10K_widowx_reacher-v5/ppo2/widowx_reacher-v5_1
pybullet build time: May 18 2020 02:46:26
