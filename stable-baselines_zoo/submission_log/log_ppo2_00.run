WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/ppo2/ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/ppo2/ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/ppo2/ppo2.py:242: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

========== widowx_reacher-v34 ==========
Seed: 0
OrderedDict([('cliprange', 0.2),
             ('ent_coef', 0.01),
             ('gamma', 0.99),
             ('lam', 0.95),
             ('learning_rate', 0.00025),
             ('max_grad_norm', 0.5),
             ('n_envs', 8),
             ('n_steps', 128),
             ('n_timesteps', 10000000),
             ('nminibatches', 4),
             ('noptepochs', 4),
             ('normalize', True),
             ('policy', 'MlpPolicy'),
             ('policy_kwargs', 'None'),
             ('vf_coef', 0.5)])
Using 8 environments
Normalizing input and reward
Creating test environment
TRAINING ENV TYPE :  <stable_baselines.common.vec_env.vec_normalize.VecNormalize object at 0x7f9dede4c048>
Normalization activated: {'norm_reward': False}
EVAL ENV TYPE :  <stable_baselines.common.vec_env.vec_normalize.VecNormalize object at 0x7f9dede51400>
Log path: logs/benchmark/train_widowx_reacher-v34_sparseDense_noptepochs4_10M/ppo2/widowx_reacher-v34_3
--------------------------------------
| approxkl           | 0.00031164743 |
| clipfrac           | 0.0           |
| ep_len_mean        | 100           |
| ep_reward_mean     | -2.86         |
| explained_variance | -0.0129       |
| fps                | 2367          |
| n_updates          | 1             |
| policy_entropy     | 8.516364      |
| policy_loss        | -0.002508724  |
| serial_timesteps   | 128           |
| time_elapsed       | 1.43e-05      |
| total_timesteps    | 1024          |
| value_loss         | 1.5491843     |
--------------------------------------
--------------------------------------
| approxkl           | 0.00022598222 |
| clipfrac           | 0.0           |
| ep_len_mean        | 100           |
| ep_reward_mean     | -2.85         |
| explained_variance | 0.283         |
| fps                | 4751          |
| n_updates          | 2             |
| policy_entropy     | 8.52325       |
| policy_loss        | -0.0028384323 |
| serial_timesteps   | 256           |
| time_elapsed       | 0.433         |
| total_timesteps    | 2048          |
| value_loss         | 0.2358031     |
--------------------------------------
--------------------------------------
| approxkl           | 0.00041966466 |
| clipfrac           | 0.0012207031  |
| ep_len_mean        | 100           |
| ep_reward_mean     | -2.91         |
| explained_variance | 0.356         |
| fps                | 4853          |
| n_updates          | 3             |
| policy_entropy     | 8.533089      |
| policy_loss        | -0.004256023  |
| serial_timesteps   | 384           |
| time_elapsed       | 0.648         |
| total_timesteps    | 3072          |
| value_loss         | 0.23138067    |
--------------------------------------
--------------------------------------
| approxkl           | 0.00034862247 |
| clipfrac           | 0.0           |
| ep_len_mean        | 100           |
| ep_reward_mean     | -2.92         |
| explained_variance | 0.316         |
| fps                | 4770          |
| n_updates          | 4             |
| policy_entropy     | 8.536142      |
| policy_loss        | -0.0030987805 |
| serial_timesteps   | 512           |
| time_elapsed       | 0.86          |
| total_timesteps    | 4096          |
| value_loss         | 0.20498058    |
--------------------------------------
--------------------------------------
| approxkl           | 0.00038831428 |
| clipfrac           | 0.00024414062 |
| ep_len_mean        | 100           |
| ep_reward_mean     | -2.92         |
| explained_variance | 0.104         |
| fps                | 4844          |
| n_updates          | 5             |
| policy_entropy     | 8.540061      |
| policy_loss        | -0.004060311  |
| serial_timesteps   | 640           |
| time_elapsed       | 1.07          |
| total_timesteps    | 5120          |
| value_loss         | 0.22275755    |
--------------------------------------
--------------------------------------
| approxkl           | 0.0007899145  |
| clipfrac           | 0.0034179688  |
| ep_len_mean        | 100           |
| ep_reward_mean     | -2.85         |
| explained_variance | 0.69          |
| fps                | 4729          |
| n_updates          | 6             |
| policy_entropy     | 8.545379      |
| policy_loss        | -0.0042766193 |
| serial_timesteps   | 768           |
| time_elapsed       | 1.29          |
| total_timesteps    | 6144          |
| value_loss         | 0.1697461     |
--------------------------------------
--------------------------------------
| approxkl           | 0.00041967933 |
| clipfrac           | 0.00024414062 |
| ep_len_mean        | 100           |
| ep_reward_mean     | -2.84         |
| explained_variance | 0.497         |
| fps                | 4712          |
| n_updates          | 7             |
| policy_entropy     | 8.547815      |
| policy_loss        | -0.0026622734 |
| serial_timesteps   | 896           |
| time_elapsed       | 1.5           |
| total_timesteps    | 7168          |
| value_loss         | 0.2196208     |
--------------------------------------
--------------------------------------
| approxkl           | 0.0005138327  |
| clipfrac           | 0.0           |
| ep_len_mean        | 100           |
| ep_reward_mean     | -2.9          |
| explained_variance | 0.638         |
| fps                | 4845          |
| n_updates          | 8             |
| policy_entropy     | 8.550585      |
| policy_loss        | -0.0025157826 |
| serial_timesteps   | 1024          |
| time_elapsed       | 1.72          |
| total_timesteps    | 8192          |
| value_loss         | 0.19698761    |
--------------------------------------
-------------------------------------
| approxkl           | 0.0006531328 |
| clipfrac           | 0.0017089844 |
| ep_len_mean        | 100          |
| ep_reward_mean     | -2.87        |
| explained_variance | 0.825        |
| fps                | 4825         |
| n_updates          | 9            |
| policy_entropy     | 8.553655     |
| policy_loss        | -0.004439673 |
| serial_timesteps   | 1152         |
| time_elapsed       | 1.93         |
| total_timesteps    | 9216         |
| value_loss         | 0.14574924   |
-------------------------------------
Eval num_timesteps=10000, episode_reward=-5.43 +/- 1.83
Episode length: 100.00 +/- 0.00
New best mean reward!
--------------------------------------
| approxkl           | 0.0005562599  |
| clipfrac           | 0.00024414062 |
| ep_len_mean        | 100           |
| ep_reward_mean     | -2.89         |
| explained_variance | 0.689         |
| fps                | 1827          |
| n_updates          | 10            |
| policy_entropy     | 8.552767      |
| policy_loss        | -0.0032691099 |
| serial_timesteps   | 1280          |
| time_elapsed       | 2.14          |
| total_timesteps    | 10240         |
| value_loss         | 0.13930154    |
--------------------------------------
--------------------------------------
| approxkl           | 0.00044081098 |
| clipfrac           | 0.00048828125 |
| ep_len_mean        | 100           |
| ep_reward_mean     | -2.86         |
| explained_variance | 0.696         |
| fps                | 4843          |
| n_updates          | 11            |
| policy_entropy     | 8.557384      |
| policy_loss        | -0.0036625639 |
| serial_timesteps   | 1408          |
| time_elapsed       | 2.7           |
| total_timesteps    | 11264         |
| value_loss         | 0.15065987    |
--------------------------------------
--------------------------------------
| approxkl           | 0.00068195903 |
| clipfrac           | 0.001953125   |
| ep_len_mean        | 100           |
| ep_reward_mean     | -2.84         |
| explained_variance | 0.684         |
| fps                | 4854          |
| n_updates          | 12            |
| policy_entropy     | 8.561162      |
| policy_loss        | -0.0042493194 |
| serial_timesteps   | 1536          |
| time_elapsed       | 2.92          |
| total_timesteps    | 12288         |
| value_loss         | 0.13923061    |
--------------------------------------
--------------------------------------
| approxkl           | 0.0007656452  |
| clipfrac           | 0.0014648438  |
| ep_len_mean        | 100           |
| ep_reward_mean     | -2.86         |
| explained_variance | 0.732         |
| fps                | 4787          |
| n_updates          | 13            |
| policy_entropy     | 8.560997      |
| policy_loss        | -0.0059040417 |
| serial_timesteps   | 1664          |
| time_elapsed       | 3.13          |
| total_timesteps    | 13312         |
| value_loss         | 0.1050747     |
--------------------------------------
-------------------------------------
| approxkl           | 0.0009235601 |
| clipfrac           | 0.001953125  |
| ep_len_mean        | 100          |
| ep_reward_mean     | -2.82        |
| explained_variance | 0.771        |
| fps                | 4778         |
| n_updates          | 14           |
| policy_entropy     | 8.5615835    |
| policy_loss        | -0.0051222   |
| serial_timesteps   | 1792         |
| time_elapsed       | 3.34         |
| total_timesteps    | 14336        |
| value_loss         | 0.10907435   |
-------------------------------------
--------------------------------------
| approxkl           | 0.0015389503  |
| clipfrac           | 0.012207031   |
| ep_len_mean        | 100           |
| ep_reward_mean     | -2.78         |
| explained_variance | 0.723         |
| fps                | 4764          |
| n_updates          | 15            |
| policy_entropy     | 8.563217      |
| policy_loss        | -0.0063557867 |
| serial_timesteps   | 1920          |
| time_elapsed       | 3.56          |
| total_timesteps    | 15360         |
| value_loss         | 0.08493836    |
--------------------------------------
-------------------------------------
| approxkl           | 0.002683677  |
| clipfrac           | 0.027832031  |
| ep_len_mean        | 100          |
| ep_reward_mean     | -2.71        |
| explained_variance | 0.868        |
| fps                | 4795         |
| n_updates          | 16           |
| policy_entropy     | 8.56721      |
| policy_loss        | -0.008901287 |
| serial_timesteps   | 2048         |
| time_elapsed       | 3.77         |
| total_timesteps    | 16384        |
| value_loss         | 0.039570156  |
-------------------------------------
-------------------------------------
| approxkl           | 0.001657502  |
| clipfrac           | 0.012451172  |
| ep_len_mean        | 100          |
| ep_reward_mean     | -2.69        |
| explained_variance | 0.732        |
| fps                | 4801         |
| n_updates          | 17           |
| policy_entropy     | 8.572036     |
| policy_loss        | -0.004942129 |
| serial_timesteps   | 2176         |
| time_elapsed       | 3.99         |
| total_timesteps    | 17408        |
| value_loss         | 0.06769977   |
-------------------------------------
-------------------------------------
| approxkl           | 0.0041002114 |
| clipfrac           | 0.05029297   |
| ep_len_mean        | 100          |
| ep_reward_mean     | -2.49        |
| explained_variance | 0.907        |
| fps                | 4705         |
| n_updates          | 18           |
| policy_entropy     | 8.573868     |
| policy_loss        | -0.008937313 |
| serial_timesteps   | 2304         |
| time_elapsed       | 4.2          |
| total_timesteps    | 18432        |
| value_loss         | 0.01718284   |
-------------------------------------
-------------------------------------
| approxkl           | 0.0020991983 |
| clipfrac           | 0.021484375  |
| ep_len_mean        | 100          |
| ep_reward_mean     | -2.43        |
| explained_variance | 0.799        |
| fps                | 4836         |
| n_updates          | 19           |
| policy_entropy     | 8.569974     |
| policy_loss        | -0.006413223 |
| serial_timesteps   | 2432         |
| time_elapsed       | 4.42         |
| total_timesteps    | 19456        |
| value_loss         | 0.07230427   |
-------------------------------------
Eval num_timesteps=20000, episode_reward=-2.02 +/- 0.04
Episode length: 100.00 +/- 0.00
New best mean reward!
--------------------------------------
| approxkl           | 0.0017740815  |
| clipfrac           | 0.013916016   |
| ep_len_mean        | 100           |
| ep_reward_mean     | -2.41         |
| explained_variance | 0.743         |
| fps                | 1868          |
| n_updates          | 20            |
| policy_entropy     | 8.565826      |
| policy_loss        | -0.0038759883 |
| serial_timesteps   | 2560          |
| time_elapsed       | 4.63          |
| total_timesteps    | 20480         |
| value_loss         | 0.12507707    |
--------------------------------------
-------------------------------------
| approxkl           | 0.00100247   |
| clipfrac           | 0.00390625   |
| ep_len_mean        | 100          |
| ep_reward_mean     | -2.48        |
| explained_variance | 0.752        |
| fps                | 4671         |
| n_updates          | 21           |
| policy_entropy     | 8.566637     |
| policy_loss        | -0.003798223 |
| serial_timesteps   | 2688         |
| time_elapsed       | 5.18         |
| total_timesteps    | 21504        |
| value_loss         | 0.18617934   |
-------------------------------------
-------------------------------------
| approxkl           | 0.0016121669 |
| clipfrac           | 0.011962891  |
| ep_len_mean        | 100          |
| ep_reward_mean     | -2.34        |
| explained_variance | 0.786        |
| fps                | 4765         |
| n_updates          | 22           |
| policy_entropy     | 8.569018     |
| policy_loss        | -0.005752262 |
| serial_timesteps   | 2816         |
| time_elapsed       | 5.4          |
| total_timesteps    | 22528        |
| value_loss         | 0.1062786    |
-------------------------------------
-------------------------------------
| approxkl           | 0.0023871588 |
| clipfrac           | 0.01928711   |
| ep_len_mean        | 100          |
| ep_reward_mean     | -2.25        |
| explained_variance | 0.822        |
| fps                | 4774         |
| n_updates          | 23           |
| policy_entropy     | 8.569873     |
| policy_loss        | -0.006834727 |
| serial_timesteps   | 2944         |
| time_elapsed       | 5.61         |
| total_timesteps    | 23552        |
| value_loss         | 0.08303303   |
-------------------------------------
--------------------------------------
| approxkl           | 0.0029704524  |
| clipfrac           | 0.033935547   |
| ep_len_mean        | 100           |
| ep_reward_mean     | -2.14         |
| explained_variance | 0.861         |
| fps                | 4777          |
| n_updates          | 24            |
| policy_entropy     | 8.571177      |
| policy_loss        | -0.0057561183 |
| serial_timesteps   | 3072          |
| time_elapsed       | 5.83          |
| total_timesteps    | 24576         |
| value_loss         | 0.064003065   |
--------------------------------------
--------------------------------------
| approxkl           | 0.0015228041  |
| clipfrac           | 0.012207031   |
| ep_len_mean        | 100           |
| ep_reward_mean     | -2.05         |
| explained_variance | 0.812         |
| fps                | 4496          |
| n_updates          | 25            |
| policy_entropy     | 8.577175      |
| policy_loss        | -0.0042593176 |
| serial_timesteps   | 3200          |
| time_elapsed       | 6.04          |
| total_timesteps    | 25600         |
| value_loss         | 0.0993155     |
--------------------------------------
-------------------------------------
| approxkl           | 0.004072387  |
| clipfrac           | 0.046875     |
| ep_len_mean        | 100          |
| ep_reward_mean     | -1.97        |
| explained_variance | 0.772        |
| fps                | 4690         |
| n_updates          | 26           |
| policy_entropy     | 8.57929      |
| policy_loss        | -0.008928423 |
| serial_timesteps   | 3328         |
| time_elapsed       | 6.27         |
| total_timesteps    | 26624        |
| value_loss         | 0.05547119   |
-------------------------------------
-------------------------------------
| approxkl           | 0.003723927  |
| clipfrac           | 0.045898438  |
| ep_len_mean        | 100          |
| ep_reward_mean     | -1.88        |
| explained_variance | 0.858        |
| fps                | 4391         |
| n_updates          | 27           |
| policy_entropy     | 8.579022     |
| policy_loss        | -0.009919468 |
| serial_timesteps   | 3456         |
| time_elapsed       | 6.49         |
| total_timesteps    | 27648        |
| value_loss         | 0.03796327   |
-------------------------------------
--------------------------------------
| approxkl           | 0.0039852755  |
| clipfrac           | 0.043945312   |
| ep_len_mean        | 100           |
| ep_reward_mean     | -1.85         |
| explained_variance | 0.717         |
| fps                | 4697          |
| n_updates          | 28            |
| policy_entropy     | 8.579135      |
| policy_loss        | -0.0073735556 |
| serial_timesteps   | 3584          |
| time_elapsed       | 6.72          |
| total_timesteps    | 28672         |
| value_loss         | 0.046333555   |
--------------------------------------
-------------------------------------
| approxkl           | 0.0041445694 |
| clipfrac           | 0.04736328   |
| ep_len_mean        | 100          |
| ep_reward_mean     | -1.71        |
| explained_variance | 0.854        |
| fps                | 4742         |
| n_updates          | 29           |
| policy_entropy     | 8.578959     |
| policy_loss        | -0.008808098 |
| serial_timesteps   | 3712         |
| time_elapsed       | 6.94         |
| total_timesteps    | 29696        |
| value_loss         | 0.025954043  |
-------------------------------------
Eval num_timesteps=30000, episode_reward=-3.35 +/- 0.02
Episode length: 100.00 +/- 0.00
--------------------------------------
| approxkl           | 0.0034244673  |
| clipfrac           | 0.036132812   |
| ep_len_mean        | 100           |
| ep_reward_mean     | -1.55         |
| explained_variance | 0.802         |
| fps                | 1987          |
| n_updates          | 30            |
| policy_entropy     | 8.580734      |
| policy_loss        | -0.0069641005 |
| serial_timesteps   | 3840          |
| time_elapsed       | 7.15          |
| total_timesteps    | 30720         |
| value_loss         | 0.024115203   |
--------------------------------------
-------------------------------------
| approxkl           | 0.002057212  |
| clipfrac           | 0.016845703  |
| ep_len_mean        | 100          |
| ep_reward_mean     | -1.41        |
| explained_variance | 0.752        |
| fps                | 4570         |
| n_updates          | 31           |
| policy_entropy     | 8.576653     |
| policy_loss        | -0.004236102 |
| serial_timesteps   | 3968         |
| time_elapsed       | 7.67         |
| total_timesteps    | 31744        |
| value_loss         | 0.015195384  |
-------------------------------------
-------------------------------------
| approxkl           | 0.0031394344 |
| clipfrac           | 0.033691406  |
| ep_len_mean        | 100          |
| ep_reward_mean     | -1.34        |
| explained_variance | 0.402        |
| fps                | 4836         |
| n_updates          | 32           |
| policy_entropy     | 8.576986     |
| policy_loss        | -0.005875037 |
| serial_timesteps   | 4096         |
| time_elapsed       | 7.89         |
| total_timesteps    | 32768        |
| value_loss         | 0.026293796  |
-------------------------------------
-------------------------------------
| approxkl           | 0.0029477475 |
| clipfrac           | 0.027832031  |
| ep_len_mean        | 100          |
| ep_reward_mean     | -1.25        |
| explained_variance | 0.57         |
| fps                | 4720         |
| n_updates          | 33           |
| policy_entropy     | 8.579592     |
| policy_loss        | -0.005309215 |
| serial_timesteps   | 4224         |
| time_elapsed       | 8.11         |
| total_timesteps    | 33792        |
| value_loss         | 0.026211746  |
-------------------------------------
--------------------------------------
| approxkl           | 0.002325723   |
| clipfrac           | 0.01977539    |
| ep_len_mean        | 100           |
| ep_reward_mean     | -1.24         |
| explained_variance | 0.62          |
| fps                | 4605          |
| n_updates          | 34            |
| policy_entropy     | 8.58201       |
| policy_loss        | -0.0048591406 |
| serial_timesteps   | 4352          |
| time_elapsed       | 8.32          |
| total_timesteps    | 34816         |
| value_loss         | 0.013981712   |
--------------------------------------
--------------------------------------
| approxkl           | 0.0030864957  |
| clipfrac           | 0.034423828   |
| ep_len_mean        | 100           |
| ep_reward_mean     | -1.13         |
| explained_variance | 0.764         |
| fps                | 4731          |
| n_updates          | 35            |
| policy_entropy     | 8.582214      |
| policy_loss        | -0.0067101084 |
| serial_timesteps   | 4480          |
| time_elapsed       | 8.55          |
| total_timesteps    | 35840         |
| value_loss         | 0.011892534   |
--------------------------------------
--------------------------------------
| approxkl           | 0.0025435484  |
| clipfrac           | 0.017333984   |
| ep_len_mean        | 100           |
| ep_reward_mean     | -1.11         |
| explained_variance | 0.721         |
| fps                | 4498          |
| n_updates          | 36            |
| policy_entropy     | 8.584447      |
| policy_loss        | -0.0039735273 |
| serial_timesteps   | 4608          |
| time_elapsed       | 8.76          |
| total_timesteps    | 36864         |
| value_loss         | 0.012861079   |
--------------------------------------
Saving to logs/benchmark/train_widowx_reacher-v34_sparseDense_noptepochs4_10M/ppo2/widowx_reacher-v34_3
pybullet build time: May 18 2020 02:46:26
