--------------------------------------------------------------------------
WARNING: There was an error initializing an OpenFabrics device.

  Local host:   n295
  Local device: hfi1_0
--------------------------------------------------------------------------
WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/td3/td3.py:131: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/td3/policies.py:124: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/common/tf_layers.py:57: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/td3/td3.py:194: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /ichec/home/users/pierre/.conda/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/td3/td3.py:226: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /ichec/home/users/pierre/WidowX-reacher/stable-baselines/stable_baselines/td3/td3.py:240: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

========== widowx_reacher-v8 ==========
Seed: 1
OrderedDict([('batch_size', 256),
             ('buffer_size', 1000000),
             ('gamma', 0.95),
             ('goal_selection_strategy', 'future'),
             ('learning_rate', 0.001),
             ('learning_starts', 1000),
             ('model_class', 'td3'),
             ('n_sampled_goal', 4),
             ('n_timesteps', 25000.0),
             ('policy', 'MlpPolicy')])
Using 1 environments
Overwriting n_timesteps with n=500000
Creating test environment
TRAINING ENV TYPE :  <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7faa577566d8>
Not replacing HERGoalEnvWrapper env by a DummyVecEnv
EVAL ENV TYPE :  <stable_baselines.her.utils.HERGoalEnvWrapper object at 0x7faa550c5160>
Log path: logs/train_0.5M_widowx_reacher-v7_KAY/her/widowx_reacher-v8_2
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -2.19         |
| episodes                | 100           |
| eplenmean               | 100           |
| fps                     | 287           |
| mean 100 episode reward | -2.2          |
| n_updates               | 9000          |
| qf1_loss                | 0.00012530875 |
| qf2_loss                | 0.00013374793 |
| time_elapsed            | 34            |
| total timesteps         | 9900          |
-------------------------------------------
Eval num_timesteps=10000, episode_reward=-7.04 +/- 1.70
Episode length: 100.00 +/- 0.00
New best mean reward!
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -1.56         |
| episodes                | 200           |
| eplenmean               | 100           |
| fps                     | 272           |
| mean 100 episode reward | -1.6          |
| n_updates               | 19000         |
| qf1_loss                | 0.00015340062 |
| qf2_loss                | 0.00015911198 |
| time_elapsed            | 73            |
| total timesteps         | 19900         |
-------------------------------------------
Eval num_timesteps=20000, episode_reward=-3.20 +/- 2.29
Episode length: 100.00 +/- 0.00
New best mean reward!
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -1.13         |
| episodes                | 300           |
| eplenmean               | 100           |
| fps                     | 267           |
| mean 100 episode reward | -1.1          |
| n_updates               | 29000         |
| qf1_loss                | 0.00012769032 |
| qf2_loss                | 0.0001229638  |
| time_elapsed            | 111           |
| total timesteps         | 29900         |
-------------------------------------------
Eval num_timesteps=30000, episode_reward=-3.13 +/- 2.22
Episode length: 100.00 +/- 0.00
New best mean reward!
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -1.21         |
| episodes                | 400           |
| eplenmean               | 100           |
| fps                     | 264           |
| mean 100 episode reward | -1.2          |
| n_updates               | 39000         |
| qf1_loss                | 0.00014877411 |
| qf2_loss                | 0.00013804958 |
| time_elapsed            | 150           |
| total timesteps         | 39900         |
-------------------------------------------
Eval num_timesteps=40000, episode_reward=-0.71 +/- 0.44
Episode length: 100.00 +/- 0.00
New best mean reward!
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.717        |
| episodes                | 500           |
| eplenmean               | 100           |
| fps                     | 262           |
| mean 100 episode reward | -0.7          |
| n_updates               | 49000         |
| qf1_loss                | 0.00013727661 |
| qf2_loss                | 0.00013316608 |
| time_elapsed            | 190           |
| total timesteps         | 49900         |
-------------------------------------------
Eval num_timesteps=50000, episode_reward=-4.82 +/- 3.56
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.428        |
| episodes                | 600           |
| eplenmean               | 100           |
| fps                     | 260           |
| mean 100 episode reward | -0.4          |
| n_updates               | 59000         |
| qf1_loss                | 7.2677e-05    |
| qf2_loss                | 7.0256676e-05 |
| time_elapsed            | 229           |
| total timesteps         | 59900         |
-------------------------------------------
Eval num_timesteps=60000, episode_reward=-5.49 +/- 3.39
Episode length: 100.00 +/- 0.00
------------------------------------------
| current_lr              | 0.001        |
| ep_rewmean              | -0.375       |
| episodes                | 700          |
| eplenmean               | 100          |
| fps                     | 259          |
| mean 100 episode reward | -0.4         |
| n_updates               | 69000        |
| qf1_loss                | 8.120811e-05 |
| qf2_loss                | 8.020876e-05 |
| time_elapsed            | 269          |
| total timesteps         | 69900        |
------------------------------------------
Eval num_timesteps=70000, episode_reward=-2.43 +/- 2.72
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.357        |
| episodes                | 800           |
| eplenmean               | 100           |
| fps                     | 258           |
| mean 100 episode reward | -0.4          |
| n_updates               | 79000         |
| qf1_loss                | 7.3818315e-05 |
| qf2_loss                | 7.223284e-05  |
| time_elapsed            | 309           |
| total timesteps         | 79900         |
-------------------------------------------
Eval num_timesteps=80000, episode_reward=-3.35 +/- 1.45
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.347        |
| episodes                | 900           |
| eplenmean               | 100           |
| fps                     | 257           |
| mean 100 episode reward | -0.3          |
| n_updates               | 89000         |
| qf1_loss                | 5.0980354e-05 |
| qf2_loss                | 4.5581204e-05 |
| time_elapsed            | 348           |
| total timesteps         | 89900         |
-------------------------------------------
Eval num_timesteps=90000, episode_reward=-4.03 +/- 2.73
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.34         |
| episodes                | 1000          |
| eplenmean               | 100           |
| fps                     | 256           |
| mean 100 episode reward | -0.3          |
| n_updates               | 99000         |
| qf1_loss                | 6.38286e-05   |
| qf2_loss                | 6.0852683e-05 |
| time_elapsed            | 389           |
| total timesteps         | 99900         |
-------------------------------------------
Eval num_timesteps=100000, episode_reward=-1.57 +/- 1.69
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.446        |
| episodes                | 1100          |
| eplenmean               | 100           |
| fps                     | 255           |
| mean 100 episode reward | -0.4          |
| n_updates               | 109000        |
| qf1_loss                | 5.3786556e-05 |
| qf2_loss                | 5.6181634e-05 |
| time_elapsed            | 429           |
| total timesteps         | 109900        |
-------------------------------------------
Eval num_timesteps=110000, episode_reward=-1.68 +/- 0.90
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.33         |
| episodes                | 1200          |
| eplenmean               | 100           |
| fps                     | 254           |
| mean 100 episode reward | -0.3          |
| n_updates               | 119000        |
| qf1_loss                | 5.5373897e-05 |
| qf2_loss                | 5.3447227e-05 |
| time_elapsed            | 471           |
| total timesteps         | 119900        |
-------------------------------------------
Eval num_timesteps=120000, episode_reward=-5.00 +/- 4.02
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.323        |
| episodes                | 1300          |
| eplenmean               | 100           |
| fps                     | 253           |
| mean 100 episode reward | -0.3          |
| n_updates               | 129000        |
| qf1_loss                | 3.911725e-05  |
| qf2_loss                | 3.8556824e-05 |
| time_elapsed            | 511           |
| total timesteps         | 129900        |
-------------------------------------------
Eval num_timesteps=130000, episode_reward=-4.81 +/- 3.26
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.245        |
| episodes                | 1400          |
| eplenmean               | 100           |
| fps                     | 253           |
| mean 100 episode reward | -0.2          |
| n_updates               | 139000        |
| qf1_loss                | 4.355723e-05  |
| qf2_loss                | 4.2446558e-05 |
| time_elapsed            | 551           |
| total timesteps         | 139900        |
-------------------------------------------
Eval num_timesteps=140000, episode_reward=-2.83 +/- 3.01
Episode length: 100.00 +/- 0.00
------------------------------------------
| current_lr              | 0.001        |
| ep_rewmean              | -0.306       |
| episodes                | 1500         |
| eplenmean               | 100          |
| fps                     | 253          |
| mean 100 episode reward | -0.3         |
| n_updates               | 149000       |
| qf1_loss                | 7.148799e-05 |
| qf2_loss                | 6.837716e-05 |
| time_elapsed            | 590          |
| total timesteps         | 149900       |
------------------------------------------
Eval num_timesteps=150000, episode_reward=-2.16 +/- 0.88
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.287        |
| episodes                | 1600          |
| eplenmean               | 100           |
| fps                     | 253           |
| mean 100 episode reward | -0.3          |
| n_updates               | 159000        |
| qf1_loss                | 3.4565695e-05 |
| qf2_loss                | 3.259922e-05  |
| time_elapsed            | 630           |
| total timesteps         | 159900        |
-------------------------------------------
Eval num_timesteps=160000, episode_reward=-3.31 +/- 1.91
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.312        |
| episodes                | 1700          |
| eplenmean               | 100           |
| fps                     | 253           |
| mean 100 episode reward | -0.3          |
| n_updates               | 169000        |
| qf1_loss                | 2.4825516e-05 |
| qf2_loss                | 2.260084e-05  |
| time_elapsed            | 670           |
| total timesteps         | 169900        |
-------------------------------------------
Eval num_timesteps=170000, episode_reward=-1.30 +/- 0.78
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.256        |
| episodes                | 1800          |
| eplenmean               | 100           |
| fps                     | 253           |
| mean 100 episode reward | -0.3          |
| n_updates               | 179000        |
| qf1_loss                | 1.9945153e-05 |
| qf2_loss                | 1.9811454e-05 |
| time_elapsed            | 710           |
| total timesteps         | 179900        |
-------------------------------------------
Eval num_timesteps=180000, episode_reward=-1.89 +/- 2.08
Episode length: 100.00 +/- 0.00
------------------------------------------
| current_lr              | 0.001        |
| ep_rewmean              | -0.26        |
| episodes                | 1900         |
| eplenmean               | 100          |
| fps                     | 253          |
| mean 100 episode reward | -0.3         |
| n_updates               | 189000       |
| qf1_loss                | 7.021821e-05 |
| qf2_loss                | 6.983069e-05 |
| time_elapsed            | 749          |
| total timesteps         | 189900       |
------------------------------------------
Eval num_timesteps=190000, episode_reward=-1.39 +/- 0.96
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.296        |
| episodes                | 2000          |
| eplenmean               | 100           |
| fps                     | 253           |
| mean 100 episode reward | -0.3          |
| n_updates               | 199000        |
| qf1_loss                | 3.126211e-05  |
| qf2_loss                | 3.2101256e-05 |
| time_elapsed            | 788           |
| total timesteps         | 199900        |
-------------------------------------------
Eval num_timesteps=200000, episode_reward=-6.60 +/- 5.43
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.221        |
| episodes                | 2100          |
| eplenmean               | 100           |
| fps                     | 253           |
| mean 100 episode reward | -0.2          |
| n_updates               | 209000        |
| qf1_loss                | 2.208252e-05  |
| qf2_loss                | 2.0886948e-05 |
| time_elapsed            | 827           |
| total timesteps         | 209900        |
-------------------------------------------
Eval num_timesteps=210000, episode_reward=-4.81 +/- 4.08
Episode length: 100.00 +/- 0.00
------------------------------------------
| current_lr              | 0.001        |
| ep_rewmean              | -0.24        |
| episodes                | 2200         |
| eplenmean               | 100          |
| fps                     | 253          |
| mean 100 episode reward | -0.2         |
| n_updates               | 219000       |
| qf1_loss                | 9.299279e-05 |
| qf2_loss                | 8.962889e-05 |
| time_elapsed            | 866          |
| total timesteps         | 219900       |
------------------------------------------
Eval num_timesteps=220000, episode_reward=-2.96 +/- 2.15
Episode length: 100.00 +/- 0.00
------------------------------------------
| current_lr              | 0.001        |
| ep_rewmean              | -0.3         |
| episodes                | 2300         |
| eplenmean               | 100          |
| fps                     | 253          |
| mean 100 episode reward | -0.3         |
| n_updates               | 229000       |
| qf1_loss                | 1.711529e-05 |
| qf2_loss                | 1.716308e-05 |
| time_elapsed            | 905          |
| total timesteps         | 229900       |
------------------------------------------
Eval num_timesteps=230000, episode_reward=-4.68 +/- 4.70
Episode length: 100.00 +/- 0.00
------------------------------------------
| current_lr              | 0.001        |
| ep_rewmean              | -0.264       |
| episodes                | 2400         |
| eplenmean               | 100          |
| fps                     | 254          |
| mean 100 episode reward | -0.3         |
| n_updates               | 239000       |
| qf1_loss                | 8.721772e-06 |
| qf2_loss                | 8.370652e-06 |
| time_elapsed            | 943          |
| total timesteps         | 239900       |
------------------------------------------
Eval num_timesteps=240000, episode_reward=-4.76 +/- 4.24
Episode length: 100.00 +/- 0.00
------------------------------------------
| current_lr              | 0.001        |
| ep_rewmean              | -0.238       |
| episodes                | 2500         |
| eplenmean               | 100          |
| fps                     | 254          |
| mean 100 episode reward | -0.2         |
| n_updates               | 249000       |
| qf1_loss                | 8.935207e-06 |
| qf2_loss                | 9.063844e-06 |
| time_elapsed            | 982          |
| total timesteps         | 249900       |
------------------------------------------
Eval num_timesteps=250000, episode_reward=-2.34 +/- 2.21
Episode length: 100.00 +/- 0.00
------------------------------------------
| current_lr              | 0.001        |
| ep_rewmean              | -0.187       |
| episodes                | 2600         |
| eplenmean               | 100          |
| fps                     | 254          |
| mean 100 episode reward | -0.2         |
| n_updates               | 259000       |
| qf1_loss                | 8.584062e-06 |
| qf2_loss                | 8.459949e-06 |
| time_elapsed            | 1022         |
| total timesteps         | 259900       |
------------------------------------------
Eval num_timesteps=260000, episode_reward=-1.46 +/- 1.06
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.278        |
| episodes                | 2700          |
| eplenmean               | 100           |
| fps                     | 253           |
| mean 100 episode reward | -0.3          |
| n_updates               | 269000        |
| qf1_loss                | 2.0965568e-05 |
| qf2_loss                | 2.0624822e-05 |
| time_elapsed            | 1062          |
| total timesteps         | 269900        |
-------------------------------------------
Eval num_timesteps=270000, episode_reward=-1.40 +/- 1.35
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.24         |
| episodes                | 2800          |
| eplenmean               | 100           |
| fps                     | 254           |
| mean 100 episode reward | -0.2          |
| n_updates               | 279000        |
| qf1_loss                | 1.9056186e-05 |
| qf2_loss                | 1.940176e-05  |
| time_elapsed            | 1101          |
| total timesteps         | 279900        |
-------------------------------------------
Eval num_timesteps=280000, episode_reward=-0.87 +/- 0.36
Episode length: 100.00 +/- 0.00
------------------------------------------
| current_lr              | 0.001        |
| ep_rewmean              | -0.21        |
| episodes                | 2900         |
| eplenmean               | 99.4         |
| fps                     | 254          |
| mean 100 episode reward | -0.2         |
| n_updates               | 288900       |
| qf1_loss                | 4.624903e-06 |
| qf2_loss                | 5.40158e-06  |
| time_elapsed            | 1140         |
| total timesteps         | 289843       |
------------------------------------------
Eval num_timesteps=290000, episode_reward=-2.70 +/- 2.57
Episode length: 100.00 +/- 0.00
------------------------------------------
| current_lr              | 0.001        |
| ep_rewmean              | -0.208       |
| episodes                | 3000         |
| eplenmean               | 100          |
| fps                     | 254          |
| mean 100 episode reward | -0.2         |
| n_updates               | 298900       |
| qf1_loss                | 7.488457e-06 |
| qf2_loss                | 6.992941e-06 |
| time_elapsed            | 1179         |
| total timesteps         | 299843       |
------------------------------------------
Eval num_timesteps=300000, episode_reward=-3.94 +/- 3.77
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.234        |
| episodes                | 3100          |
| eplenmean               | 100           |
| fps                     | 254           |
| mean 100 episode reward | -0.2          |
| n_updates               | 308900        |
| qf1_loss                | 5.274231e-06  |
| qf2_loss                | 4.8671486e-06 |
| time_elapsed            | 1219          |
| total timesteps         | 309843        |
-------------------------------------------
Eval num_timesteps=310000, episode_reward=-1.79 +/- 0.89
Episode length: 100.00 +/- 0.00
------------------------------------------
| current_lr              | 0.001        |
| ep_rewmean              | -0.202       |
| episodes                | 3200         |
| eplenmean               | 100          |
| fps                     | 254          |
| mean 100 episode reward | -0.2         |
| n_updates               | 318900       |
| qf1_loss                | 8.614839e-06 |
| qf2_loss                | 8.69023e-06  |
| time_elapsed            | 1258         |
| total timesteps         | 319843       |
------------------------------------------
Eval num_timesteps=320000, episode_reward=-4.55 +/- 5.12
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.215        |
| episodes                | 3300          |
| eplenmean               | 100           |
| fps                     | 254           |
| mean 100 episode reward | -0.2          |
| n_updates               | 328900        |
| qf1_loss                | 4.6003693e-06 |
| qf2_loss                | 4.411058e-06  |
| time_elapsed            | 1296          |
| total timesteps         | 329843        |
-------------------------------------------
Eval num_timesteps=330000, episode_reward=-2.73 +/- 2.35
Episode length: 100.00 +/- 0.00
------------------------------------------
| current_lr              | 0.001        |
| ep_rewmean              | -0.186       |
| episodes                | 3400         |
| eplenmean               | 100          |
| fps                     | 254          |
| mean 100 episode reward | -0.2         |
| n_updates               | 338900       |
| qf1_loss                | 4.749038e-06 |
| qf2_loss                | 5.0816e-06   |
| time_elapsed            | 1335         |
| total timesteps         | 339843       |
------------------------------------------
Eval num_timesteps=340000, episode_reward=-2.05 +/- 0.72
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.2          |
| episodes                | 3500          |
| eplenmean               | 99.4          |
| fps                     | 254           |
| mean 100 episode reward | -0.2          |
| n_updates               | 348800        |
| qf1_loss                | 3.9998135e-06 |
| qf2_loss                | 4.2026777e-06 |
| time_elapsed            | 1373          |
| total timesteps         | 349780        |
-------------------------------------------
Eval num_timesteps=350000, episode_reward=-1.60 +/- 0.79
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.203        |
| episodes                | 3600          |
| eplenmean               | 100           |
| fps                     | 254           |
| mean 100 episode reward | -0.2          |
| n_updates               | 358800        |
| qf1_loss                | 5.1714346e-06 |
| qf2_loss                | 4.812773e-06  |
| time_elapsed            | 1413          |
| total timesteps         | 359780        |
-------------------------------------------
Eval num_timesteps=360000, episode_reward=-2.41 +/- 1.45
Episode length: 100.00 +/- 0.00
------------------------------------------
| current_lr              | 0.001        |
| ep_rewmean              | -0.202       |
| episodes                | 3700         |
| eplenmean               | 100          |
| fps                     | 254          |
| mean 100 episode reward | -0.2         |
| n_updates               | 368800       |
| qf1_loss                | 3.958283e-06 |
| qf2_loss                | 3.880965e-06 |
| time_elapsed            | 1453         |
| total timesteps         | 369780       |
------------------------------------------
Eval num_timesteps=370000, episode_reward=-1.99 +/- 2.47
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.182        |
| episodes                | 3800          |
| eplenmean               | 99.5          |
| fps                     | 254           |
| mean 100 episode reward | -0.2          |
| n_updates               | 378800        |
| qf1_loss                | 6.751056e-06  |
| qf2_loss                | 6.4616256e-06 |
| time_elapsed            | 1491          |
| total timesteps         | 379735        |
-------------------------------------------
Eval num_timesteps=380000, episode_reward=-6.44 +/- 2.62
Episode length: 100.00 +/- 0.00
------------------------------------------
| current_lr              | 0.001        |
| ep_rewmean              | -0.185       |
| episodes                | 3900         |
| eplenmean               | 100          |
| fps                     | 254          |
| mean 100 episode reward | -0.2         |
| n_updates               | 388800       |
| qf1_loss                | 9.367665e-06 |
| qf2_loss                | 9.630887e-06 |
| time_elapsed            | 1530         |
| total timesteps         | 389735       |
------------------------------------------
Eval num_timesteps=390000, episode_reward=-3.06 +/- 3.82
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.205        |
| episodes                | 4000          |
| eplenmean               | 100           |
| fps                     | 254           |
| mean 100 episode reward | -0.2          |
| n_updates               | 398800        |
| qf1_loss                | 5.2905634e-06 |
| qf2_loss                | 5.2438936e-06 |
| time_elapsed            | 1570          |
| total timesteps         | 399735        |
-------------------------------------------
Eval num_timesteps=400000, episode_reward=-2.16 +/- 0.89
Episode length: 100.00 +/- 0.00
------------------------------------------
| current_lr              | 0.001        |
| ep_rewmean              | -0.199       |
| episodes                | 4100         |
| eplenmean               | 100          |
| fps                     | 254          |
| mean 100 episode reward | -0.2         |
| n_updates               | 408800       |
| qf1_loss                | 3.654644e-06 |
| qf2_loss                | 3.724619e-06 |
| time_elapsed            | 1609         |
| total timesteps         | 409735       |
------------------------------------------
Eval num_timesteps=410000, episode_reward=-2.36 +/- 3.33
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.211        |
| episodes                | 4200          |
| eplenmean               | 99.8          |
| fps                     | 254           |
| mean 100 episode reward | -0.2          |
| n_updates               | 418800        |
| qf1_loss                | 2.976739e-06  |
| qf2_loss                | 3.0363747e-06 |
| time_elapsed            | 1648          |
| total timesteps         | 419719        |
-------------------------------------------
Eval num_timesteps=420000, episode_reward=-1.87 +/- 1.26
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.216        |
| episodes                | 4300          |
| eplenmean               | 100           |
| fps                     | 254           |
| mean 100 episode reward | -0.2          |
| n_updates               | 428800        |
| qf1_loss                | 3.4819016e-06 |
| qf2_loss                | 3.3321517e-06 |
| time_elapsed            | 1687          |
| total timesteps         | 429719        |
-------------------------------------------
Eval num_timesteps=430000, episode_reward=-1.32 +/- 0.62
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.197        |
| episodes                | 4400          |
| eplenmean               | 100           |
| fps                     | 254           |
| mean 100 episode reward | -0.2          |
| n_updates               | 438800        |
| qf1_loss                | 3.3392787e-06 |
| qf2_loss                | 3.4718128e-06 |
| time_elapsed            | 1726          |
| total timesteps         | 439719        |
-------------------------------------------
Eval num_timesteps=440000, episode_reward=-1.77 +/- 0.88
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.208        |
| episodes                | 4500          |
| eplenmean               | 100           |
| fps                     | 254           |
| mean 100 episode reward | -0.2          |
| n_updates               | 448800        |
| qf1_loss                | 3.9083866e-06 |
| qf2_loss                | 3.4412897e-06 |
| time_elapsed            | 1764          |
| total timesteps         | 449719        |
-------------------------------------------
Eval num_timesteps=450000, episode_reward=-1.44 +/- 1.20
Episode length: 100.00 +/- 0.00
------------------------------------------
| current_lr              | 0.001        |
| ep_rewmean              | -0.222       |
| episodes                | 4600         |
| eplenmean               | 100          |
| fps                     | 254          |
| mean 100 episode reward | -0.2         |
| n_updates               | 458800       |
| qf1_loss                | 5.587152e-06 |
| qf2_loss                | 5.186096e-06 |
| time_elapsed            | 1803         |
| total timesteps         | 459719       |
------------------------------------------
Eval num_timesteps=460000, episode_reward=-3.79 +/- 2.46
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.23         |
| episodes                | 4700          |
| eplenmean               | 100           |
| fps                     | 255           |
| mean 100 episode reward | -0.2          |
| n_updates               | 468800        |
| qf1_loss                | 7.434586e-06  |
| qf2_loss                | 7.2846806e-06 |
| time_elapsed            | 1841          |
| total timesteps         | 469719        |
-------------------------------------------
Eval num_timesteps=470000, episode_reward=-1.68 +/- 1.77
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.184        |
| episodes                | 4800          |
| eplenmean               | 100           |
| fps                     | 255           |
| mean 100 episode reward | -0.2          |
| n_updates               | 478800        |
| qf1_loss                | 3.3806455e-06 |
| qf2_loss                | 2.9317628e-06 |
| time_elapsed            | 1880          |
| total timesteps         | 479719        |
-------------------------------------------
Eval num_timesteps=480000, episode_reward=-3.93 +/- 1.78
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.192        |
| episodes                | 4900          |
| eplenmean               | 100           |
| fps                     | 255           |
| mean 100 episode reward | -0.2          |
| n_updates               | 488800        |
| qf1_loss                | 6.0739667e-06 |
| qf2_loss                | 5.4604066e-06 |
| time_elapsed            | 1918          |
| total timesteps         | 489719        |
-------------------------------------------
Eval num_timesteps=490000, episode_reward=-1.98 +/- 1.43
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ep_rewmean              | -0.219        |
| episodes                | 5000          |
| eplenmean               | 100           |
| fps                     | 255           |
| mean 100 episode reward | -0.2          |
| n_updates               | 498800        |
| qf1_loss                | 3.4222016e-06 |
| qf2_loss                | 3.045526e-06  |
| time_elapsed            | 1956          |
| total timesteps         | 499719        |
-------------------------------------------
Eval num_timesteps=500000, episode_reward=-2.33 +/- 2.03
Episode length: 100.00 +/- 0.00
Saving to logs/train_0.5M_widowx_reacher-v7_KAY/her/widowx_reacher-v8_2
pybullet build time: May 18 2020 02:46:26
