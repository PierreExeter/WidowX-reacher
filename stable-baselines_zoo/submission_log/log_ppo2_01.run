WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/ppo2/ppo2.py:190: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/ppo2/ppo2.py:206: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/ppo2/ppo2.py:242: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

========== widowx_reacher-v5 ==========
Seed: 1
OrderedDict([('cliprange', 0.2),
             ('ent_coef', 0.01),
             ('gamma', 0.99),
             ('lam', 0.95),
             ('learning_rate', 0.00025),
             ('max_grad_norm', 0.5),
             ('n_envs', 8),
             ('n_steps', 128),
             ('n_timesteps', 10000),
             ('nminibatches', 4),
             ('noptepochs', 4),
             ('normalize', True),
             ('policy', 'MlpPolicy'),
             ('policy_kwargs', 'None'),
             ('vf_coef', 0.5)])
Using 8 environments
Normalizing input and reward
Creating test environment
TRAINING ENV TYPE :  <stable_baselines.common.vec_env.vec_normalize.VecNormalize object at 0x7f5560125908>
Normalization activated: {'norm_reward': False}
EVAL ENV TYPE :  <stable_baselines.common.vec_env.vec_normalize.VecNormalize object at 0x7f55600c7898>
Log path: logs/train_10K_widowx_reacher-v5/ppo2/widowx_reacher-v5_2
--------------------------------------
| approxkl           | 0.00043755304 |
| clipfrac           | 0.00024414062 |
| ep_len_mean        | 100           |
| ep_reward_mean     | -2.85         |
| explained_variance | 0.0466        |
| fps                | 2598          |
| n_updates          | 1             |
| policy_entropy     | 8.516618      |
| policy_loss        | -0.0038388544 |
| serial_timesteps   | 128           |
| time_elapsed       | 4.34e-05      |
| total_timesteps    | 1024          |
| value_loss         | 1.4683361     |
--------------------------------------
--------------------------------------
| approxkl           | 0.00038397545 |
| clipfrac           | 0.0           |
| ep_len_mean        | 100           |
| ep_reward_mean     | -2.65         |
| explained_variance | 0.334         |
| fps                | 5757          |
| n_updates          | 2             |
| policy_entropy     | 8.524964      |
| policy_loss        | -0.0039805896 |
| serial_timesteps   | 256           |
| time_elapsed       | 0.394         |
| total_timesteps    | 2048          |
| value_loss         | 0.17968158    |
--------------------------------------
--------------------------------------
| approxkl           | 0.00033048485 |
| clipfrac           | 0.0007324219  |
| ep_len_mean        | 100           |
| ep_reward_mean     | -2.89         |
| explained_variance | 0.541         |
| fps                | 5682          |
| n_updates          | 3             |
| policy_entropy     | 8.530543      |
| policy_loss        | -0.00289787   |
| serial_timesteps   | 384           |
| time_elapsed       | 0.572         |
| total_timesteps    | 3072          |
| value_loss         | 0.116452634   |
--------------------------------------
--------------------------------------
| approxkl           | 0.0002918453  |
| clipfrac           | 0.00024414062 |
| ep_len_mean        | 100           |
| ep_reward_mean     | -2.89         |
| explained_variance | 0.539         |
| fps                | 5541          |
| n_updates          | 4             |
| policy_entropy     | 8.530525      |
| policy_loss        | -0.0022178083 |
| serial_timesteps   | 512           |
| time_elapsed       | 0.753         |
| total_timesteps    | 4096          |
| value_loss         | 0.10972227    |
--------------------------------------
--------------------------------------
| approxkl           | 0.00023481794 |
| clipfrac           | 0.0           |
| ep_len_mean        | 100           |
| ep_reward_mean     | -2.92         |
| explained_variance | 0.587         |
| fps                | 5791          |
| n_updates          | 5             |
| policy_entropy     | 8.531169      |
| policy_loss        | -0.0019568978 |
| serial_timesteps   | 640           |
| time_elapsed       | 0.938         |
| total_timesteps    | 5120          |
| value_loss         | 0.13875394    |
--------------------------------------
-------------------------------------
| approxkl           | 0.000282606  |
| clipfrac           | 0.0          |
| ep_len_mean        | 100          |
| ep_reward_mean     | -2.87        |
| explained_variance | 0.602        |
| fps                | 5912         |
| n_updates          | 6            |
| policy_entropy     | 8.535593     |
| policy_loss        | -0.004561969 |
| serial_timesteps   | 768          |
| time_elapsed       | 1.11         |
| total_timesteps    | 6144         |
| value_loss         | 0.11398821   |
-------------------------------------
--------------------------------------
| approxkl           | 0.00054456934 |
| clipfrac           | 0.0017089844  |
| ep_len_mean        | 100           |
| ep_reward_mean     | -2.83         |
| explained_variance | 0.678         |
| fps                | 5779          |
| n_updates          | 7             |
| policy_entropy     | 8.540639      |
| policy_loss        | -0.004111342  |
| serial_timesteps   | 896           |
| time_elapsed       | 1.29          |
| total_timesteps    | 7168          |
| value_loss         | 0.10204731    |
--------------------------------------
--------------------------------------
| approxkl           | 0.0005661923  |
| clipfrac           | 0.0009765625  |
| ep_len_mean        | 100           |
| ep_reward_mean     | -2.76         |
| explained_variance | 0.74          |
| fps                | 5870          |
| n_updates          | 8             |
| policy_entropy     | 8.542035      |
| policy_loss        | -0.0062566353 |
| serial_timesteps   | 1024          |
| time_elapsed       | 1.47          |
| total_timesteps    | 8192          |
| value_loss         | 0.09674843    |
--------------------------------------
--------------------------------------
| approxkl           | 0.00028440647 |
| clipfrac           | 0.0           |
| ep_len_mean        | 100           |
| ep_reward_mean     | -2.73         |
| explained_variance | 0.751         |
| fps                | 5622          |
| n_updates          | 9             |
| policy_entropy     | 8.54388       |
| policy_loss        | -0.0032662188 |
| serial_timesteps   | 1152          |
| time_elapsed       | 1.64          |
| total_timesteps    | 9216          |
| value_loss         | 0.10132716    |
--------------------------------------
Saving to logs/train_10K_widowx_reacher-v5/ppo2/widowx_reacher-v5_2
pybullet build time: May 18 2020 02:46:26
