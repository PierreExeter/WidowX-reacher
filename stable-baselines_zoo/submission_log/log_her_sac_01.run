WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/sac/sac.py:141: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/sac/policies.py:194: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_layers.py:57: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/sac/sac.py:232: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/sac/sac.py:294: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/sac/sac.py:314: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

========== widowx_reacher-v8 ==========
Seed: 1
OrderedDict([('batch_size', 256),
             ('buffer_size', 1000000),
             ('ent_coef', 'auto'),
             ('gamma', 0.95),
             ('goal_selection_strategy', 'future'),
             ('learning_rate', 0.001),
             ('learning_starts', 1000),
             ('model_class', 'sac'),
             ('n_sampled_goal', 4),
             ('n_timesteps', 20000.0),
             ('policy', 'MlpPolicy')])
Using 1 environments
Overwriting n_timesteps with n=200000
Creating test environment
TRAINING ENV TYPE :  <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f2f407422e8>
Not replacing HERGoalEnvWrapper env by a DummyVecEnv
EVAL ENV TYPE :  <stable_baselines.her.utils.HERGoalEnvWrapper object at 0x7f2f4003ae80>
Log path: logs/train_0.2M_widowx_reacher-v7_HER_SAC_G5/her/widowx_reacher-v8_2
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 0.0049712057  |
| ent_coef_loss           | -8.4626875    |
| entropy                 | 5.5002213     |
| ep_rewmean              | -1.73         |
| episodes                | 100           |
| eplenmean               | 100           |
| fps                     | 310           |
| mean 100 episode reward | -1.7          |
| n_updates               | 8901          |
| policy_loss             | -2.0463314    |
| qf1_loss                | 0.00036790824 |
| qf2_loss                | 0.00039982522 |
| time_elapsed            | 31            |
| total timesteps         | 9900          |
| value_loss              | 0.0014032263  |
-------------------------------------------
Eval num_timesteps=10000, episode_reward=-4.24 +/- 3.46
Episode length: 100.00 +/- 0.00
New best mean reward!
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 0.00043042985 |
| ent_coef_loss           | -1.0948266    |
| entropy                 | 3.850605      |
| ep_rewmean              | -2.18         |
| episodes                | 200           |
| eplenmean               | 100           |
| fps                     | 293           |
| mean 100 episode reward | -2.2          |
| n_updates               | 18901         |
| policy_loss             | 0.06780053    |
| qf1_loss                | 0.00012455578 |
| qf2_loss                | 9.294036e-05  |
| time_elapsed            | 67            |
| total timesteps         | 19900         |
| value_loss              | 0.00014382455 |
-------------------------------------------
Eval num_timesteps=20000, episode_reward=-1.17 +/- 0.90
Episode length: 100.00 +/- 0.00
New best mean reward!
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 0.0002846046  |
| ent_coef_loss           | 9.026634      |
| entropy                 | 3.6616545     |
| ep_rewmean              | -1.69         |
| episodes                | 300           |
| eplenmean               | 100           |
| fps                     | 289           |
| mean 100 episode reward | -1.7          |
| n_updates               | 28901         |
| policy_loss             | 0.12511978    |
| qf1_loss                | 0.00018921611 |
| qf2_loss                | 0.00014566886 |
| time_elapsed            | 103           |
| total timesteps         | 29900         |
| value_loss              | 0.00014417122 |
-------------------------------------------
Eval num_timesteps=30000, episode_reward=-1.44 +/- 1.20
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 9.567963e-05  |
| ent_coef_loss           | 0.9971221     |
| entropy                 | 3.3534114     |
| ep_rewmean              | -1.08         |
| episodes                | 400           |
| eplenmean               | 100           |
| fps                     | 287           |
| mean 100 episode reward | -1.1          |
| n_updates               | 38901         |
| policy_loss             | 0.08904962    |
| qf1_loss                | 4.1024607e-05 |
| qf2_loss                | 4.17751e-05   |
| time_elapsed            | 138           |
| total timesteps         | 39900         |
| value_loss              | 2.8409184e-05 |
-------------------------------------------
Eval num_timesteps=40000, episode_reward=-4.37 +/- 2.91
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 8.621903e-05  |
| ent_coef_loss           | -12.889376    |
| entropy                 | 4.4610734     |
| ep_rewmean              | -0.722        |
| episodes                | 500           |
| eplenmean               | 100           |
| fps                     | 287           |
| mean 100 episode reward | -0.7          |
| n_updates               | 48901         |
| policy_loss             | 0.06560829    |
| qf1_loss                | 2.1163654e-05 |
| qf2_loss                | 1.942827e-05  |
| time_elapsed            | 173           |
| total timesteps         | 49900         |
| value_loss              | 2.523688e-05  |
-------------------------------------------
Eval num_timesteps=50000, episode_reward=-4.44 +/- 2.32
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 9.01293e-05   |
| ent_coef_loss           | 2.1866183     |
| entropy                 | 4.2467527     |
| ep_rewmean              | -0.534        |
| episodes                | 600           |
| eplenmean               | 100           |
| fps                     | 287           |
| mean 100 episode reward | -0.5          |
| n_updates               | 58901         |
| policy_loss             | 0.0538648     |
| qf1_loss                | 0.0001528723  |
| qf2_loss                | 0.00012265203 |
| time_elapsed            | 208           |
| total timesteps         | 59900         |
| value_loss              | 3.0867694e-05 |
-------------------------------------------
Eval num_timesteps=60000, episode_reward=-4.17 +/- 3.93
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 7.770446e-05  |
| ent_coef_loss           | -6.024767     |
| entropy                 | 3.938204      |
| ep_rewmean              | -0.556        |
| episodes                | 700           |
| eplenmean               | 100           |
| fps                     | 287           |
| mean 100 episode reward | -0.6          |
| n_updates               | 68901         |
| policy_loss             | 0.04829492    |
| qf1_loss                | 8.898447e-05  |
| qf2_loss                | 2.1683898e-05 |
| time_elapsed            | 242           |
| total timesteps         | 69900         |
| value_loss              | 0.00047704772 |
-------------------------------------------
Eval num_timesteps=70000, episode_reward=-4.08 +/- 4.12
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 4.0497736e-05 |
| ent_coef_loss           | -1.4194348    |
| entropy                 | 3.8249335     |
| ep_rewmean              | -0.616        |
| episodes                | 800           |
| eplenmean               | 100           |
| fps                     | 287           |
| mean 100 episode reward | -0.6          |
| n_updates               | 78901         |
| policy_loss             | 0.04120493    |
| qf1_loss                | 1.3704377e-05 |
| qf2_loss                | 1.243572e-05  |
| time_elapsed            | 277           |
| total timesteps         | 79900         |
| value_loss              | 1.76385e-05   |
-------------------------------------------
Eval num_timesteps=80000, episode_reward=-3.86 +/- 3.14
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 4.326134e-05  |
| ent_coef_loss           | -11.337289    |
| entropy                 | 4.9685984     |
| ep_rewmean              | -0.425        |
| episodes                | 900           |
| eplenmean               | 100           |
| fps                     | 287           |
| mean 100 episode reward | -0.4          |
| n_updates               | 88901         |
| policy_loss             | 0.04011937    |
| qf1_loss                | 3.6536858e-05 |
| qf2_loss                | 2.7693655e-05 |
| time_elapsed            | 312           |
| total timesteps         | 89900         |
| value_loss              | 4.4499193e-05 |
-------------------------------------------
Eval num_timesteps=90000, episode_reward=-1.41 +/- 1.90
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 3.059213e-05  |
| ent_coef_loss           | -9.593955     |
| entropy                 | 4.2170744     |
| ep_rewmean              | -0.379        |
| episodes                | 1000          |
| eplenmean               | 100           |
| fps                     | 287           |
| mean 100 episode reward | -0.4          |
| n_updates               | 98901         |
| policy_loss             | 0.040014856   |
| qf1_loss                | 6.8191707e-06 |
| qf2_loss                | 1.2956924e-05 |
| time_elapsed            | 347           |
| total timesteps         | 99900         |
| value_loss              | 1.4281296e-05 |
-------------------------------------------
Eval num_timesteps=100000, episode_reward=-3.67 +/- 1.52
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 3.0341847e-05 |
| ent_coef_loss           | 2.777565      |
| entropy                 | 4.0163918     |
| ep_rewmean              | -0.366        |
| episodes                | 1100          |
| eplenmean               | 100           |
| fps                     | 287           |
| mean 100 episode reward | -0.4          |
| n_updates               | 108901        |
| policy_loss             | 0.037768707   |
| qf1_loss                | 9.230629e-06  |
| qf2_loss                | 6.8412273e-06 |
| time_elapsed            | 382           |
| total timesteps         | 109900        |
| value_loss              | 7.681339e-06  |
-------------------------------------------
Eval num_timesteps=110000, episode_reward=-4.55 +/- 3.28
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 2.4791398e-05 |
| ent_coef_loss           | 9.375017      |
| entropy                 | 4.246214      |
| ep_rewmean              | -0.404        |
| episodes                | 1200          |
| eplenmean               | 100           |
| fps                     | 287           |
| mean 100 episode reward | -0.4          |
| n_updates               | 118901        |
| policy_loss             | 0.041898515   |
| qf1_loss                | 1.169751e-05  |
| qf2_loss                | 1.204074e-05  |
| time_elapsed            | 417           |
| total timesteps         | 119900        |
| value_loss              | 9.704044e-06  |
-------------------------------------------
Eval num_timesteps=120000, episode_reward=-4.14 +/- 3.23
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 3.0260386e-05 |
| ent_coef_loss           | -5.8843236    |
| entropy                 | 4.958248      |
| ep_rewmean              | -0.322        |
| episodes                | 1300          |
| eplenmean               | 100           |
| fps                     | 286           |
| mean 100 episode reward | -0.3          |
| n_updates               | 128901        |
| policy_loss             | 0.040308557   |
| qf1_loss                | 1.0589285e-05 |
| qf2_loss                | 5.923057e-06  |
| time_elapsed            | 452           |
| total timesteps         | 129900        |
| value_loss              | 7.761882e-06  |
-------------------------------------------
Eval num_timesteps=130000, episode_reward=-1.37 +/- 1.20
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 2.9008137e-05 |
| ent_coef_loss           | -9.111452     |
| entropy                 | 4.291047      |
| ep_rewmean              | -0.364        |
| episodes                | 1400          |
| eplenmean               | 100           |
| fps                     | 286           |
| mean 100 episode reward | -0.4          |
| n_updates               | 138901        |
| policy_loss             | 0.03757651    |
| qf1_loss                | 0.0003265305  |
| qf2_loss                | 0.00034275802 |
| time_elapsed            | 487           |
| total timesteps         | 139900        |
| value_loss              | 1.4077239e-05 |
-------------------------------------------
Eval num_timesteps=140000, episode_reward=-1.71 +/- 1.19
Episode length: 100.00 +/- 0.00
------------------------------------------
| current_lr              | 0.001        |
| ent_coef                | 3.071623e-05 |
| ent_coef_loss           | 6.066759     |
| entropy                 | 4.930789     |
| ep_rewmean              | -0.371       |
| episodes                | 1500         |
| eplenmean               | 100          |
| fps                     | 286          |
| mean 100 episode reward | -0.4         |
| n_updates               | 148901       |
| policy_loss             | 0.041691244  |
| qf1_loss                | 6.065939e-06 |
| qf2_loss                | 7.741664e-06 |
| time_elapsed            | 522          |
| total timesteps         | 149900       |
| value_loss              | 6.292825e-06 |
------------------------------------------
Eval num_timesteps=150000, episode_reward=-4.68 +/- 3.00
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 2.7813148e-05 |
| ent_coef_loss           | -0.60019135   |
| entropy                 | 4.989752      |
| ep_rewmean              | -0.317        |
| episodes                | 1600          |
| eplenmean               | 100           |
| fps                     | 287           |
| mean 100 episode reward | -0.3          |
| n_updates               | 158901        |
| policy_loss             | 0.034645468   |
| qf1_loss                | 0.00030961353 |
| qf2_loss                | 0.00032976962 |
| time_elapsed            | 556           |
| total timesteps         | 159900        |
| value_loss              | 4.7509184e-06 |
-------------------------------------------
Eval num_timesteps=160000, episode_reward=-4.51 +/- 4.65
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 2.7968697e-05 |
| ent_coef_loss           | -10.231834    |
| entropy                 | 5.1433144     |
| ep_rewmean              | -0.31         |
| episodes                | 1700          |
| eplenmean               | 100           |
| fps                     | 287           |
| mean 100 episode reward | -0.3          |
| n_updates               | 168901        |
| policy_loss             | 0.029150065   |
| qf1_loss                | 7.108668e-06  |
| qf2_loss                | 5.5052774e-06 |
| time_elapsed            | 590           |
| total timesteps         | 169900        |
| value_loss              | 8.671322e-06  |
-------------------------------------------
Eval num_timesteps=170000, episode_reward=-2.09 +/- 2.45
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 3.0395542e-05 |
| ent_coef_loss           | 9.75872       |
| entropy                 | 5.137334      |
| ep_rewmean              | -0.284        |
| episodes                | 1800          |
| eplenmean               | 100           |
| fps                     | 288           |
| mean 100 episode reward | -0.3          |
| n_updates               | 178901        |
| policy_loss             | 0.03381194    |
| qf1_loss                | 7.000661e-06  |
| qf2_loss                | 4.510527e-06  |
| time_elapsed            | 624           |
| total timesteps         | 179900        |
| value_loss              | 5.303736e-06  |
-------------------------------------------
Eval num_timesteps=180000, episode_reward=-0.87 +/- 0.68
Episode length: 100.00 +/- 0.00
New best mean reward!
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 3.0436993e-05 |
| ent_coef_loss           | -9.109703     |
| entropy                 | 5.6502714     |
| ep_rewmean              | -0.298        |
| episodes                | 1900          |
| eplenmean               | 100           |
| fps                     | 288           |
| mean 100 episode reward | -0.3          |
| n_updates               | 188901        |
| policy_loss             | 0.02915577    |
| qf1_loss                | 6.932733e-06  |
| qf2_loss                | 5.810977e-06  |
| time_elapsed            | 658           |
| total timesteps         | 189900        |
| value_loss              | 4.275558e-06  |
-------------------------------------------
Eval num_timesteps=190000, episode_reward=-4.18 +/- 3.98
Episode length: 100.00 +/- 0.00
-------------------------------------------
| current_lr              | 0.001         |
| ent_coef                | 2.4268593e-05 |
| ent_coef_loss           | -9.813667     |
| entropy                 | 5.464432      |
| ep_rewmean              | -0.305        |
| episodes                | 2000          |
| eplenmean               | 100           |
| fps                     | 288           |
| mean 100 episode reward | -0.3          |
| n_updates               | 198901        |
| policy_loss             | 0.025862467   |
| qf1_loss                | 3.631852e-06  |
| qf2_loss                | 3.602876e-06  |
| time_elapsed            | 692           |
| total timesteps         | 199900        |
| value_loss              | 3.948473e-06  |
-------------------------------------------
Eval num_timesteps=200000, episode_reward=-7.35 +/- 5.28
Episode length: 100.00 +/- 0.00
Saving to logs/train_0.2M_widowx_reacher-v7_HER_SAC_G5/her/widowx_reacher-v8_2
pybullet build time: May 18 2020 02:46:26
