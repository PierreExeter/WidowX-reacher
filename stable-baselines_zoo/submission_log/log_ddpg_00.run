[sonicgpu1.compute:22582] pml_ucx.c:285  Error: UCP worker does not support MPI_THREAD_MULTIPLE
WARNING:tensorflow:From /home/people/paumjaud/WidowX-reacher/stable-baselines/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING:tensorflow:From /home/people/paumjaud/WidowX-reacher/stable-baselines/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /home/people/paumjaud/WidowX-reacher/stable-baselines/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /home/people/paumjaud/WidowX-reacher/stable-baselines/stable_baselines/ddpg/ddpg.py:332: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /home/people/paumjaud/WidowX-reacher/stable-baselines/stable_baselines/common/mpi_running_mean_std.py:17: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

WARNING:tensorflow:From /home/people/paumjaud/WidowX-reacher/stable-baselines/stable_baselines/ddpg/policies.py:134: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From /home/people/paumjaud/WidowX-reacher/stable-baselines/stable_baselines/ddpg/policies.py:136: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /home/people/paumjaud/.conda/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /home/people/paumjaud/WidowX-reacher/stable-baselines/stable_baselines/ddpg/ddpg.py:412: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /home/people/paumjaud/.conda/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /home/people/paumjaud/WidowX-reacher/stable-baselines/stable_baselines/ddpg/ddpg.py:452: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

========== widowx_reacher-v5 ==========
Seed: 0
OrderedDict([('actor_lr', 0.0010561308249352115),
             ('batch_size', 64),
             ('critic_lr', 0.0010561308249352115),
             ('gamma', 0.98),
             ('memory_limit', 100000),
             ('n_timesteps', 200000.0),
             ('noise_std', 0.6404031783349042),
             ('noise_type', 'normal'),
             ('normalize_observations', True),
             ('normalize_returns', True),
             ('policy', 'MlpPolicy')])
Using 1 environments
Overwriting n_timesteps with n=200000
Creating test environment
TRAINING ENV TYPE :  <Monitor<TimeLimit<WidowxEnv<widowx_reacher-v5>>>>
EVAL ENV TYPE :  <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f17650565c0>
Applying normal noise with std 0.6404031783349042
Log path: logs/train_0.2M_widowx_reacher-v5_SONIC/ddpg/widowx_reacher-v5_1
Eval num_timesteps=10000, episode_reward=-0.17 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
--------------------------------------
| obs_rms_mean            | -0.0232  |
| obs_rms_std             | 0.179    |
| reference_Q_mean        | -0.0135  |
| reference_Q_std         | 0.0209   |
| reference_action_mean   | 0.127    |
| reference_action_std    | 0.943    |
| reference_actor_Q_mean  | -0.0115  |
| reference_actor_Q_std   | 0.0201   |
| ret_rms_mean            | 0        |
| ret_rms_std             | 1        |
| rollout/Q_mean          | -0.01    |
| rollout/actions_mean    | 0.147    |
| rollout/actions_std     | 0.692    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 100      |
| rollout/return          | -0.374   |
| rollout/return_history  | -0.374   |
| total/duration          | 30.5     |
| total/episodes          | 100      |
| total/epochs            | 1        |
| total/steps             | 9998     |
| total/steps_per_second  | 328      |
| train/loss_actor        | 0.0156   |
| train/loss_critic       | 8.55e-06 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=20000, episode_reward=-0.87 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| obs_rms_mean            | 0.0156   |
| obs_rms_std             | 0.202    |
| reference_Q_mean        | -0.0107  |
| reference_Q_std         | 0.023    |
| reference_action_mean   | 0.237    |
| reference_action_std    | 0.936    |
| reference_actor_Q_mean  | -0.00821 |
| reference_actor_Q_std   | 0.0226   |
| ret_rms_mean            | 0        |
| ret_rms_std             | 1        |
| rollout/Q_mean          | -0.0109  |
| rollout/actions_mean    | 0.236    |
| rollout/actions_std     | 0.714    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 200      |
| rollout/return          | -0.341   |
| rollout/return_history  | -0.307   |
| total/duration          | 60.8     |
| total/episodes          | 200      |
| total/epochs            | 1        |
| total/steps             | 19998    |
| total/steps_per_second  | 329      |
| train/loss_actor        | 0.0127   |
| train/loss_critic       | 1.53e-05 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=30000, episode_reward=-1.39 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| obs_rms_mean            | 0.0252   |
| obs_rms_std             | 0.253    |
| reference_Q_mean        | -0.011   |
| reference_Q_std         | 0.0213   |
| reference_action_mean   | 0.433    |
| reference_action_std    | 0.887    |
| reference_actor_Q_mean  | -0.0101  |
| reference_actor_Q_std   | 0.0206   |
| ret_rms_mean            | 0        |
| ret_rms_std             | 1        |
| rollout/Q_mean          | -0.0171  |
| rollout/actions_mean    | 0.228    |
| rollout/actions_std     | 0.736    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 300      |
| rollout/return          | -0.516   |
| rollout/return_history  | -0.867   |
| total/duration          | 91.5     |
| total/episodes          | 300      |
| total/epochs            | 1        |
| total/steps             | 29998    |
| total/steps_per_second  | 328      |
| train/loss_actor        | 0.0176   |
| train/loss_critic       | 2.21e-05 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=40000, episode_reward=-0.17 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
--------------------------------------
| obs_rms_mean            | 0.0409   |
| obs_rms_std             | 0.25     |
| reference_Q_mean        | -0.0117  |
| reference_Q_std         | 0.0245   |
| reference_action_mean   | 0.29     |
| reference_action_std    | 0.948    |
| reference_actor_Q_mean  | -0.01    |
| reference_actor_Q_std   | 0.0238   |
| ret_rms_mean            | 0        |
| ret_rms_std             | 1        |
| rollout/Q_mean          | -0.0168  |
| rollout/actions_mean    | 0.244    |
| rollout/actions_std     | 0.74     |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 400      |
| rollout/return          | -0.488   |
| rollout/return_history  | -0.403   |
| total/duration          | 122      |
| total/episodes          | 400      |
| total/epochs            | 1        |
| total/steps             | 39998    |
| total/steps_per_second  | 327      |
| train/loss_actor        | 0.0208   |
| train/loss_critic       | 3.88e-05 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=50000, episode_reward=-0.31 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| obs_rms_mean            | 0.0311   |
| obs_rms_std             | 0.247    |
| reference_Q_mean        | -0.0101  |
| reference_Q_std         | 0.0265   |
| reference_action_mean   | 0.369    |
| reference_action_std    | 0.915    |
| reference_actor_Q_mean  | -0.00847 |
| reference_actor_Q_std   | 0.0263   |
| ret_rms_mean            | 0        |
| ret_rms_std             | 1        |
| rollout/Q_mean          | -0.0158  |
| rollout/actions_mean    | 0.25     |
| rollout/actions_std     | 0.742    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 500      |
| rollout/return          | -0.452   |
| rollout/return_history  | -0.311   |
| total/duration          | 153      |
| total/episodes          | 500      |
| total/epochs            | 1        |
| total/steps             | 49998    |
| total/steps_per_second  | 327      |
| train/loss_actor        | 0.0168   |
| train/loss_critic       | 5.66e-05 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=60000, episode_reward=-0.42 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| obs_rms_mean            | 0.016    |
| obs_rms_std             | 0.263    |
| reference_Q_mean        | -0.00961 |
| reference_Q_std         | 0.024    |
| reference_action_mean   | 0.256    |
| reference_action_std    | 0.954    |
| reference_actor_Q_mean  | -0.00832 |
| reference_actor_Q_std   | 0.0237   |
| ret_rms_mean            | 0        |
| ret_rms_std             | 1        |
| rollout/Q_mean          | -0.0147  |
| rollout/actions_mean    | 0.224    |
| rollout/actions_std     | 0.755    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 600      |
| rollout/return          | -0.503   |
| rollout/return_history  | -0.754   |
| total/duration          | 184      |
| total/episodes          | 600      |
| total/epochs            | 1        |
| total/steps             | 59998    |
| total/steps_per_second  | 326      |
| train/loss_actor        | 0.0163   |
| train/loss_critic       | 4.14e-05 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=70000, episode_reward=-0.15 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
--------------------------------------
| obs_rms_mean            | 0.0115   |
| obs_rms_std             | 0.268    |
| reference_Q_mean        | -0.00885 |
| reference_Q_std         | 0.023    |
| reference_action_mean   | 0.167    |
| reference_action_std    | 0.963    |
| reference_actor_Q_mean  | -0.00608 |
| reference_actor_Q_std   | 0.0215   |
| ret_rms_mean            | 0        |
| ret_rms_std             | 1        |
| rollout/Q_mean          | -0.013   |
| rollout/actions_mean    | 0.21     |
| rollout/actions_std     | 0.761    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 700      |
| rollout/return          | -0.508   |
| rollout/return_history  | -0.54    |
| total/duration          | 215      |
| total/episodes          | 700      |
| total/epochs            | 1        |
| total/steps             | 69998    |
| total/steps_per_second  | 326      |
| train/loss_actor        | 0.0178   |
| train/loss_critic       | 6.33e-05 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=80000, episode_reward=-0.13 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
--------------------------------------
| obs_rms_mean            | 0.0201   |
| obs_rms_std             | 0.259    |
| reference_Q_mean        | -0.00835 |
| reference_Q_std         | 0.0249   |
| reference_action_mean   | 0.345    |
| reference_action_std    | 0.927    |
| reference_actor_Q_mean  | -0.00595 |
| reference_actor_Q_std   | 0.0238   |
| ret_rms_mean            | 0        |
| ret_rms_std             | 1        |
| rollout/Q_mean          | -0.0102  |
| rollout/actions_mean    | 0.226    |
| rollout/actions_std     | 0.759    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 800      |
| rollout/return          | -0.475   |
| rollout/return_history  | -0.241   |
| total/duration          | 246      |
| total/episodes          | 800      |
| total/epochs            | 1        |
| total/steps             | 79998    |
| total/steps_per_second  | 325      |
| train/loss_actor        | 0.0173   |
| train/loss_critic       | 5.23e-05 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=90000, episode_reward=-0.13 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
--------------------------------------
| obs_rms_mean            | 0.0262   |
| obs_rms_std             | 0.253    |
| reference_Q_mean        | -0.00944 |
| reference_Q_std         | 0.0246   |
| reference_action_mean   | 0.506    |
| reference_action_std    | 0.846    |
| reference_actor_Q_mean  | -0.00743 |
| reference_actor_Q_std   | 0.0232   |
| ret_rms_mean            | 0        |
| ret_rms_std             | 1        |
| rollout/Q_mean          | -0.00777 |
| rollout/actions_mean    | 0.233    |
| rollout/actions_std     | 0.759    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 900      |
| rollout/return          | -0.445   |
| rollout/return_history  | -0.203   |
| total/duration          | 277      |
| total/episodes          | 900      |
| total/epochs            | 1        |
| total/steps             | 89998    |
| total/steps_per_second  | 325      |
| train/loss_actor        | 0.0151   |
| train/loss_critic       | 0.000148 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=100000, episode_reward=-0.14 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| obs_rms_mean            | 0.0355   |
| obs_rms_std             | 0.255    |
| reference_Q_mean        | -0.00984 |
| reference_Q_std         | 0.0268   |
| reference_action_mean   | 0.242    |
| reference_action_std    | 0.943    |
| reference_actor_Q_mean  | -0.00767 |
| reference_actor_Q_std   | 0.0259   |
| ret_rms_mean            | 0        |
| ret_rms_std             | 1        |
| rollout/Q_mean          | -0.00501 |
| rollout/actions_mean    | 0.229    |
| rollout/actions_std     | 0.762    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1e+03    |
| rollout/return          | -0.424   |
| rollout/return_history  | -0.235   |
| total/duration          | 308      |
| total/episodes          | 1e+03    |
| total/epochs            | 1        |
| total/steps             | 99998    |
| total/steps_per_second  | 325      |
| train/loss_actor        | 0.0117   |
| train/loss_critic       | 0.000151 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=110000, episode_reward=-0.13 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| obs_rms_mean            | 0.0454   |
| obs_rms_std             | 0.258    |
| reference_Q_mean        | -0.00899 |
| reference_Q_std         | 0.0264   |
| reference_action_mean   | 0.218    |
| reference_action_std    | 0.949    |
| reference_actor_Q_mean  | -0.00757 |
| reference_actor_Q_std   | 0.0252   |
| ret_rms_mean            | 0        |
| ret_rms_std             | 1        |
| rollout/Q_mean          | -0.00272 |
| rollout/actions_mean    | 0.229    |
| rollout/actions_std     | 0.762    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1.1e+03  |
| rollout/return          | -0.406   |
| rollout/return_history  | -0.23    |
| total/duration          | 339      |
| total/episodes          | 1.1e+03  |
| total/epochs            | 1        |
| total/steps             | 109998   |
| total/steps_per_second  | 325      |
| train/loss_actor        | 0.00975  |
| train/loss_critic       | 4.5e-05  |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=120000, episode_reward=-0.15 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------------
| obs_rms_mean            | 0.0523    |
| obs_rms_std             | 0.256     |
| reference_Q_mean        | -0.00846  |
| reference_Q_std         | 0.0265    |
| reference_action_mean   | 0.714     |
| reference_action_std    | 0.685     |
| reference_actor_Q_mean  | -0.00659  |
| reference_actor_Q_std   | 0.0263    |
| ret_rms_mean            | 0         |
| ret_rms_std             | 1         |
| rollout/Q_mean          | -0.000731 |
| rollout/actions_mean    | 0.229     |
| rollout/actions_std     | 0.762     |
| rollout/episode_steps   | 100       |
| rollout/episodes        | 1.2e+03   |
| rollout/return          | -0.391    |
| rollout/return_history  | -0.231    |
| total/duration          | 370       |
| total/episodes          | 1.2e+03   |
| total/epochs            | 1         |
| total/steps             | 119998    |
| total/steps_per_second  | 325       |
| train/loss_actor        | 0.00541   |
| train/loss_critic       | 0.000177  |
| train/param_noise_di... | 0         |
---------------------------------------

Eval num_timesteps=130000, episode_reward=-0.21 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| obs_rms_mean            | 0.057    |
| obs_rms_std             | 0.255    |
| reference_Q_mean        | -0.00619 |
| reference_Q_std         | 0.0267   |
| reference_action_mean   | 0.739    |
| reference_action_std    | 0.664    |
| reference_actor_Q_mean  | -0.00417 |
| reference_actor_Q_std   | 0.026    |
| ret_rms_mean            | 0        |
| ret_rms_std             | 1        |
| rollout/Q_mean          | 0.000134 |
| rollout/actions_mean    | 0.23     |
| rollout/actions_std     | 0.762    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1.3e+03  |
| rollout/return          | -0.383   |
| rollout/return_history  | -0.279   |
| total/duration          | 401      |
| total/episodes          | 1.3e+03  |
| total/epochs            | 1        |
| total/steps             | 129998   |
| total/steps_per_second  | 324      |
| train/loss_actor        | -0.00329 |
| train/loss_critic       | 2.62e-05 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=140000, episode_reward=-0.17 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| obs_rms_mean            | 0.0609   |
| obs_rms_std             | 0.257    |
| reference_Q_mean        | -0.00397 |
| reference_Q_std         | 0.0251   |
| reference_action_mean   | 0.522    |
| reference_action_std    | 0.832    |
| reference_actor_Q_mean  | -0.00201 |
| reference_actor_Q_std   | 0.0245   |
| ret_rms_mean            | 0        |
| ret_rms_std             | 1        |
| rollout/Q_mean          | 0.001    |
| rollout/actions_mean    | 0.227    |
| rollout/actions_std     | 0.764    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1.4e+03  |
| rollout/return          | -0.376   |
| rollout/return_history  | -0.282   |
| total/duration          | 432      |
| total/episodes          | 1.4e+03  |
| total/epochs            | 1        |
| total/steps             | 139998   |
| total/steps_per_second  | 324      |
| train/loss_actor        | -0.00667 |
| train/loss_critic       | 7.05e-05 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=150000, episode_reward=-0.20 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| obs_rms_mean            | 0.064    |
| obs_rms_std             | 0.258    |
| reference_Q_mean        | -0.00349 |
| reference_Q_std         | 0.0265   |
| reference_action_mean   | 0.809    |
| reference_action_std    | 0.579    |
| reference_actor_Q_mean  | -0.0022  |
| reference_actor_Q_std   | 0.0258   |
| ret_rms_mean            | 0        |
| ret_rms_std             | 1        |
| rollout/Q_mean          | 0.0022   |
| rollout/actions_mean    | 0.227    |
| rollout/actions_std     | 0.765    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1.5e+03  |
| rollout/return          | -0.368   |
| rollout/return_history  | -0.262   |
| total/duration          | 464      |
| total/episodes          | 1.5e+03  |
| total/epochs            | 1        |
| total/steps             | 149998   |
| total/steps_per_second  | 323      |
| train/loss_actor        | -0.00594 |
| train/loss_critic       | 2.13e-05 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=160000, episode_reward=-0.13 +/- 0.00
Episode length: 100.00 +/- 0.00
--------------------------------------
| obs_rms_mean            | 0.0656   |
| obs_rms_std             | 0.253    |
| reference_Q_mean        | -0.00374 |
| reference_Q_std         | 0.0283   |
| reference_action_mean   | 0.685    |
| reference_action_std    | 0.713    |
| reference_actor_Q_mean  | -0.00153 |
| reference_actor_Q_std   | 0.0277   |
| ret_rms_mean            | 0        |
| ret_rms_std             | 1        |
| rollout/Q_mean          | 0.00274  |
| rollout/actions_mean    | 0.235    |
| rollout/actions_std     | 0.764    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1.6e+03  |
| rollout/return          | -0.36    |
| rollout/return_history  | -0.24    |
| total/duration          | 495      |
| total/episodes          | 1.6e+03  |
| total/epochs            | 1        |
| total/steps             | 159998   |
| total/steps_per_second  | 323      |
| train/loss_actor        | -0.00977 |
| train/loss_critic       | 8.19e-06 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=170000, episode_reward=-0.12 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
--------------------------------------
| obs_rms_mean            | 0.0676   |
| obs_rms_std             | 0.249    |
| reference_Q_mean        | -0.00372 |
| reference_Q_std         | 0.0289   |
| reference_action_mean   | 0.266    |
| reference_action_std    | 0.937    |
| reference_actor_Q_mean  | -0.00196 |
| reference_actor_Q_std   | 0.0281   |
| ret_rms_mean            | 0        |
| ret_rms_std             | 1        |
| rollout/Q_mean          | 0.00359  |
| rollout/actions_mean    | 0.239    |
| rollout/actions_std     | 0.763    |
| rollout/episode_steps   | 100      |
| rollout/episodes        | 1.7e+03  |
| rollout/return          | -0.35    |
| rollout/return_history  | -0.192   |
| total/duration          | 527      |
| total/episodes          | 1.7e+03  |
| total/epochs            | 1        |
| total/steps             | 169998   |
| total/steps_per_second  | 323      |
| train/loss_actor        | -0.0133  |
| train/loss_critic       | 5.26e-06 |
| train/param_noise_di... | 0        |
--------------------------------------

Eval num_timesteps=180000, episode_reward=-0.13 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------------
| obs_rms_mean            | 0.0692    |
| obs_rms_std             | 0.247     |
| reference_Q_mean        | -0.000617 |
| reference_Q_std         | 0.0271    |
| reference_action_mean   | 0.43      |
| reference_action_std    | 0.88      |
| reference_actor_Q_mean  | 0.00132   |
| reference_actor_Q_std   | 0.0261    |
| ret_rms_mean            | 0         |
| ret_rms_std             | 1         |
| rollout/Q_mean          | 0.00413   |
| rollout/actions_mean    | 0.237     |
| rollout/actions_std     | 0.763     |
| rollout/episode_steps   | 100       |
| rollout/episodes        | 1.8e+03   |
| rollout/return          | -0.344    |
| rollout/return_history  | -0.232    |
| total/duration          | 559       |
| total/episodes          | 1.8e+03   |
| total/epochs            | 1         |
| total/steps             | 179998    |
| total/steps_per_second  | 322       |
| train/loss_actor        | -0.0134   |
| train/loss_critic       | 7.37e-06  |
| train/param_noise_di... | 0         |
---------------------------------------

Eval num_timesteps=190000, episode_reward=-0.15 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------------
| obs_rms_mean            | 0.0721    |
| obs_rms_std             | 0.246     |
| reference_Q_mean        | -9.22e-05 |
| reference_Q_std         | 0.0267    |
| reference_action_mean   | 0.35      |
| reference_action_std    | 0.907     |
| reference_actor_Q_mean  | 0.00203   |
| reference_actor_Q_std   | 0.0264    |
| ret_rms_mean            | 0         |
| ret_rms_std             | 1         |
| rollout/Q_mean          | 0.00458   |
| rollout/actions_mean    | 0.233     |
| rollout/actions_std     | 0.763     |
| rollout/episode_steps   | 100       |
| rollout/episodes        | 1.9e+03   |
| rollout/return          | -0.338    |
| rollout/return_history  | -0.243    |
| total/duration          | 591       |
| total/episodes          | 1.9e+03   |
| total/epochs            | 1         |
| total/steps             | 189998    |
| total/steps_per_second  | 322       |
| train/loss_actor        | -0.0122   |
| train/loss_critic       | 5.79e-06  |
| train/param_noise_di... | 0         |
---------------------------------------

Eval num_timesteps=200000, episode_reward=-0.15 +/- 0.00
Episode length: 100.00 +/- 0.00
---------------------------------------
| obs_rms_mean            | 0.075     |
| obs_rms_std             | 0.245     |
| reference_Q_mean        | -0.000451 |
| reference_Q_std         | 0.0277    |
| reference_action_mean   | 0.0696    |
| reference_action_std    | 0.961     |
| reference_actor_Q_mean  | 0.00108   |
| reference_actor_Q_std   | 0.0272    |
| ret_rms_mean            | 0         |
| ret_rms_std             | 1         |
| rollout/Q_mean          | 0.00511   |
| rollout/actions_mean    | 0.229     |
| rollout/actions_std     | 0.763     |
| rollout/episode_steps   | 100       |
| rollout/episodes        | 2e+03     |
| rollout/return          | -0.331    |
| rollout/return_history  | -0.199    |
| total/duration          | 622       |
| total/episodes          | 2e+03     |
| total/epochs            | 1         |
| total/steps             | 199998    |
| total/steps_per_second  | 321       |
| train/loss_actor        | -0.0133   |
| train/loss_critic       | 5.26e-06  |
| train/param_noise_di... | 0         |
---------------------------------------

/home/people/paumjaud/WidowX-reacher/stable-baselines/stable_baselines/common/callbacks.py:285: UserWarning: Training and eval env are not of the same type<Monitor<TimeLimit<WidowxEnv<widowx_reacher-v5>>>> != <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f17650565c0>
  "{} != {}".format(self.training_env, self.eval_env))
Saving to logs/train_0.2M_widowx_reacher-v5_SONIC/ddpg/widowx_reacher-v5_1
pybullet build time: May 18 2020 02:46:26
