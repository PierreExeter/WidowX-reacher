WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/sac/sac.py:141: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/sac/policies.py:194: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_layers.py:57: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/sac/sac.py:232: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/sac/sac.py:294: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/sac/sac.py:314: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

========== widowx_reacher-v5 ==========
Seed: 0
OrderedDict([('batch_size', 64),
             ('buffer_size', 50000),
             ('ent_coef', 'auto'),
             ('gamma', 0.99),
             ('gradient_steps', 1),
             ('learning_rate', 0.0003),
             ('learning_starts', 100),
             ('n_timesteps', 10000),
             ('policy', 'MlpPolicy'),
             ('policy_kwargs', 'None'),
             ('random_exploration', 0.0),
             ('target_entropy', 'auto'),
             ('target_update_interval', 1),
             ('tau', 0.005),
             ('train_freq', 1)])
Using 1 environments
Creating test environment
TRAINING ENV TYPE :  <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f9f58186828>
EVAL ENV TYPE :  <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f9f581a0f98>
Log path: logs/train_10K_widowx_reacher-v5/sac/widowx_reacher-v5_1
------------------------------------------
| current_lr              | 0.0003       |
| ent_coef                | 0.9416784    |
| ent_coef_loss           | -0.60930777  |
| entropy                 | 7.7379646    |
| ep_rewmean              | -2.7         |
| episodes                | 4            |
| eplenmean               | 100          |
| fps                     | 415          |
| mean 100 episode reward | -2.7         |
| n_updates               | 201          |
| policy_loss             | -5.6777964   |
| qf1_loss                | 0.0015207949 |
| qf2_loss                | 0.0015624962 |
| time_elapsed            | 0            |
| total timesteps         | 300          |
| value_loss              | 0.13294286   |
------------------------------------------
-----------------------------------------
| current_lr              | 0.0003      |
| ent_coef                | 0.8351812   |
| ent_coef_loss           | -1.8145341  |
| entropy                 | 7.777307    |
| ep_rewmean              | -2.9        |
| episodes                | 8           |
| eplenmean               | 100         |
| fps                     | 449         |
| mean 100 episode reward | -2.9        |
| n_updates               | 601         |
| policy_loss             | -10.66531   |
| qf1_loss                | 0.33669862  |
| qf2_loss                | 0.34550497  |
| time_elapsed            | 1           |
| total timesteps         | 700         |
| value_loss              | 0.041829318 |
-----------------------------------------
-----------------------------------------
| current_lr              | 0.0003      |
| ent_coef                | 0.74073005  |
| ent_coef_loss           | -3.0302315  |
| entropy                 | 7.830754    |
| ep_rewmean              | -3.12       |
| episodes                | 12          |
| eplenmean               | 100         |
| fps                     | 459         |
| mean 100 episode reward | -3.1        |
| n_updates               | 1001        |
| policy_loss             | -15.220545  |
| qf1_loss                | 0.047175586 |
| qf2_loss                | 0.035323374 |
| time_elapsed            | 2           |
| total timesteps         | 1100        |
| value_loss              | 0.06221981  |
-----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.65704525 |
| ent_coef_loss           | -4.202549  |
| entropy                 | 7.8558064  |
| ep_rewmean              | -3.04      |
| episodes                | 16         |
| eplenmean               | 100        |
| fps                     | 457        |
| mean 100 episode reward | -3         |
| n_updates               | 1401       |
| policy_loss             | -19.645052 |
| qf1_loss                | 1.8752791  |
| qf2_loss                | 1.8874863  |
| time_elapsed            | 3          |
| total timesteps         | 1500       |
| value_loss              | 0.06264593 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.582878   |
| ent_coef_loss           | -5.450806  |
| entropy                 | 7.9343786  |
| ep_rewmean              | -3.07      |
| episodes                | 20         |
| eplenmean               | 100        |
| fps                     | 456        |
| mean 100 episode reward | -3.1       |
| n_updates               | 1801       |
| policy_loss             | -23.286257 |
| qf1_loss                | 0.06814578 |
| qf2_loss                | 0.06576388 |
| time_elapsed            | 4          |
| total timesteps         | 1900       |
| value_loss              | 0.15577285 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.51717615 |
| ent_coef_loss           | -6.5192933 |
| entropy                 | 7.909051   |
| ep_rewmean              | -3.13      |
| episodes                | 24         |
| eplenmean               | 100        |
| fps                     | 457        |
| mean 100 episode reward | -3.1       |
| n_updates               | 2201       |
| policy_loss             | -26.57914  |
| qf1_loss                | 4.325586   |
| qf2_loss                | 4.2511773  |
| time_elapsed            | 5          |
| total timesteps         | 2300       |
| value_loss              | 0.09494072 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.45897245 |
| ent_coef_loss           | -7.7400312 |
| entropy                 | 7.775594   |
| ep_rewmean              | -3.21      |
| episodes                | 28         |
| eplenmean               | 100        |
| fps                     | 456        |
| mean 100 episode reward | -3.2       |
| n_updates               | 2601       |
| policy_loss             | -28.96106  |
| qf1_loss                | 9.894746   |
| qf2_loss                | 9.309102   |
| time_elapsed            | 5          |
| total timesteps         | 2700       |
| value_loss              | 0.15316552 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.40736973 |
| ent_coef_loss           | -8.933236  |
| entropy                 | 7.6594954  |
| ep_rewmean              | -3.12      |
| episodes                | 32         |
| eplenmean               | 100        |
| fps                     | 455        |
| mean 100 episode reward | -3.1       |
| n_updates               | 3001       |
| policy_loss             | -30.809875 |
| qf1_loss                | 0.20827931 |
| qf2_loss                | 0.21555369 |
| time_elapsed            | 6          |
| total timesteps         | 3100       |
| value_loss              | 0.08862123 |
----------------------------------------
-----------------------------------------
| current_lr              | 0.0003      |
| ent_coef                | 0.3616832   |
| ent_coef_loss           | -10.150827  |
| entropy                 | 7.7753954   |
| ep_rewmean              | -3.11       |
| episodes                | 36          |
| eplenmean               | 100         |
| fps                     | 455         |
| mean 100 episode reward | -3.1        |
| n_updates               | 3401        |
| policy_loss             | -32.310417  |
| qf1_loss                | 0.14349091  |
| qf2_loss                | 0.16129369  |
| time_elapsed            | 7           |
| total timesteps         | 3500        |
| value_loss              | 0.095321834 |
-----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.32134464 |
| ent_coef_loss           | -10.875926 |
| entropy                 | 7.5937195  |
| ep_rewmean              | -3.04      |
| episodes                | 40         |
| eplenmean               | 100        |
| fps                     | 457        |
| mean 100 episode reward | -3         |
| n_updates               | 3801       |
| policy_loss             | -33.661575 |
| qf1_loss                | 0.17117357 |
| qf2_loss                | 0.16543737 |
| time_elapsed            | 8          |
| total timesteps         | 3900       |
| value_loss              | 0.15057406 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.28546694 |
| ent_coef_loss           | -12.160963 |
| entropy                 | 7.7081585  |
| ep_rewmean              | -3.07      |
| episodes                | 44         |
| eplenmean               | 100        |
| fps                     | 458        |
| mean 100 episode reward | -3.1       |
| n_updates               | 4201       |
| policy_loss             | -34.836437 |
| qf1_loss                | 0.14938076 |
| qf2_loss                | 0.15913185 |
| time_elapsed            | 9          |
| total timesteps         | 4300       |
| value_loss              | 0.23812552 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.2539289  |
| ent_coef_loss           | -13.468451 |
| entropy                 | 7.7385244  |
| ep_rewmean              | -3.01      |
| episodes                | 48         |
| eplenmean               | 100        |
| fps                     | 459        |
| mean 100 episode reward | -3         |
| n_updates               | 4601       |
| policy_loss             | -35.65825  |
| qf1_loss                | 0.24584103 |
| qf2_loss                | 0.2537349  |
| time_elapsed            | 10         |
| total timesteps         | 4700       |
| value_loss              | 0.11765061 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.2256543  |
| ent_coef_loss           | -13.927807 |
| entropy                 | 7.513237   |
| ep_rewmean              | -2.92      |
| episodes                | 52         |
| eplenmean               | 100        |
| fps                     | 460        |
| mean 100 episode reward | -2.9       |
| n_updates               | 5001       |
| policy_loss             | -36.497986 |
| qf1_loss                | 24.18626   |
| qf2_loss                | 24.462852  |
| time_elapsed            | 11         |
| total timesteps         | 5100       |
| value_loss              | 0.19670552 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.20097741 |
| ent_coef_loss           | -15.222533 |
| entropy                 | 7.146619   |
| ep_rewmean              | -2.89      |
| episodes                | 56         |
| eplenmean               | 100        |
| fps                     | 461        |
| mean 100 episode reward | -2.9       |
| n_updates               | 5401       |
| policy_loss             | -36.08622  |
| qf1_loss                | 25.109205  |
| qf2_loss                | 25.069723  |
| time_elapsed            | 11         |
| total timesteps         | 5500       |
| value_loss              | 0.20149228 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.17901488 |
| ent_coef_loss           | -14.551725 |
| entropy                 | 7.0340376  |
| ep_rewmean              | -2.85      |
| episodes                | 60         |
| eplenmean               | 100        |
| fps                     | 461        |
| mean 100 episode reward | -2.9       |
| n_updates               | 5801       |
| policy_loss             | -36.420483 |
| qf1_loss                | 9.319445   |
| qf2_loss                | 9.213845   |
| time_elapsed            | 12         |
| total timesteps         | 5900       |
| value_loss              | 0.27393585 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.16002283 |
| ent_coef_loss           | -15.139265 |
| entropy                 | 7.0528646  |
| ep_rewmean              | -2.77      |
| episodes                | 64         |
| eplenmean               | 100        |
| fps                     | 462        |
| mean 100 episode reward | -2.8       |
| n_updates               | 6201       |
| policy_loss             | -35.405655 |
| qf1_loss                | 11.247549  |
| qf2_loss                | 11.5921955 |
| time_elapsed            | 13         |
| total timesteps         | 6300       |
| value_loss              | 0.3182491  |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.14302552 |
| ent_coef_loss           | -15.715868 |
| entropy                 | 6.431918   |
| ep_rewmean              | -2.79      |
| episodes                | 68         |
| eplenmean               | 100        |
| fps                     | 462        |
| mean 100 episode reward | -2.8       |
| n_updates               | 6601       |
| policy_loss             | -35.311935 |
| qf1_loss                | 0.6508571  |
| qf2_loss                | 0.5937222  |
| time_elapsed            | 14         |
| total timesteps         | 6700       |
| value_loss              | 0.17045012 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.12885469 |
| ent_coef_loss           | -15.275981 |
| entropy                 | 5.408908   |
| ep_rewmean              | -2.71      |
| episodes                | 72         |
| eplenmean               | 100        |
| fps                     | 463        |
| mean 100 episode reward | -2.7       |
| n_updates               | 7001       |
| policy_loss             | -35.78123  |
| qf1_loss                | 9.348252   |
| qf2_loss                | 9.510371   |
| time_elapsed            | 15         |
| total timesteps         | 7100       |
| value_loss              | 0.1881606  |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.11639467 |
| ent_coef_loss           | -14.67527  |
| entropy                 | 5.159066   |
| ep_rewmean              | -2.67      |
| episodes                | 76         |
| eplenmean               | 100        |
| fps                     | 462        |
| mean 100 episode reward | -2.7       |
| n_updates               | 7401       |
| policy_loss             | -35.027664 |
| qf1_loss                | 10.38099   |
| qf2_loss                | 10.219649  |
| time_elapsed            | 16         |
| total timesteps         | 7500       |
| value_loss              | 0.1435135  |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.1050582  |
| ent_coef_loss           | -15.223909 |
| entropy                 | 4.890911   |
| ep_rewmean              | -2.63      |
| episodes                | 80         |
| eplenmean               | 100        |
| fps                     | 461        |
| mean 100 episode reward | -2.6       |
| n_updates               | 7801       |
| policy_loss             | -34.483078 |
| qf1_loss                | 7.704115   |
| qf2_loss                | 7.8601656  |
| time_elapsed            | 17         |
| total timesteps         | 7900       |
| value_loss              | 0.1222754  |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.09505928 |
| ent_coef_loss           | -12.735517 |
| entropy                 | 4.0270157  |
| ep_rewmean              | -2.59      |
| episodes                | 84         |
| eplenmean               | 100        |
| fps                     | 460        |
| mean 100 episode reward | -2.6       |
| n_updates               | 8201       |
| policy_loss             | -34.356934 |
| qf1_loss                | 0.14072484 |
| qf2_loss                | 0.18375972 |
| time_elapsed            | 18         |
| total timesteps         | 8300       |
| value_loss              | 0.2516039  |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.08619478 |
| ent_coef_loss           | -12.038624 |
| entropy                 | 3.9083786  |
| ep_rewmean              | -2.56      |
| episodes                | 88         |
| eplenmean               | 100        |
| fps                     | 460        |
| mean 100 episode reward | -2.6       |
| n_updates               | 8601       |
| policy_loss             | -33.345943 |
| qf1_loss                | 21.40122   |
| qf2_loss                | 21.724861  |
| time_elapsed            | 18         |
| total timesteps         | 8700       |
| value_loss              | 0.19601318 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.07798842 |
| ent_coef_loss           | -12.300505 |
| entropy                 | 3.5226228  |
| ep_rewmean              | -2.54      |
| episodes                | 92         |
| eplenmean               | 100        |
| fps                     | 460        |
| mean 100 episode reward | -2.5       |
| n_updates               | 9001       |
| policy_loss             | -32.628223 |
| qf1_loss                | 8.111461   |
| qf2_loss                | 7.633438   |
| time_elapsed            | 19         |
| total timesteps         | 9100       |
| value_loss              | 0.08584736 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.07022724 |
| ent_coef_loss           | -14.760811 |
| entropy                 | 3.548513   |
| ep_rewmean              | -2.53      |
| episodes                | 96         |
| eplenmean               | 100        |
| fps                     | 461        |
| mean 100 episode reward | -2.5       |
| n_updates               | 9401       |
| policy_loss             | -31.455782 |
| qf1_loss                | 14.458134  |
| qf2_loss                | 13.867039  |
| time_elapsed            | 20         |
| total timesteps         | 9500       |
| value_loss              | 0.07431947 |
----------------------------------------
----------------------------------------
| current_lr              | 0.0003     |
| ent_coef                | 0.06418405 |
| ent_coef_loss           | -6.02762   |
| entropy                 | 4.0626936  |
| ep_rewmean              | -2.59      |
| episodes                | 100        |
| eplenmean               | 100        |
| fps                     | 460        |
| mean 100 episode reward | -2.6       |
| n_updates               | 9801       |
| policy_loss             | -32.60373  |
| qf1_loss                | 0.08793563 |
| qf2_loss                | 0.10960376 |
| time_elapsed            | 21         |
| total timesteps         | 9900       |
| value_loss              | 0.06693968 |
----------------------------------------
/home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/callbacks.py:285: UserWarning: Training and eval env are not of the same type<stable_baselines.common.base_class._UnvecWrapper object at 0x7f9f581a6128> != <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f9f581a0f98>
  "{} != {}".format(self.training_env, self.eval_env))
Eval num_timesteps=10000, episode_reward=-5.89 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
Saving to logs/train_10K_widowx_reacher-v5/sac/widowx_reacher-v5_1
pybullet build time: May 18 2020 02:46:26
