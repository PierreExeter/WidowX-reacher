WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/misc_util.py:26: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/sac/sac.py:141: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/sac/policies.py:194: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.flatten instead.
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/tf_layers.py:57: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/sac/sac.py:254: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /home/pierre/bin/anaconda3/envs/SB_widowx/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/sac/sac.py:294: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.

WARNING:tensorflow:From /home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/sac/sac.py:314: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.

========== widowx_reacher-v5 ==========
Seed: 0
OrderedDict([('batch_size', 32),
             ('buffer_size', 100000),
             ('ent_coef', 0.0001),
             ('gamma', 0.95),
             ('gradient_steps', 1),
             ('learning_rate', 0.0014704926021044404),
             ('learning_starts', 0),
             ('n_timesteps', 60000.0),
             ('policy', 'MlpPolicy'),
             ('policy_kwargs', 'dict(layers=[256, 256])'),
             ('train_freq', 1)])
Using 1 environments
Overwriting n_timesteps with n=10000
Creating test environment
TRAINING ENV TYPE :  <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f278fb46160>
EVAL ENV TYPE :  <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f278fb4ab00>
Log path: logs/train_0.01M_widowx_reacher-v5/sac/widowx_reacher-v5_1
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -3.0178626    |
| ep_rewmean              | -2.3          |
| episodes                | 4             |
| eplenmean               | 100           |
| fps                     | 302           |
| mean 100 episode reward | -2.3          |
| n_updates               | 269           |
| policy_loss             | -0.0707825    |
| qf1_loss                | 3.179667e-05  |
| qf2_loss                | 4.1652645e-05 |
| time_elapsed            | 0             |
| total timesteps         | 300           |
| value_loss              | 0.00012309852 |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -6.4828663    |
| ep_rewmean              | -2.28         |
| episodes                | 8             |
| eplenmean               | 100           |
| fps                     | 349           |
| mean 100 episode reward | -2.3          |
| n_updates               | 669           |
| policy_loss             | -0.055969775  |
| qf1_loss                | 0.00013249823 |
| qf2_loss                | 8.7467044e-05 |
| time_elapsed            | 1             |
| total timesteps         | 700           |
| value_loss              | 3.0873147e-05 |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -3.4356418    |
| ep_rewmean              | -2.25         |
| episodes                | 12            |
| eplenmean               | 100           |
| fps                     | 366           |
| mean 100 episode reward | -2.2          |
| n_updates               | 1069          |
| policy_loss             | 0.008980004   |
| qf1_loss                | 3.1933872e-05 |
| qf2_loss                | 5.891271e-05  |
| time_elapsed            | 3             |
| total timesteps         | 1100          |
| value_loss              | 0.0001350955  |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -2.2950478    |
| ep_rewmean              | -2.26         |
| episodes                | 16            |
| eplenmean               | 100           |
| fps                     | 373           |
| mean 100 episode reward | -2.3          |
| n_updates               | 1469          |
| policy_loss             | 0.047871206   |
| qf1_loss                | 7.152213e-05  |
| qf2_loss                | 3.2968273e-05 |
| time_elapsed            | 4             |
| total timesteps         | 1500          |
| value_loss              | 6.377561e-05  |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -4.238709     |
| ep_rewmean              | -2.06         |
| episodes                | 20            |
| eplenmean               | 100           |
| fps                     | 378           |
| mean 100 episode reward | -2.1          |
| n_updates               | 1869          |
| policy_loss             | 0.061381444   |
| qf1_loss                | 0.0008942527  |
| qf2_loss                | 0.00090130814 |
| time_elapsed            | 5             |
| total timesteps         | 1900          |
| value_loss              | 3.039249e-05  |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -4.21176      |
| ep_rewmean              | -1.8          |
| episodes                | 24            |
| eplenmean               | 100           |
| fps                     | 381           |
| mean 100 episode reward | -1.8          |
| n_updates               | 2269          |
| policy_loss             | 0.056950293   |
| qf1_loss                | 2.9144336e-05 |
| qf2_loss                | 8.0064465e-06 |
| time_elapsed            | 6             |
| total timesteps         | 2300          |
| value_loss              | 3.0666106e-05 |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -3.3717117    |
| ep_rewmean              | -1.75         |
| episodes                | 28            |
| eplenmean               | 100           |
| fps                     | 383           |
| mean 100 episode reward | -1.8          |
| n_updates               | 2669          |
| policy_loss             | 0.076417595   |
| qf1_loss                | 4.9307375e-05 |
| qf2_loss                | 2.8508153e-05 |
| time_elapsed            | 7             |
| total timesteps         | 2700          |
| value_loss              | 4.1772182e-05 |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -5.5247917    |
| ep_rewmean              | -1.59         |
| episodes                | 32            |
| eplenmean               | 100           |
| fps                     | 385           |
| mean 100 episode reward | -1.6          |
| n_updates               | 3069          |
| policy_loss             | 0.074038856   |
| qf1_loss                | 2.4342204e-05 |
| qf2_loss                | 1.7544013e-05 |
| time_elapsed            | 8             |
| total timesteps         | 3100          |
| value_loss              | 7.0011585e-05 |
-------------------------------------------
------------------------------------------
| current_lr              | 0.00147      |
| entropy                 | -2.6605146   |
| ep_rewmean              | -1.48        |
| episodes                | 36           |
| eplenmean               | 100          |
| fps                     | 386          |
| mean 100 episode reward | -1.5         |
| n_updates               | 3469         |
| policy_loss             | 0.08861022   |
| qf1_loss                | 0.0012465854 |
| qf2_loss                | 0.0013092007 |
| time_elapsed            | 9            |
| total timesteps         | 3500         |
| value_loss              | 3.188737e-05 |
------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -3.8147933    |
| ep_rewmean              | -1.38         |
| episodes                | 40            |
| eplenmean               | 100           |
| fps                     | 387           |
| mean 100 episode reward | -1.4          |
| n_updates               | 3869          |
| policy_loss             | 0.07801914    |
| qf1_loss                | 1.1982076e-05 |
| qf2_loss                | 2.3605353e-05 |
| time_elapsed            | 10            |
| total timesteps         | 3900          |
| value_loss              | 6.6994166e-05 |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -4.50102      |
| ep_rewmean              | -1.3          |
| episodes                | 44            |
| eplenmean               | 100           |
| fps                     | 388           |
| mean 100 episode reward | -1.3          |
| n_updates               | 4269          |
| policy_loss             | 0.093418464   |
| qf1_loss                | 1.0368208e-05 |
| qf2_loss                | 1.1910948e-05 |
| time_elapsed            | 11            |
| total timesteps         | 4300          |
| value_loss              | 1.815842e-05  |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -1.7462041    |
| ep_rewmean              | -1.25         |
| episodes                | 48            |
| eplenmean               | 100           |
| fps                     | 389           |
| mean 100 episode reward | -1.2          |
| n_updates               | 4669          |
| policy_loss             | 0.089748144   |
| qf1_loss                | 0.0003076355  |
| qf2_loss                | 0.00030432985 |
| time_elapsed            | 12            |
| total timesteps         | 4700          |
| value_loss              | 3.178945e-05  |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -1.5485923    |
| ep_rewmean              | -1.2          |
| episodes                | 52            |
| eplenmean               | 100           |
| fps                     | 389           |
| mean 100 episode reward | -1.2          |
| n_updates               | 5069          |
| policy_loss             | 0.08902608    |
| qf1_loss                | 1.41523e-05   |
| qf2_loss                | 1.4246019e-05 |
| time_elapsed            | 13            |
| total timesteps         | 5100          |
| value_loss              | 2.4617078e-05 |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -2.81299      |
| ep_rewmean              | -1.14         |
| episodes                | 56            |
| eplenmean               | 100           |
| fps                     | 390           |
| mean 100 episode reward | -1.1          |
| n_updates               | 5469          |
| policy_loss             | 0.09655417    |
| qf1_loss                | 1.4364715e-05 |
| qf2_loss                | 1.1586063e-05 |
| time_elapsed            | 14            |
| total timesteps         | 5500          |
| value_loss              | 1.0959631e-05 |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -4.124172     |
| ep_rewmean              | -1.1          |
| episodes                | 60            |
| eplenmean               | 100           |
| fps                     | 390           |
| mean 100 episode reward | -1.1          |
| n_updates               | 5869          |
| policy_loss             | 0.10137359    |
| qf1_loss                | 1.3960225e-05 |
| qf2_loss                | 1.1244099e-05 |
| time_elapsed            | 15            |
| total timesteps         | 5900          |
| value_loss              | 3.0437548e-05 |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -1.8160503    |
| ep_rewmean              | -1.06         |
| episodes                | 64            |
| eplenmean               | 100           |
| fps                     | 390           |
| mean 100 episode reward | -1.1          |
| n_updates               | 6269          |
| policy_loss             | 0.081996426   |
| qf1_loss                | 9.6083604e-05 |
| qf2_loss                | 9.4093906e-05 |
| time_elapsed            | 16            |
| total timesteps         | 6300          |
| value_loss              | 2.5618403e-05 |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -4.089102     |
| ep_rewmean              | -1.09         |
| episodes                | 68            |
| eplenmean               | 100           |
| fps                     | 391           |
| mean 100 episode reward | -1.1          |
| n_updates               | 6669          |
| policy_loss             | 0.10803817    |
| qf1_loss                | 5.614972e-06  |
| qf2_loss                | 4.729186e-06  |
| time_elapsed            | 17            |
| total timesteps         | 6700          |
| value_loss              | 2.5799327e-05 |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -2.5596087    |
| ep_rewmean              | -1.08         |
| episodes                | 72            |
| eplenmean               | 100           |
| fps                     | 390           |
| mean 100 episode reward | -1.1          |
| n_updates               | 7069          |
| policy_loss             | 0.097628534   |
| qf1_loss                | 1.5442836e-05 |
| qf2_loss                | 1.0058768e-05 |
| time_elapsed            | 18            |
| total timesteps         | 7100          |
| value_loss              | 2.8571501e-05 |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -3.4158018    |
| ep_rewmean              | -1.05         |
| episodes                | 76            |
| eplenmean               | 100           |
| fps                     | 391           |
| mean 100 episode reward | -1            |
| n_updates               | 7469          |
| policy_loss             | 0.09438666    |
| qf1_loss                | 8.0378115e-05 |
| qf2_loss                | 7.697926e-05  |
| time_elapsed            | 19            |
| total timesteps         | 7500          |
| value_loss              | 8.897822e-05  |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -2.4940815    |
| ep_rewmean              | -1.03         |
| episodes                | 80            |
| eplenmean               | 100           |
| fps                     | 391           |
| mean 100 episode reward | -1            |
| n_updates               | 7869          |
| policy_loss             | 0.09802328    |
| qf1_loss                | 3.0243737e-06 |
| qf2_loss                | 3.3043143e-06 |
| time_elapsed            | 20            |
| total timesteps         | 7900          |
| value_loss              | 2.2244438e-05 |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -1.6061634    |
| ep_rewmean              | -1.04         |
| episodes                | 84            |
| eplenmean               | 100           |
| fps                     | 391           |
| mean 100 episode reward | -1            |
| n_updates               | 8269          |
| policy_loss             | 0.08442703    |
| qf1_loss                | 5.6018307e-06 |
| qf2_loss                | 3.511953e-06  |
| time_elapsed            | 21            |
| total timesteps         | 8300          |
| value_loss              | 4.6625068e-05 |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -0.3073187    |
| ep_rewmean              | -1.04         |
| episodes                | 88            |
| eplenmean               | 100           |
| fps                     | 391           |
| mean 100 episode reward | -1            |
| n_updates               | 8669          |
| policy_loss             | 0.10037206    |
| qf1_loss                | 1.0141342e-05 |
| qf2_loss                | 1.4004049e-05 |
| time_elapsed            | 22            |
| total timesteps         | 8700          |
| value_loss              | 6.863946e-05  |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -0.9251139    |
| ep_rewmean              | -1.04         |
| episodes                | 92            |
| eplenmean               | 100           |
| fps                     | 389           |
| mean 100 episode reward | -1            |
| n_updates               | 9069          |
| policy_loss             | 0.1171931     |
| qf1_loss                | 3.853705e-05  |
| qf2_loss                | 2.8562863e-05 |
| time_elapsed            | 23            |
| total timesteps         | 9100          |
| value_loss              | 9.413785e-06  |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -0.55308974   |
| ep_rewmean              | -1.02         |
| episodes                | 96            |
| eplenmean               | 100           |
| fps                     | 389           |
| mean 100 episode reward | -1            |
| n_updates               | 9469          |
| policy_loss             | 0.117009535   |
| qf1_loss                | 6.7590145e-06 |
| qf2_loss                | 5.024098e-06  |
| time_elapsed            | 24            |
| total timesteps         | 9500          |
| value_loss              | 7.0129963e-06 |
-------------------------------------------
-------------------------------------------
| current_lr              | 0.00147       |
| entropy                 | -0.6196606    |
| ep_rewmean              | -1.01         |
| episodes                | 100           |
| eplenmean               | 100           |
| fps                     | 388           |
| mean 100 episode reward | -1            |
| n_updates               | 9869          |
| policy_loss             | 0.12058912    |
| qf1_loss                | 2.039456e-05  |
| qf2_loss                | 4.4700337e-05 |
| time_elapsed            | 25            |
| total timesteps         | 9900          |
| value_loss              | 1.5306992e-05 |
-------------------------------------------
/home/pierre/HDD/0_Complearn/1_learning/0_Reinforcement_learning/18_replab/pierreExeter_github/stable-baselines/stable_baselines/common/callbacks.py:285: UserWarning: Training and eval env are not of the same type<stable_baselines.common.base_class._UnvecWrapper object at 0x7f278fb4add8> != <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f278fb4ab00>
  "{} != {}".format(self.training_env, self.eval_env))
Eval num_timesteps=10000, episode_reward=-0.89 +/- 0.00
Episode length: 100.00 +/- 0.00
New best mean reward!
Saving to logs/train_0.01M_widowx_reacher-v5/sac/widowx_reacher-v5_1
pybullet build time: May 18 2020 02:46:26
