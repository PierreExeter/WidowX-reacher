doodad not detected
Not a valid git repo: /ichec/home/users/pierre/pierreExeter_github/rlkit/rlkit/..
2020-05-17 22:22:13.139787 IST | Variant:
2020-05-17 22:22:13.140344 IST | {
  "algo_params": {
    "max_path_length": 1000,
    "qf_learning_rate": 0.001,
    "discount": 0.99,
    "use_soft_update": true,
    "batch_size": 100,
    "num_steps_per_eval": 1000,
    "tau": 0.01,
    "num_steps_per_epoch": 5000,
    "num_epochs": 2,
    "policy_learning_rate": 0.0001
  }
}
pybullet build time: May 16 2020 18:16:17
b3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:
No inertial data for link, using mass=1, localinertiadiagonal = 1,1,1, identity local inertial frameb3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:
gripper_aux_linkb3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:
No inertial data for link, using mass=1, localinertiadiagonal = 1,1,1, identity local inertial frameb3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:
base_linkb3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:
No inertial data for link, using mass=1, localinertiadiagonal = 1,1,1, identity local inertial frameb3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:
gripper_aux_linkb3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:
No inertial data for link, using mass=1, localinertiadiagonal = 1,1,1, identity local inertial frameb3Warning[examples/Importers/ImportURDFDemo/BulletUrdfImporter.cpp,126]:
base_link********goal is : *********** [ 0.06836802  0.07010331  0.284764  ]
[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.[0m
[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.[0m
2020-05-17 22:22:15.653225 IST | [DDPG_Experiment_2020_05_17_22_22_13_0000--s-0] Iteration #0 | Collecting samples for evaluation
---------------------------  --------------
QF Loss                         0.000482388
Policy Loss                    -0.00401397
Raw Policy Loss                -0.00401397
Preactivation Policy Loss       0
Q Predictions Mean              0.00376186
Q Predictions Std               0.000490948
Q Predictions Max               0.00486066
Q Predictions Min               0.00251934
Q Targets Mean                 -0.0141997
Q Targets Std                   0.0126418
Q Targets Max                   0.00390875
Q Targets Min                  -0.0521735
Bellman Errors Mean             0.000482388
Bellman Errors Std              0.00064587
Bellman Errors Max              0.00309111
Bellman Errors Min              7.20987e-07
Policy Action Mean              0.0011666
Policy Action Std               0.0032167
Policy Action Max               0.00661466
Policy Action Min              -0.00476447
Test Rewards Mean              -0.0134348
Test Rewards Std                0.000995299
Test Rewards Max               -0.0117026
Test Rewards Min               -0.0151399
Test Returns Mean             -13.4348
Test Returns Std                0
Test Returns Max              -13.4348
Test Returns Min              -13.4348
Test Actions Mean               0.00218266
Test Actions Std                0.00191144
Test Actions Max                0.00456552
Test Actions Min               -0.000969064
Num Paths                       5
Exploration Rewards Mean       -0.0202181
Exploration Rewards Std         0.0141194
Exploration Rewards Max        -0.000102092
Exploration Rewards Min        -0.0588733
Exploration Returns Mean      -20.2181
Exploration Returns Std         9.11338
Exploration Returns Max        -9.18753
Exploration Returns Min       -30.9488
Exploration Actions Mean       -0.00193236
Exploration Actions Std         0.717041
Exploration Actions Max         1
Exploration Actions Min        -1
Final total_distance Mean       0.108178
Final total_distance Std        0
Final total_distance Max        0.108178
Final total_distance Min        0.108178
AverageReturn                 -13.4348
Number of train steps total     1
Number of env steps total    5000
Number of rollouts total        5
Train Time (s)                  0.15762
(Previous) Eval Time (s)        0
Sample Time (s)                 2.18431
Epoch Time (s)                  2.34193
Total Train Time (s)            3.17789
Epoch                           0
---------------------------  --------------
2020-05-17 22:22:16.469802 IST | [DDPG_Experiment_2020_05_17_22_22_13_0000--s-0] Iteration #0 | Epoch Duration: 3.2384378910064697
2020-05-17 22:22:16.470027 IST | [DDPG_Experiment_2020_05_17_22_22_13_0000--s-0] Iteration #0 | Started Training: True
2020-05-17 22:30:04.965271 IST | [DDPG_Experiment_2020_05_17_22_22_13_0000--s-0] Iteration #1 | Collecting samples for evaluation
---------------------------  ---------------
QF Loss                          0.00105248
Policy Loss                      0.0441581
Raw Policy Loss                  0.0441581
Preactivation Policy Loss        0
Q Predictions Mean              -0.0447972
Q Predictions Std                0.00100971
Q Predictions Max               -0.0426737
Q Predictions Min               -0.0481746
Q Targets Mean                  -0.0152627
Q Targets Std                    0.0131592
Q Targets Max                    0.00320949
Q Targets Min                   -0.0522097
Bellman Errors Mean              0.00105248
Bellman Errors Std               0.000648452
Bellman Errors Max               0.00254412
Bellman Errors Min               4.69013e-06
Policy Action Mean               0.0023888
Policy Action Std                0.00210957
Policy Action Max                0.00569611
Policy Action Min               -0.00155723
Test Rewards Mean               -0.00200518
Test Rewards Std                 0.000614783
Test Rewards Max                -0.000312249
Test Rewards Min                -0.0130356
Test Returns Mean               -2.00518
Test Returns Std                 0
Test Returns Max                -2.00518
Test Returns Min                -2.00518
Test Actions Mean                0.0590841
Test Actions Std                 0.275867
Test Actions Max                 0.999795
Test Actions Min                -0.995029
Num Paths                        5
Exploration Rewards Mean        -0.0202749
Exploration Rewards Std          0.0224477
Exploration Rewards Max         -7.57525e-05
Exploration Rewards Min         -0.061342
Exploration Returns Mean       -16.3132
Exploration Returns Std         18.05
Exploration Returns Max         -0.10331
Exploration Returns Min        -44.4869
Exploration Actions Mean         0.113589
Exploration Actions Std          0.749498
Exploration Actions Max          1
Exploration Actions Min         -1
Final total_distance Mean        0.0454367
Final total_distance Std         0
Final total_distance Max         0.0454367
Final total_distance Min         0.0454367
AverageReturn                   -2.00518
Number of train steps total   5001
Number of env steps total    10000
Number of rollouts total        10
Train Time (s)                 465.106
(Previous) Eval Time (s)         0.840854
Sample Time (s)                  3.25628
Epoch Time (s)                 469.204
Total Train Time (s)           472.373
Epoch                            1
---------------------------  ---------------
2020-05-17 22:30:05.788222 IST | [DDPG_Experiment_2020_05_17_22_22_13_0000--s-0] Iteration #1 | Epoch Duration: 469.3179817199707
2020-05-17 22:30:05.788448 IST | [DDPG_Experiment_2020_05_17_22_22_13_0000--s-0] Iteration #1 | Started Training: True
